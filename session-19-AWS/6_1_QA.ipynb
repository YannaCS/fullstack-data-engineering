{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Interview Questions and Answers\n",
    "\n",
    "## 1. Explain the difference between Amazon SQS and Amazon SNS. In what scenarios would you use SNS → SQS → Lambda instead of using Lambda directly?\n",
    "\n",
    "**Difference between SQS and SNS**:\n",
    "\n",
    "**Amazon SQS (Simple Queue Service)**:\n",
    "- Message queue service\n",
    "- Pull-based: Consumers poll the queue for messages\n",
    "- One-to-one communication (one message to one consumer, though multiple consumers can compete for messages)\n",
    "- Messages persist until explicitly deleted\n",
    "- Supports delayed processing and message ordering (FIFO)\n",
    "- Use case: Work queues, task distribution, buffering\n",
    "\n",
    "**Amazon SNS (Simple Notification Service)**:\n",
    "- Pub/sub messaging service\n",
    "- Push-based: SNS pushes messages to subscribers\n",
    "- One-to-many communication (fan-out pattern)\n",
    "- No persistence (messages are immediately delivered)\n",
    "- Use case: Broadcasting notifications, multi-subscriber scenarios\n",
    "\n",
    "**When to use SNS → SQS → Lambda instead of Lambda directly**:\n",
    "\n",
    "1. **Buffering and rate limiting**: SQS acts as a buffer between SNS and Lambda, preventing Lambda from being overwhelmed by sudden traffic spikes. Lambda polls SQS at a controlled rate.\n",
    "\n",
    "2. **Retry logic and fault tolerance**: If Lambda fails to process a message, it remains in SQS for retry. With direct Lambda invocation from SNS, failed messages are lost (unless DLQ is configured).\n",
    "\n",
    "3. **Message persistence**: SQS stores messages durably. If Lambda is temporarily unavailable, messages wait in the queue. Direct Lambda invocation doesn't persist messages.\n",
    "\n",
    "4. **Decoupling and flexibility**: SQS decouples the notification system from processing logic. You can change Lambda functions, add multiple consumers, or temporarily pause processing without affecting message ingestion.\n",
    "\n",
    "5. **Batch processing**: Lambda can process multiple SQS messages in a single invocation, improving efficiency. Direct SNS invocation processes one message per invocation.\n",
    "\n",
    "6. **Fan-out with different processing speeds**: If you have multiple Lambda functions with different processing times, SQS queues allow each Lambda to consume at its own pace.\n",
    "\n",
    "**Example scenario**:\n",
    "```text\n",
    "File uploaded to S3\n",
    "    → SNS (broadcasts to multiple subscribers)\n",
    "        → SQS Queue 1 → Lambda 1 (data validation)\n",
    "        → SQS Queue 2 → Lambda 2 (thumbnail generation)\n",
    "        → SQS Queue 3 → Lambda 3 (metadata extraction)\n",
    "\n",
    "Each Lambda processes independently, with its own retry logic and pace.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explain how an S3 event can trigger downstream processing\n",
    "\n",
    "**S3 Event-Driven Processing Flow**:\n",
    "\n",
    "1. **Event configuration**: Configure S3 bucket to send notifications when specific events occur (ObjectCreated, ObjectRemoved, etc.).\n",
    "\n",
    "2. **Event types**:\n",
    "   - ObjectCreated (PUT, POST, COPY, CompleteMultipartUpload)\n",
    "   - ObjectRemoved (DELETE)\n",
    "   - ObjectRestore (Glacier restore)\n",
    "   - Replication events\n",
    "\n",
    "3. **Event destinations**:\n",
    "   - Lambda function (direct invocation)\n",
    "   - SQS queue (message sent to queue)\n",
    "   - SNS topic (notification published)\n",
    "   - EventBridge (advanced routing)\n",
    "\n",
    "4. **Event payload**: S3 sends a JSON payload containing:\n",
    "   - Bucket name\n",
    "   - Object key (file path)\n",
    "   - Event type\n",
    "   - Request parameters\n",
    "   - User identity\n",
    "   - Source IP address\n",
    "\n",
    "5. **Processing flow example**:\n",
    "\n",
    "```text\n",
    "User uploads \"data.csv\" to S3 bucket\n",
    "    ↓\n",
    "S3 generates ObjectCreated:Put event\n",
    "    ↓\n",
    "S3 sends event to Lambda function\n",
    "    ↓\n",
    "Lambda receives event payload:\n",
    "{\n",
    "  \"Records\": [{\n",
    "    \"s3\": {\n",
    "      \"bucket\": {\"name\": \"my-bucket\"},\n",
    "      \"object\": {\"key\": \"data.csv\", \"size\": 1024}\n",
    "    }\n",
    "  }]\n",
    "}\n",
    "    ↓\n",
    "Lambda extracts bucket and key\n",
    "    ↓\n",
    "Lambda uses boto3 to read file from S3\n",
    "    ↓\n",
    "Lambda processes data (validate, transform, etc.)\n",
    "    ↓\n",
    "Lambda writes results to another S3 bucket or database\n",
    "    ↓\n",
    "Lambda logs execution details to CloudWatch\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Near real-time: Processing starts within seconds of file upload\n",
    "- Fully automated: No manual intervention required\n",
    "- Scalable: Lambda scales automatically with number of files\n",
    "- Cost-effective: Pay only for processing time\n",
    "- Event-driven: No polling or scheduled checks needed\n",
    "\n",
    "**Common use cases**:\n",
    "- ETL pipelines triggered by data file uploads\n",
    "- Image/video processing upon media upload\n",
    "- Log file analysis\n",
    "- Data validation and quality checks\n",
    "- Backup and archival workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why is Lambda not suitable for heavy data processing jobs like Spark or Hadoop?\n",
    "\n",
    "**Lambda limitations for heavy data processing**:\n",
    "\n",
    "1. **Maximum execution time: 15 minutes**\n",
    "   - Spark/Hadoop jobs often run for hours or even days\n",
    "   - Lambda terminates after 900 seconds (15 minutes)\n",
    "   - Big data processing requires long-running computations\n",
    "\n",
    "2. **Limited memory: Maximum 10 GB**\n",
    "   - Spark processes data in-memory for speed\n",
    "   - Large datasets require tens or hundreds of GB of RAM\n",
    "   - Lambda's 10 GB limit is insufficient for big data workloads\n",
    "\n",
    "3. **Limited CPU: Scales with memory, max ~6 vCPUs**\n",
    "   - Hadoop/Spark leverage clusters with hundreds of CPUs\n",
    "   - Parallel processing across many nodes is essential\n",
    "   - Lambda's CPU is limited and doesn't support distributed computing\n",
    "\n",
    "4. **Limited local storage: 512 MB to 10 GB /tmp**\n",
    "   - Big data processing requires substantial temporary storage\n",
    "   - Spark spills data to disk when memory is full\n",
    "   - Lambda's ephemeral storage is too small for large intermediate results\n",
    "\n",
    "5. **No distributed computing framework**\n",
    "   - Spark/Hadoop use distributed file systems (HDFS)\n",
    "   - Built-in resource management (YARN)\n",
    "   - Data locality and partition management\n",
    "   - Lambda functions are isolated and don't share state or coordinate\n",
    "\n",
    "6. **Stateless execution**\n",
    "   - Each Lambda invocation is independent\n",
    "   - No shared state between invocations\n",
    "   - Spark maintains execution context across transformations\n",
    "\n",
    "7. **Cost inefficiency for long jobs**\n",
    "   - Lambda charges per GB-second\n",
    "   - Long-running jobs become expensive\n",
    "   - Dedicated compute (EC2, EMR) is more cost-effective for sustained workloads\n",
    "\n",
    "**What to use instead**:\n",
    "\n",
    "- **AWS EMR (Elastic MapReduce)**: Managed Hadoop/Spark clusters\n",
    "- **AWS Glue**: Serverless ETL with Spark engine\n",
    "- **Amazon Athena**: SQL queries on S3 data\n",
    "- **AWS Batch**: Run batch computing workloads\n",
    "- **EC2 with Spark**: Self-managed Spark clusters\n",
    "\n",
    "**Lambda's sweet spot**:\n",
    "- Short-lived tasks (< 15 minutes)\n",
    "- Lightweight data processing (< 10 GB memory)\n",
    "- Event-driven workflows\n",
    "- API backends\n",
    "- File format conversions\n",
    "- Small-scale data transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What does \"serverless\" mean in AWS Lambda?\n",
    "\n",
    "**Serverless definition**:\n",
    "\n",
    "Serverless computing is a cloud execution model where the cloud provider (AWS) dynamically manages the allocation and provisioning of servers. Developers focus solely on code without managing infrastructure.\n",
    "\n",
    "**Key aspects of serverless in AWS Lambda**:\n",
    "\n",
    "1. **No server management**:\n",
    "   - No EC2 instances to launch or configure\n",
    "   - No operating system updates or patches\n",
    "   - No capacity planning or scaling decisions\n",
    "   - AWS handles all infrastructure automatically\n",
    "\n",
    "2. **Automatic scaling**:\n",
    "   - Lambda automatically scales from zero to thousands of concurrent executions\n",
    "   - No need to configure auto-scaling rules\n",
    "   - Scales up during traffic spikes, scales down to zero when idle\n",
    "\n",
    "3. **Pay-per-use pricing**:\n",
    "   - Charged only for actual execution time (billed per millisecond)\n",
    "   - No charges when code isn't running\n",
    "   - No idle time costs (unlike EC2 instances running 24/7)\n",
    "\n",
    "4. **Event-driven execution**:\n",
    "   - Code runs in response to events (S3 uploads, API calls, schedule)\n",
    "   - No long-running processes or daemon services\n",
    "   - Ephemeral: Each execution is independent and short-lived\n",
    "\n",
    "5. **Built-in high availability**:\n",
    "   - AWS runs Lambda across multiple availability zones\n",
    "   - Automatic failover and redundancy\n",
    "   - No need to architect for fault tolerance\n",
    "\n",
    "6. **No infrastructure to maintain**:\n",
    "   - No SSH access to servers\n",
    "   - No system administration tasks\n",
    "   - Focus purely on application logic\n",
    "\n",
    "**What serverless does NOT mean**:\n",
    "\n",
    "- Servers still exist (AWS manages them behind the scenes)\n",
    "- Not always cheaper (depends on usage patterns)\n",
    "- Not suitable for all workloads (long-running, stateful apps)\n",
    "- Still has limitations (execution time, memory, concurrent executions)\n",
    "\n",
    "**Benefits of serverless**:\n",
    "- Faster time to market (no infrastructure setup)\n",
    "- Lower operational overhead\n",
    "- Automatic scaling and high availability\n",
    "- Cost efficiency for sporadic workloads\n",
    "\n",
    "**Trade-offs**:\n",
    "- Cold start latency (first invocation delay)\n",
    "- Limited execution time (15 minutes max)\n",
    "- Vendor lock-in\n",
    "- Debugging and monitoring challenges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Lambda Function for S3 Event Processing\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Write a Python AWS Lambda function that:\n",
    "- Is triggered by an S3 ObjectCreated event\n",
    "- Extracts bucket and object key from the event\n",
    "- Reads the file content from S3 using boto3\n",
    "- Prints the file size and first 100 characters to CloudWatch\n",
    "- Returns HTTP status code 200\n",
    "\n",
    "## Complete Implementation\n",
    "\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function triggered by S3 ObjectCreated event\n",
    "    \n",
    "    Args:\n",
    "        event: S3 event payload containing bucket and object information\n",
    "        context: Lambda runtime information\n",
    "    \n",
    "    Returns:\n",
    "        Response with status code 200\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Extract bucket name and object key from event\n",
    "        # S3 event structure: event['Records'][0]['s3']['bucket']['name']\n",
    "        record = event['Records'][0]\n",
    "        bucket_name = record['s3']['bucket']['name']\n",
    "        object_key = record['s3']['object']['key']\n",
    "        \n",
    "        print(f\"Processing file: s3://{bucket_name}/{object_key}\")\n",
    "        \n",
    "        # Read file content from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        file_content = response['Body'].read()\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = len(file_content)\n",
    "        print(f\"File size: {file_size} bytes\")\n",
    "        \n",
    "        # Decode content and get first 100 characters\n",
    "        # Handle both text and binary files\n",
    "        try:\n",
    "            content_text = file_content.decode('utf-8')\n",
    "            first_100_chars = content_text[:100]\n",
    "            print(f\"First 100 characters: {first_100_chars}\")\n",
    "        except UnicodeDecodeError:\n",
    "            # Binary file\n",
    "            print(\"Binary file detected - cannot display text preview\")\n",
    "            first_100_chars = str(file_content[:100])\n",
    "            print(f\"First 100 bytes (hex): {file_content[:100].hex()}\")\n",
    "        \n",
    "        # Return success response\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({\n",
    "                'message': 'File processed successfully',\n",
    "                'bucket': bucket_name,\n",
    "                'key': object_key,\n",
    "                'size': file_size,\n",
    "                'preview': first_100_chars\n",
    "            })\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({\n",
    "                'message': 'Error processing file',\n",
    "                'error': str(e)\n",
    "            })\n",
    "        }\n",
    "```\n",
    "\n",
    "## Lambda IAM Role Permissions\n",
    "\n",
    "The Lambda execution role must have these permissions:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetObject\"\n",
    "      ],\n",
    "      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"logs:CreateLogGroup\",\n",
    "        \"logs:CreateLogStream\",\n",
    "        \"logs:PutLogEvents\"\n",
    "      ],\n",
    "      \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## S3 Event Configuration\n",
    "\n",
    "Configure S3 bucket to trigger Lambda:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"LambdaFunctionConfigurations\": [\n",
    "    {\n",
    "      \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:my-function\",\n",
    "      \"Events\": [\"s3:ObjectCreated:*\"],\n",
    "      \"Filter\": {\n",
    "        \"Key\": {\n",
    "          \"FilterRules\": [\n",
    "            {\n",
    "              \"Name\": \"prefix\",\n",
    "              \"Value\": \"uploads/\"\n",
    "            },\n",
    "            {\n",
    "              \"Name\": \"suffix\",\n",
    "              \"Value\": \".csv\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Sample S3 Event Payload\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"eventVersion\": \"2.1\",\n",
    "      \"eventSource\": \"aws:s3\",\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventTime\": \"2024-01-01T12:00:00.000Z\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"s3\": {\n",
    "        \"bucket\": {\n",
    "          \"name\": \"my-bucket\",\n",
    "          \"arn\": \"arn:aws:s3:::my-bucket\"\n",
    "        },\n",
    "        \"object\": {\n",
    "          \"key\": \"uploads/data.csv\",\n",
    "          \"size\": 1024,\n",
    "          \"eTag\": \"d41d8cd98f00b204e9800998ecf8427e\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Expected CloudWatch Logs Output\n",
    "\n",
    "```text\n",
    "START RequestId: abc123 Version: $LATEST\n",
    "Processing file: s3://my-bucket/uploads/data.csv\n",
    "File size: 1024 bytes\n",
    "First 100 characters: id,name,email,age\n",
    "1,John Doe,john@example.com,30\n",
    "2,Jane Smith,jane@example.com,25\n",
    "3,Bob J\n",
    "END RequestId: abc123\n",
    "REPORT RequestId: abc123  Duration: 234.56 ms  Billed Duration: 235 ms  Memory Size: 128 MB  Max Memory Used: 45 MB\n",
    "```\n",
    "\n",
    "## Testing the Function\n",
    "\n",
    "Test event for Lambda console:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"s3\": {\n",
    "        \"bucket\": {\n",
    "          \"name\": \"my-test-bucket\"\n",
    "        },\n",
    "        \"object\": {\n",
    "          \"key\": \"test-file.txt\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Enhanced Version with Error Handling\n",
    "\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Enhanced Lambda function with comprehensive error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Validate event structure\n",
    "        if 'Records' not in event or len(event['Records']) == 0:\n",
    "            raise ValueError(\"Invalid S3 event: No records found\")\n",
    "        \n",
    "        record = event['Records'][0]\n",
    "        \n",
    "        # Extract S3 information\n",
    "        bucket_name = record['s3']['bucket']['name']\n",
    "        object_key = record['s3']['object']['key']\n",
    "        object_size = record['s3']['object'].get('size', 0)\n",
    "        \n",
    "        print(f\"Event: {record['eventName']}\")\n",
    "        print(f\"Bucket: {bucket_name}\")\n",
    "        print(f\"Object: {object_key}\")\n",
    "        print(f\"Size: {object_size} bytes\")\n",
    "        \n",
    "        # Skip processing for very large files\n",
    "        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB\n",
    "        if object_size > MAX_FILE_SIZE:\n",
    "            print(f\"File too large ({object_size} bytes). Skipping preview.\")\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps({\n",
    "                    'message': 'File too large for preview',\n",
    "                    'size': object_size\n",
    "                })\n",
    "            }\n",
    "        \n",
    "        # Read file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "            file_content = response['Body'].read()\n",
    "            actual_size = len(file_content)\n",
    "            \n",
    "            print(f\"Successfully read file. Size: {actual_size} bytes\")\n",
    "            \n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == 'NoSuchKey':\n",
    "                print(f\"Object not found: {object_key}\")\n",
    "            elif error_code == 'AccessDenied':\n",
    "                print(f\"Access denied to object: {object_key}\")\n",
    "            else:\n",
    "                print(f\"S3 error: {error_code}\")\n",
    "            raise\n",
    "        \n",
    "        # Process content\n",
    "        try:\n",
    "            content_text = file_content.decode('utf-8')\n",
    "            first_100 = content_text[:100]\n",
    "            print(f\"First 100 characters:\\n{first_100}\")\n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Binary file detected\")\n",
    "            first_100 = file_content[:100].hex()\n",
    "            print(f\"First 100 bytes (hex): {first_100}\")\n",
    "        \n",
    "        # Success response\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({\n",
    "                'message': 'Successfully processed file',\n",
    "                'bucket': bucket_name,\n",
    "                'key': object_key,\n",
    "                'size': actual_size,\n",
    "                'preview_length': len(first_100)\n",
    "            })\n",
    "        }\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"Missing required field in event: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 400,\n",
    "            'body': json.dumps({'error': f'Invalid event structure: {str(e)}'})\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "```\n",
    "\n",
    "## Key Points to Remember\n",
    "\n",
    "1. **Event structure**: Always access `event['Records'][0]` for S3 events\n",
    "2. **IAM permissions**: Lambda needs `s3:GetObject` permission\n",
    "3. **Error handling**: Handle both S3 errors and content decoding errors\n",
    "4. **CloudWatch logging**: Use `print()` statements for CloudWatch Logs\n",
    "5. **Response format**: Return structured response with statusCode and body\n",
    "6. **Memory efficiency**: For large files, consider streaming or processing in chunks\n",
    "7. **Timeouts**: Set appropriate timeout (default 3s might be too short)\n",
    "8. **Binary files**: Handle binary content gracefully (images, PDFs, etc.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
