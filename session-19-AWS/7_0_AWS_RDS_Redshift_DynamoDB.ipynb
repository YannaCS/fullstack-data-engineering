{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Database Services Comprehensive Notes\n",
    "\n",
    "# Amazon RDS (Relational Database Service)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Amazon RDS is a managed relational database service on AWS that simplifies database administration tasks. It supports multiple database engines and is optimized for transactional workloads.\n",
    "\n",
    "## Supported Database Engines\n",
    "\n",
    "RDS supports the following database engines:\n",
    "\n",
    "- **Amazon Aurora**: AWS's proprietary database engine, compatible with MySQL and PostgreSQL\n",
    "- **MySQL**: Open-source relational database\n",
    "- **PostgreSQL**: Advanced open-source relational database\n",
    "- **MariaDB**: Community-developed fork of MySQL\n",
    "- **Oracle Database**: Enterprise-grade commercial database\n",
    "- **SQL Server**: Microsoft's relational database management system\n",
    "\n",
    "## Workload Characteristics\n",
    "\n",
    "### Designed For: OLTP (Online Transaction Processing)\n",
    "\n",
    "RDS is optimized for OLTP workloads that require:\n",
    "\n",
    "- **Transactions with ACID properties**: Ensures data integrity and reliability\n",
    "- **High consistency**: Maintains data accuracy across all operations\n",
    "- **Low-latency reads and writes**: Fast response times for individual operations\n",
    "- **Frequent small updates**: Handling many concurrent small transactions\n",
    "\n",
    "### Not Designed For: OLAP (Online Analytical Processing)\n",
    "\n",
    "RDS is not suitable for:\n",
    "\n",
    "- **Large-scale table scans**: Reading entire large tables\n",
    "- **Heavy aggregations**: Complex analytical queries across massive datasets\n",
    "- **Big data analytics**: Data warehousing and business intelligence workloads\n",
    "\n",
    "For analytical workloads, AWS offers Redshift instead.\n",
    "\n",
    "## ACID Properties\n",
    "\n",
    "RDS databases provide full ACID compliance, ensuring reliable transaction processing.\n",
    "\n",
    "### Atomicity\n",
    "\n",
    "A transaction is treated as a single, indivisible unit of work. Either all operations in the transaction complete successfully, or none of them do. This is an all-or-nothing approach.\n",
    "\n",
    "**Example**: When transferring money between bank accounts, both the debit and credit must occur together, or neither should happen.\n",
    "\n",
    "### Consistency\n",
    "\n",
    "A transaction brings the database from one valid state to another valid state. All database rules, constraints, and triggers are maintained.\n",
    "\n",
    "**Example**: If a database constraint requires account balances to be non-negative, any transaction that would violate this rule will be rejected.\n",
    "\n",
    "### Isolation\n",
    "\n",
    "Concurrent transactions execute independently without interfering with each other. Multiple transactions can run simultaneously without causing data corruption.\n",
    "\n",
    "#### Isolation Levels\n",
    "\n",
    "Different isolation levels provide varying degrees of protection:\n",
    "\n",
    "**1. Read Uncommitted (Lowest isolation)**\n",
    "\n",
    "- Allows dirty reads: Transaction can read uncommitted changes from other transactions\n",
    "- Example scenario: Balance is 1000, Transaction T1 changes it to 500 but hasn't committed. Transaction T2 can read 500 even though it's not committed yet.\n",
    "- **Risk**: If T1 rolls back, T2 has read invalid data\n",
    "\n",
    "**2. Read Committed**\n",
    "\n",
    "- Prevents dirty reads: Transaction can only read committed data\n",
    "- Example scenario:\n",
    "  - T1: Changes balance from 1000 to 500 and commits\n",
    "  - T2: Checks balance and sees 500 (only after T1 commits)\n",
    "  - T3: Transfer from A to C, deducting 200 and commits\n",
    "  - T4: Now sees balance of 300\n",
    "- **Issue**: Non-repeatable reads can occur\n",
    "\n",
    "**3. Repeatable Read**\n",
    "\n",
    "- Provides stable row reads within a transaction\n",
    "- Example scenario: Balance is 1000. Transaction T1 starts, then Transaction T2 transfers 500 and commits. If T1 checks the balance again, it still sees 1000 (the value when T1 started).\n",
    "- Uses shared locks (S-lock) to prevent modifications\n",
    "- **Issue**: Phantom reads can still occur (new rows can appear)\n",
    "\n",
    "**4. Serializable (Highest isolation)**\n",
    "\n",
    "- Strongest isolation level: Full isolation from concurrent transactions\n",
    "- Transactions execute as if they were running serially (one after another)\n",
    "- Uses various locking mechanisms:\n",
    "  - Shared locks (S-lock): Multiple transactions can read the same data\n",
    "  - Exclusive locks (X-lock): Only one transaction can modify data\n",
    "  - Range locks: Prevents phantom reads\n",
    "  - Predicate locks: Locks based on query conditions\n",
    "- **Trade-off**: Highest consistency but lowest concurrency\n",
    "\n",
    "### Durability\n",
    "\n",
    "Once a transaction is committed, the changes are permanent and will survive system failures. Data is persisted to disk and can be recovered even after crashes.\n",
    "\n",
    "**Example**: After a successful bank transfer commits, the transaction is saved even if the database server crashes immediately afterward.\n",
    "\n",
    "# Amazon Redshift\n",
    "\n",
    "## Overview\n",
    "\n",
    "Amazon Redshift is a fully managed, cloud-based data warehousing service designed for processing and analyzing large volumes of data. It uses a columnar storage architecture optimized for OLAP (Online Analytical Processing) workloads.\n",
    "\n",
    "## Key Characteristics\n",
    "\n",
    "- **Fully managed**: AWS handles infrastructure, maintenance, and updates\n",
    "- **Cloud-based**: Scalable and accessible from anywhere\n",
    "- **Columnar storage**: Data is stored by columns rather than rows\n",
    "- **Optimized for OLAP**: Designed for analytical queries, not transactional workloads\n",
    "- **Cost-effective**: Pay for what you use with various pricing options\n",
    "- **High performance**: MPP architecture enables fast query processing\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "### Primary Use Cases\n",
    "\n",
    "1. **Accelerate analytics workloads**: Fast processing of complex analytical queries\n",
    "2. **Unified data warehouses and data lakes**: Combine structured and semi-structured data\n",
    "3. **Data warehouse modernization**: Migrate from legacy on-premises data warehouses\n",
    "4. **Analyze global sales data**: Process sales data from multiple regions and sources\n",
    "5. **Store historical stock trade data**: Maintain long-term financial data for analysis\n",
    "6. **Analyze ad impressions and clicks**: Process digital advertising metrics\n",
    "7. **Aggregate gaming data**: Analyze player behavior and game performance\n",
    "8. **Analyze social trends**: Process social media data for insights\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### MPP (Massively Parallel Processing) Architecture\n",
    "\n",
    "Redshift uses an MPP architecture that distributes data and query processing across multiple nodes.\n",
    "\n",
    "#### Components\n",
    "\n",
    "**1. Leader Node**\n",
    "\n",
    "- Coordinates query execution across compute nodes\n",
    "- Manages client connections via JDBC/ODBC\n",
    "- Parses and optimizes SQL queries\n",
    "- Aggregates results from compute nodes\n",
    "- Does not store user data\n",
    "\n",
    "**2. Compute Nodes**\n",
    "\n",
    "- Execute queries in parallel\n",
    "- Store actual data in node slices\n",
    "- Each compute node is divided into multiple slices\n",
    "- Process their portion of data independently\n",
    "\n",
    "**3. Node Slices**\n",
    "\n",
    "- Logical partitions within compute nodes\n",
    "- Each slice processes a portion of the workload\n",
    "- Data is distributed across slices for parallel processing\n",
    "\n",
    "#### How MPP Works with Columnar Storage\n",
    "\n",
    "Consider this query:\n",
    "\n",
    "```text\n",
    "SELECT country, SUM(price)\n",
    "FROM orders\n",
    "GROUP BY country\n",
    "```\n",
    "\n",
    "With MPP and columnar storage:\n",
    "\n",
    "1. Query is sent to the leader node\n",
    "2. Leader node distributes work to compute nodes\n",
    "3. Each node processes its data slices in parallel\n",
    "4. Only the `country` and `price` columns are scanned (columnar benefit)\n",
    "5. Partial aggregations happen on each node\n",
    "6. Leader node combines results and returns final output\n",
    "\n",
    "This architecture enables:\n",
    "\n",
    "- **Parallel processing**: Multiple nodes work simultaneously\n",
    "- **Column-based scanning**: Only read necessary columns\n",
    "- **Efficient aggregations**: Distribute computation across nodes\n",
    "- **High performance**: Dramatically faster than single-server processing\n",
    "\n",
    "### Columnar Storage Benefits\n",
    "\n",
    "Traditional row-based storage stores data like:\n",
    "\n",
    "```text\n",
    "Row 1: [id, name, country, price, date]\n",
    "Row 2: [id, name, country, price, date]\n",
    "```\n",
    "\n",
    "Columnar storage stores data like:\n",
    "\n",
    "```text\n",
    "Column id: [val1, val2, val3, ...]\n",
    "Column name: [val1, val2, val3, ...]\n",
    "Column country: [val1, val2, val3, ...]\n",
    "Column price: [val1, val2, val3, ...]\n",
    "```\n",
    "\n",
    "**Advantages for Analytics:**\n",
    "\n",
    "1. **Reduced I/O**: Only read columns needed for the query\n",
    "2. **Better compression**: Similar data types compress more efficiently\n",
    "3. **Faster aggregations**: Process entire columns at once\n",
    "4. **Cache efficiency**: Better CPU cache utilization\n",
    "\n",
    "## Redshift Spectrum\n",
    "\n",
    "Redshift Spectrum extends Redshift's querying capabilities to data stored in Amazon S3 without loading it into the cluster.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Query exabytes of unstructured data in S3**: Access massive datasets directly\n",
    "2. **No data loading required**: Query data where it lives in S3\n",
    "3. **Limitless concurrency**: Scale query processing independently\n",
    "4. **Horizontal scaling**: Add processing capacity as needed\n",
    "5. **Separate storage and compute**: Scale each resource independently\n",
    "6. **Wide variety of data formats**: Support for:\n",
    "   - CSV\n",
    "   - JSON\n",
    "   - Parquet\n",
    "   - ORC\n",
    "   - Avro\n",
    "7. **Compression support**: Gzip and Snappy compression\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```text\n",
    "Redshift Cluster → Spectrum Layer → Amazon S3\n",
    "```\n",
    "\n",
    "Spectrum nodes handle the processing of S3 data separately from the main cluster, allowing you to analyze data in S3 without moving it.\n",
    "\n",
    "## Durability and Scaling\n",
    "\n",
    "### Data Durability\n",
    "\n",
    "**1. Replication within Cluster**\n",
    "\n",
    "- Data is automatically replicated across nodes within the cluster\n",
    "- Provides redundancy in case of node failure\n",
    "- Enables quick recovery\n",
    "\n",
    "**2. Backup to S3**\n",
    "\n",
    "- Automated backups stored in Amazon S3\n",
    "- Backups are asynchronously replicated to another AWS region\n",
    "- Provides disaster recovery capabilities\n",
    "- Cross-region replication protects against regional failures\n",
    "\n",
    "**3. Automated Snapshots for Disaster Recovery**\n",
    "\n",
    "- Automatic snapshots taken periodically\n",
    "- Manual snapshots can be created on demand\n",
    "- Snapshots stored in S3 for durability\n",
    "- Can restore cluster from any snapshot\n",
    "\n",
    "### Scaling Options\n",
    "\n",
    "**1. Vertical Scaling**\n",
    "\n",
    "- Increase or decrease node size (compute power and memory)\n",
    "- Change node types (e.g., from dc2.large to dc2.8xlarge)\n",
    "\n",
    "**2. Horizontal Scaling**\n",
    "\n",
    "- Add or remove compute nodes\n",
    "- Increase storage capacity and processing power\n",
    "- Distribute workload across more nodes\n",
    "\n",
    "**Scaling Process:**\n",
    "\n",
    "When scaling operations occur:\n",
    "\n",
    "1. New cluster is provisioned with desired configuration\n",
    "2. Data is copied to new cluster\n",
    "3. CNAME (DNS record) is flipped to point to new cluster\n",
    "4. Minimal downtime during cutover\n",
    "5. Old cluster is terminated\n",
    "\n",
    "## Data Distribution Strategies\n",
    "\n",
    "Data distribution determines how data is distributed across compute nodes and slices. Choosing the right distribution strategy is critical for performance.\n",
    "\n",
    "### Distribution Styles\n",
    "\n",
    "**1. AUTO Distribution**\n",
    "\n",
    "- Redshift automatically determines the best distribution strategy\n",
    "- Based on table size and query patterns\n",
    "- Good default choice when unsure\n",
    "- May change over time as data grows\n",
    "\n",
    "**2. EVEN Distribution**\n",
    "\n",
    "- Rows distributed across slices in round-robin fashion\n",
    "- Each slice gets approximately the same number of rows\n",
    "- Good for tables not joined with other tables\n",
    "- Simple and balanced distribution\n",
    "\n",
    "**When to use EVEN:**\n",
    "\n",
    "- Tables that are not frequently joined\n",
    "- Small dimension tables\n",
    "- Temporary staging tables\n",
    "\n",
    "**3. KEY Distribution**\n",
    "\n",
    "- Rows distributed based on values in one column (distribution key)\n",
    "- Rows with the same key value are stored on the same slice\n",
    "- Enables collocated joins (joins without data movement)\n",
    "- Critical for large fact tables\n",
    "\n",
    "**When to use KEY:**\n",
    "\n",
    "- Large fact tables joined with dimension tables\n",
    "- Tables frequently joined on a specific column\n",
    "- Example: Distribute both `orders` and `order_items` by `order_id`\n",
    "\n",
    "**4. ALL Distribution**\n",
    "\n",
    "- Entire table is copied to every compute node\n",
    "- Each node has a complete copy of the table\n",
    "- Typically used for small dimension tables\n",
    "- Eliminates need to broadcast data during joins\n",
    "\n",
    "**When to use ALL:**\n",
    "\n",
    "- Small dimension tables (under a few million rows)\n",
    "- Tables frequently joined with large tables\n",
    "- Read-heavy tables where storage duplication is acceptable\n",
    "- Example: Country codes, product categories\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "**Distribution Key (KEY):**\n",
    "\n",
    "- Data distributed by key value to specific locations\n",
    "- Same keys go to same slices\n",
    "- Enables efficient joins\n",
    "\n",
    "**ALL Distribution:**\n",
    "\n",
    "- Complete table copy on every node\n",
    "- No data movement needed for joins\n",
    "- Higher storage cost\n",
    "\n",
    "**EVEN Distribution:**\n",
    "\n",
    "- Round-robin distribution\n",
    "- Balanced across all slices\n",
    "- No optimization for joins\n",
    "\n",
    "## Sort Keys\n",
    "\n",
    "Sort keys determine the physical order in which data is stored in Redshift. Properly chosen sort keys dramatically improve query performance.\n",
    "\n",
    "### Compound Sort Key\n",
    "\n",
    "A compound sort key consists of multiple columns defined in a specific order. Data is sorted by the first column, then by the second column within each value of the first column, and so on.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "SORTKEY(customer_id, country_id)\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Data is first sorted by `customer_id`\n",
    "2. Within each `customer_id`, data is sorted by `country_id`\n",
    "3. Order matters: First column has highest priority\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Queries frequently filter on the first column\n",
    "- Range queries on the first column\n",
    "- Queries that filter on multiple columns in order\n",
    "- Example: `WHERE customer_id = 123 AND country_id = 'US'`\n",
    "\n",
    "**Best practices:**\n",
    "\n",
    "- Put most frequently filtered column first\n",
    "- Use date columns if queries often filter by date ranges\n",
    "- Columns used in ORDER BY clauses\n",
    "\n",
    "**Example query that benefits:**\n",
    "\n",
    "```text\n",
    "SELECT * \n",
    "FROM orders \n",
    "WHERE customer_id = 123 \n",
    "AND country_id = 'US'\n",
    "GROUP BY customer_id, country_id\n",
    "```\n",
    "\n",
    "### Interleaved Sort Key\n",
    "\n",
    "An interleaved sort key includes multiple columns where all columns are treated with equal importance. Redshift interleaves the data based on all columns simultaneously.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "INTERLEAVED SORTKEY(customer_id, country_id, order_date)\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- All columns in the sort key are weighted equally\n",
    "- Data is distributed more evenly across all sort key columns\n",
    "- Creates a balanced distribution suitable for multiple query patterns\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Queries filter on different combinations of columns\n",
    "- No single column is always used in queries\n",
    "- Wide range of filter conditions\n",
    "- Multiple query patterns with different predicates\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "- More expensive to maintain than compound sort keys\n",
    "- VACUUM operations take longer\n",
    "- Best for read-heavy workloads with diverse query patterns\n",
    "\n",
    "**Example queries that benefit:**\n",
    "\n",
    "```text\n",
    "-- Query 1: Filter by customer\n",
    "SELECT * FROM orders WHERE customer_id = 123\n",
    "\n",
    "-- Query 2: Filter by country\n",
    "SELECT * FROM orders WHERE country_id = 'US'\n",
    "\n",
    "-- Query 3: Filter by date\n",
    "SELECT * FROM orders WHERE order_date > '2024-01-01'\n",
    "```\n",
    "\n",
    "With interleaved sort keys, all three queries perform well.\n",
    "\n",
    "### Compound vs Interleaved Comparison\n",
    "\n",
    "| Aspect | Compound Sort Key | Interleaved Sort Key |\n",
    "|--------|------------------|---------------------|\n",
    "| Column priority | First column most important | All columns equally important |\n",
    "| Query patterns | Predictable, consistent filters | Varied, unpredictable filters |\n",
    "| Maintenance cost | Lower | Higher |\n",
    "| VACUUM time | Faster | Slower |\n",
    "| Best for | Single access pattern | Multiple access patterns |\n",
    "\n",
    "## Data Import and Export\n",
    "\n",
    "### COPY Command\n",
    "\n",
    "The COPY command is the primary and most efficient way to load data into Redshift.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Parallelized loading**: Distributes load across all nodes\n",
    "2. **Highly efficient**: Optimized for bulk data loading\n",
    "3. **Compression support**: Automatically detects and decompresses data\n",
    "4. **Error handling**: Can skip errors and log bad records\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- **Amazon S3**: Most common source\n",
    "- **Amazon EMR**: Load from Hadoop clusters\n",
    "- **Amazon DynamoDB**: Import NoSQL data\n",
    "- **Remote hosts**: SSH connection to external servers\n",
    "\n",
    "**S3-Specific Requirements:**\n",
    "\n",
    "- Requires IAM Role with appropriate S3 permissions\n",
    "- Manifest file recommended for large-scale loads\n",
    "- Can load from multiple files in parallel\n",
    "\n",
    "**Example COPY command:**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 's3://bucket-name/prefix/'\n",
    "IAM_ROLE 'arn:aws:iam::account-id:role/role-name'\n",
    "FORMAT AS PARQUET;\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "- Split data into multiple files for parallel loading\n",
    "- Use compressed files (gzip, bzip2)\n",
    "- Use columnar formats (Parquet, ORC) when possible\n",
    "- Specify proper delimiters and escape characters\n",
    "\n",
    "### INSERT INTO ... SELECT\n",
    "\n",
    "Load data from existing Redshift tables or query results.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Useful for transforming data already in Redshift\n",
    "- Less efficient than COPY for large datasets\n",
    "- Good for small data movements or transformations\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "INSERT INTO target_table (col1, col2, col3)\n",
    "SELECT col1, col2, col3\n",
    "FROM source_table\n",
    "WHERE condition;\n",
    "```\n",
    "\n",
    "### UNLOAD Command\n",
    "\n",
    "Export data from Redshift to Amazon S3.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Parallel unload**: Uses all nodes for fast export\n",
    "2. **Compression support**: Can compress output files\n",
    "3. **Encryption**: Supports encryption at rest\n",
    "4. **Partitioning**: Split data into multiple files\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "UNLOAD ('SELECT * FROM table_name')\n",
    "TO 's3://bucket-name/prefix/'\n",
    "IAM_ROLE 'arn:aws:iam::account-id:role/role-name'\n",
    "PARALLEL ON\n",
    "GZIP;\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Archiving data\n",
    "- Sharing data with other systems\n",
    "- Creating backups\n",
    "- Exporting for machine learning pipelines\n",
    "\n",
    "### Auto-Copy from Amazon S3\n",
    "\n",
    "Automatically load new files as they arrive in S3.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Event-driven ingestion\n",
    "- No manual intervention required\n",
    "- Commonly used with streaming pipelines\n",
    "- Integration with AWS Lambda for triggering\n",
    "\n",
    "**Typical Architecture:**\n",
    "\n",
    "```text\n",
    "Data Source → S3 → S3 Event → Lambda → Redshift COPY\n",
    "```\n",
    "\n",
    "### Amazon Aurora Zero-ETL Integration\n",
    "\n",
    "Seamlessly replicate data from Aurora to Redshift without custom ETL code.\n",
    "\n",
    "**Data Flow:**\n",
    "\n",
    "```text\n",
    "Aurora → S3 → Glue Crawler → Glue Data Catalog → Athena\n",
    "```\n",
    "\n",
    "**Optional Path:**\n",
    "\n",
    "```text\n",
    "Aurora → Direct Replication → Redshift\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- No ETL code to maintain\n",
    "- Near real-time data availability\n",
    "- Automatic schema synchronization\n",
    "- Reduced operational overhead\n",
    "\n",
    "### Redshift Streaming Ingestion\n",
    "\n",
    "Ingest data in near real-time from streaming sources.\n",
    "\n",
    "**Supported Sources:**\n",
    "\n",
    "1. **Amazon Kinesis Data Streams**: High-throughput data streaming\n",
    "2. **Amazon MSK (Managed Streaming for Apache Kafka)**: Kafka-compatible streaming\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Real-time analytics\n",
    "- IoT data processing\n",
    "- Log analysis\n",
    "- Clickstream analysis\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```text\n",
    "Application → Kinesis/MSK → Redshift Streaming Ingestion → Redshift Tables\n",
    "```\n",
    "\n",
    "## DBLink - PostgreSQL Connection\n",
    "\n",
    "DBLink allows connecting Redshift to PostgreSQL databases for data synchronization and cross-database querying.\n",
    "\n",
    "**Common Use Cases:**\n",
    "\n",
    "1. **Incremental data sync**: Sync only changed data\n",
    "2. **Small-to-medium reference tables**: Copy dimension tables\n",
    "3. **Cross-database querying**: Query PostgreSQL from Redshift\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "**1. Enable Required Extensions:**\n",
    "\n",
    "```text\n",
    "CREATE EXTENSION postgres_fdw;\n",
    "CREATE EXTENSION dblink;\n",
    "```\n",
    "\n",
    "**2. Create Foreign Server:**\n",
    "\n",
    "```text\n",
    "CREATE SERVER foreign_server\n",
    "FOREIGN DATA WRAPPER postgres_fdw\n",
    "OPTIONS (\n",
    "    host '<postgres_host>',\n",
    "    port '<port>',\n",
    "    dbname '<database_name>',\n",
    "    sslmode 'require'\n",
    ");\n",
    "```\n",
    "\n",
    "**3. Create User Mapping:**\n",
    "\n",
    "```text\n",
    "CREATE USER MAPPING FOR <redshift_user>\n",
    "SERVER foreign_server\n",
    "OPTIONS (\n",
    "    user '<postgres_username>',\n",
    "    password '<password>'\n",
    ");\n",
    "```\n",
    "\n",
    "**4. Query Remote Data:**\n",
    "\n",
    "```text\n",
    "SELECT * FROM dblink(\n",
    "    'foreign_server',\n",
    "    'SELECT * FROM remote_table'\n",
    ") AS remote_data(col1 int, col2 text);\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Syncing reference data from operational databases\n",
    "- Validating data between systems\n",
    "- Temporary data access without full ETL\n",
    "\n",
    "## VACUUM\n",
    "\n",
    "VACUUM is a maintenance operation that reclaims disk space and reorganizes data for better performance.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "1. **Reclaim disk space**: Remove space from deleted or updated rows\n",
    "2. **Reorganize data**: Improve physical data layout\n",
    "3. **Update statistics**: Refresh table metadata for query optimization\n",
    "4. **Improve query performance**: Maintain optimal data organization\n",
    "\n",
    "### VACUUM Types\n",
    "\n",
    "**1. VACUUM FULL**\n",
    "\n",
    "- Most comprehensive vacuum operation\n",
    "- Reclaims space and resorts data\n",
    "- Can be time-consuming\n",
    "- Acquires exclusive table lock\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "VACUUM FULL table_name;\n",
    "```\n",
    "\n",
    "**2. VACUUM DELETE ONLY**\n",
    "\n",
    "- Only reclaims space from deleted rows\n",
    "- Does not resort data\n",
    "- Faster than VACUUM FULL\n",
    "- Good for tables with frequent deletes\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "VACUUM DELETE ONLY table_name;\n",
    "```\n",
    "\n",
    "**3. VACUUM SORT ONLY**\n",
    "\n",
    "- Only resorts data according to sort key\n",
    "- Does not reclaim space\n",
    "- Improves query performance\n",
    "- Good after bulk loads\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "VACUUM SORT ONLY table_name;\n",
    "```\n",
    "\n",
    "**4. VACUUM REINDEX**\n",
    "\n",
    "- Rebuilds interleaved sort key indexes\n",
    "- Necessary for tables with interleaved sort keys\n",
    "- Maintains query performance over time\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```text\n",
    "VACUUM REINDEX table_name;\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Run VACUUM during low-traffic periods\n",
    "- Use VACUUM DELETE ONLY for frequent delete operations\n",
    "- Run VACUUM SORT ONLY after large data loads\n",
    "- Schedule automatic vacuum operations\n",
    "- Monitor vacuum progress and completion\n",
    "\n",
    "## When NOT to Use Redshift\n",
    "\n",
    "### 1. Small Datasets\n",
    "\n",
    "**Issue**: Redshift is optimized for large-scale analytics\n",
    "\n",
    "**Better Alternative**: Amazon RDS (PostgreSQL, MySQL, etc.)\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "- Redshift has overhead for cluster management\n",
    "- RDS is simpler and more cost-effective for small data\n",
    "- RDS provides better performance for small datasets\n",
    "\n",
    "### 2. OLTP Workloads\n",
    "\n",
    "**Issue**: Redshift is designed for OLAP, not OLTP\n",
    "\n",
    "**Better Alternatives**:\n",
    "\n",
    "- Amazon RDS for relational OLTP\n",
    "- Amazon DynamoDB for NoSQL OLTP\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "- Redshift lacks optimizations for transactional workloads\n",
    "- Not designed for high-frequency individual updates\n",
    "- Better suited for batch processing and analytics\n",
    "\n",
    "### 3. Unstructured Data\n",
    "\n",
    "**Issue**: Redshift requires structured or semi-structured data\n",
    "\n",
    "**Better Approach**: Perform ETL first\n",
    "\n",
    "**ETL Tools**:\n",
    "\n",
    "- Amazon EMR\n",
    "- Apache Spark\n",
    "- Apache Airflow\n",
    "\n",
    "**Process:**\n",
    "\n",
    "```text\n",
    "Raw Unstructured Data → ETL (EMR/Spark) → Structured Data → Redshift\n",
    "```\n",
    "\n",
    "### 4. BLOB (Binary Large Object) Data\n",
    "\n",
    "**Issue**: Storing large binary files in Redshift is inefficient\n",
    "\n",
    "**Better Approach**: Store in S3, reference in Redshift\n",
    "\n",
    "**Recommended Pattern:**\n",
    "\n",
    "- Store files in Amazon S3\n",
    "- Store only metadata in Redshift (file paths, IDs, descriptions)\n",
    "- Query metadata in Redshift, retrieve files from S3\n",
    "\n",
    "**Example Schema:**\n",
    "\n",
    "```text\n",
    "CREATE TABLE documents (\n",
    "    document_id INT,\n",
    "    document_name VARCHAR(255),\n",
    "    s3_path VARCHAR(500),  -- S3 location\n",
    "    file_size BIGINT,\n",
    "    upload_date DATE\n",
    ");\n",
    "```\n",
    "\n",
    "## Redshift Serverless\n",
    "\n",
    "Redshift Serverless eliminates the need to manage clusters manually.\n",
    "\n",
    "### Pricing Model\n",
    "\n",
    "**Redshift Processing Units (RPUs)**:\n",
    "\n",
    "- Unit of compute capacity in Redshift Serverless\n",
    "- Billing based on RPU-hours\n",
    "- Charged per second of usage\n",
    "- Plus storage costs\n",
    "\n",
    "### Base RPUs Configuration\n",
    "\n",
    "**Capacity Range**: 32 to 512 RPUs\n",
    "\n",
    "**Factors to Consider:**\n",
    "\n",
    "1. **Query Complexity**: More complex queries need more RPUs\n",
    "2. **Concurrency Requirements**: More concurrent users need more RPUs\n",
    "3. **Performance Requirements**: Lower latency needs more RPUs\n",
    "4. **Cost Optimization**: Balance performance with cost\n",
    "\n",
    "**Scaling Behavior:**\n",
    "\n",
    "- Automatically scales up during high demand\n",
    "- Scales down during low demand\n",
    "- Base RPUs set the minimum capacity\n",
    "- Maximum capacity can be configured\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- No cluster management\n",
    "- Automatic scaling\n",
    "- Pay only for what you use\n",
    "- Simplified operations\n",
    "\n",
    "# Amazon DynamoDB\n",
    "\n",
    "## Overview\n",
    "\n",
    "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "1. **Serverless**: No servers to provision or manage\n",
    "2. **Non-relational**: NoSQL database model\n",
    "3. **Fully distributed**: Data replicated across multiple AZs\n",
    "4. **Low cost**: Pay-per-request pricing with automatic scaling\n",
    "5. **High performance**: Single-digit millisecond latency\n",
    "6. **Flexible schema**: No rigid schema requirements\n",
    "\n",
    "### Table Classes\n",
    "\n",
    "**1. Standard Table Class**\n",
    "\n",
    "- For frequently accessed data\n",
    "- Higher throughput\n",
    "- Standard pricing\n",
    "\n",
    "**2. Infrequent Access (IA) Table Class**\n",
    "\n",
    "- For rarely accessed data\n",
    "- Lower storage costs\n",
    "- Slightly higher per-request costs\n",
    "- Cost-effective for archival data\n",
    "\n",
    "## Data Model\n",
    "\n",
    "### Database Structure\n",
    "\n",
    "DynamoDB has a flat structure: **Database → Table** (No schema layer)\n",
    "\n",
    "**Key Differences from Relational Databases:**\n",
    "\n",
    "- No database/schema hierarchy like RDS\n",
    "- Tables exist at the top level\n",
    "- No foreign keys or relationships enforced by DynamoDB\n",
    "- Schema-on-read rather than schema-on-write\n",
    "\n",
    "### Table Components\n",
    "\n",
    "**Tables:**\n",
    "\n",
    "- Collection of items\n",
    "- Each table requires a primary key definition\n",
    "- No limit on number of tables per account\n",
    "\n",
    "**Items (Rows):**\n",
    "\n",
    "- Individual records in a table\n",
    "- Equivalent to rows in relational databases\n",
    "- Each table can have unlimited number of items\n",
    "- Maximum size per item: 400 KB\n",
    "\n",
    "**Attributes (Columns):**\n",
    "\n",
    "- Individual data fields within an item\n",
    "- Equivalent to columns in relational databases\n",
    "- Each item can have different attributes (flexible schema)\n",
    "- Only primary key attributes are required\n",
    "\n",
    "### Data Types\n",
    "\n",
    "**1. Scalar Types (Single Value)**\n",
    "\n",
    "- **String**: Text data (UTF-8 encoded)\n",
    "- **Number**: Numeric data (integers, decimals)\n",
    "- **Binary**: Binary data (images, compressed data)\n",
    "- **Boolean**: True or false\n",
    "- **Null**: Represents absence of value\n",
    "\n",
    "**2. Document Types (Nested Structures)**\n",
    "\n",
    "- **List**: Ordered collection of values (like arrays)\n",
    "  - Example: `[\"apple\", \"banana\", \"orange\"]`\n",
    "- **Map**: Unordered collection of key-value pairs (like JSON objects)\n",
    "  - Example: `{\"name\": \"John\", \"age\": 30}`\n",
    "\n",
    "**3. Set Types (Unordered Collections)**\n",
    "\n",
    "- **String Set**: Collection of unique strings\n",
    "  - Example: `{\"red\", \"blue\", \"green\"}`\n",
    "- **Number Set**: Collection of unique numbers\n",
    "  - Example: `{1, 2, 3, 5, 8}`\n",
    "- **Binary Set**: Collection of unique binary values\n",
    "\n",
    "### Example Item Structure\n",
    "\n",
    "```text\n",
    "{\n",
    "    \"user_id\": \"123\",              // String (Partition Key)\n",
    "    \"username\": \"john_doe\",        // String\n",
    "    \"age\": 30,                     // Number\n",
    "    \"email\": \"john@example.com\",   // String\n",
    "    \"active\": true,                // Boolean\n",
    "    \"tags\": [\"developer\", \"aws\"],  // List\n",
    "    \"preferences\": {               // Map\n",
    "        \"theme\": \"dark\",\n",
    "        \"language\": \"en\"\n",
    "    },\n",
    "    \"skills\": {\"Python\", \"SQL\"}    // String Set\n",
    "}\n",
    "```\n",
    "\n",
    "## Primary Keys\n",
    "\n",
    "Primary keys uniquely identify items in a DynamoDB table. There are two types of primary keys.\n",
    "\n",
    "### 1. Partition Key (HASH Key)\n",
    "\n",
    "A simple primary key consisting of a single attribute.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Single attribute serves as unique identifier\n",
    "- Every item must have a unique partition key value\n",
    "- No two items can have the same partition key\n",
    "- DynamoDB uses partition key to determine physical storage location\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "Table: Users\n",
    "Partition Key: user_id\n",
    "\n",
    "Items:\n",
    "- user_id: \"123\" (unique)\n",
    "- user_id: \"456\" (unique)\n",
    "- user_id: \"789\" (unique)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Simple access patterns (get item by ID)\n",
    "- Items naturally have a unique identifier\n",
    "- No need to query ranges of items\n",
    "\n",
    "### 2. Composite Primary Key (Partition Key + Sort Key)\n",
    "\n",
    "A primary key composed of two attributes: partition key and sort key.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Partition key determines which partition stores the item\n",
    "- Sort key determines order within the partition\n",
    "- Multiple items can share the same partition key\n",
    "- Items with same partition key must have different sort key values\n",
    "- Enables range queries on sort key\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "Table: Orders\n",
    "Partition Key: customer_id\n",
    "Sort Key: order_date\n",
    "\n",
    "Items:\n",
    "- customer_id: \"123\", order_date: \"2024-01-01\"\n",
    "- customer_id: \"123\", order_date: \"2024-01-15\"\n",
    "- customer_id: \"456\", order_date: \"2024-01-10\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- One-to-many relationships (one customer, many orders)\n",
    "- Need to query ranges (all orders for a customer in a date range)\n",
    "- Need to sort results by a specific attribute\n",
    "\n",
    "### Primary Key Design Considerations\n",
    "\n",
    "**Partition Key Selection:**\n",
    "\n",
    "- Choose high-cardinality attributes (many unique values)\n",
    "- Ensure even distribution of data\n",
    "- Avoid hot partitions (one partition getting too much traffic)\n",
    "- Consider access patterns\n",
    "\n",
    "**Sort Key Selection:**\n",
    "\n",
    "- Choose attributes frequently used in range queries\n",
    "- Consider how data should be ordered\n",
    "- Often timestamps, dates, or sequential IDs\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "DynamoDB automatically partitions data across multiple physical storage locations based on the partition key.\n",
    "\n",
    "### How Partitioning Works\n",
    "\n",
    "**Hash Function:**\n",
    "\n",
    "1. DynamoDB applies a hash function to the partition key\n",
    "2. Hash determines which partition stores the item\n",
    "3. Hash ensures even distribution across partitions\n",
    "\n",
    "**Partition Distribution:**\n",
    "\n",
    "```text\n",
    "Item with partition key \"user_123\"\n",
    "    ↓ (hash function)\n",
    "Partition 2\n",
    "\n",
    "Item with partition key \"user_456\"\n",
    "    ↓ (hash function)\n",
    "Partition 1\n",
    "\n",
    "Item with partition key \"user_789\"\n",
    "    ↓ (hash function)\n",
    "Partition 3\n",
    "```\n",
    "\n",
    "### Hot Partitions\n",
    "\n",
    "A **hot partition** occurs when one partition receives disproportionately high traffic compared to others.\n",
    "\n",
    "**Causes:**\n",
    "\n",
    "1. **Poor partition key design**: Using low-cardinality attributes\n",
    "   - Example: Using \"country\" as partition key when 90% of users are from one country\n",
    "2. **Time-series data without proper design**: All recent data goes to same partition\n",
    "   - Example: Using current date as partition key for real-time data\n",
    "3. **Celebrity/popular item problem**: One item accessed much more than others\n",
    "   - Example: Viral post in social media application\n",
    "\n",
    "**Problems:**\n",
    "\n",
    "- Performance degradation\n",
    "- Request throttling\n",
    "- Uneven resource utilization\n",
    "- Increased latency\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Use high-cardinality partition keys**: Ensure many unique values\n",
    "2. **Add randomness**: Append random suffix to partition key\n",
    "3. **Composite keys**: Combine multiple attributes\n",
    "4. **Write sharding**: Distribute writes across multiple partition key values\n",
    "\n",
    "## When NOT to Use DynamoDB\n",
    "\n",
    "### 1. Pre-written Applications Tied to Relational Databases\n",
    "\n",
    "**Issue**: Application designed for SQL and relational model\n",
    "\n",
    "**Better Alternative**: Amazon RDS\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "- Application expects SQL query language\n",
    "- Relies on joins, foreign keys, transactions\n",
    "- Schema already defined in relational model\n",
    "- Migration cost too high\n",
    "\n",
    "### 2. Complex Joins\n",
    "\n",
    "**Issue**: DynamoDB does not support joins between tables\n",
    "\n",
    "**Better Alternative**: Amazon RDS\n",
    "\n",
    "**How DynamoDB Handles Relationships:**\n",
    "\n",
    "- Denormalize data (embed related data in single item)\n",
    "- Multiple queries (application-level joins)\n",
    "- Use adjacency list pattern\n",
    "- Store related data in single table\n",
    "\n",
    "**When RDS is Better:**\n",
    "\n",
    "- Many-to-many relationships\n",
    "- Complex multi-table joins\n",
    "- Normalized data model required\n",
    "- Ad-hoc analytical queries\n",
    "\n",
    "### 3. Complex Transactions\n",
    "\n",
    "**Issue**: Limited transaction support\n",
    "\n",
    "**Better Alternative**: Amazon RDS\n",
    "\n",
    "**DynamoDB Transaction Limitations:**\n",
    "\n",
    "- Maximum 100 items per transaction\n",
    "- All items must be in same region\n",
    "- Performance impact for large transactions\n",
    "- Additional cost per transaction\n",
    "\n",
    "**When RDS is Better:**\n",
    "\n",
    "- Multi-step workflows requiring ACID\n",
    "- Complex business logic in transactions\n",
    "- Need for isolation levels\n",
    "- Rollback requirements across many tables\n",
    "\n",
    "### 4. BLOB Data\n",
    "\n",
    "**Issue**: 400 KB item size limit\n",
    "\n",
    "**Better Solution**: Store in S3, metadata in DynamoDB\n",
    "\n",
    "**Recommended Pattern:**\n",
    "\n",
    "```text\n",
    "DynamoDB Table: Files\n",
    "- file_id (partition key)\n",
    "- file_name\n",
    "- s3_bucket\n",
    "- s3_key\n",
    "- file_size\n",
    "- upload_date\n",
    "\n",
    "Actual file stored in S3: s3://bucket/path/to/file\n",
    "```\n",
    "\n",
    "### 5. Large Data with Low I/O Rate\n",
    "\n",
    "**Issue**: DynamoDB optimized for high-throughput access\n",
    "\n",
    "**Better Alternative**: Amazon S3\n",
    "\n",
    "**When to Use S3:**\n",
    "\n",
    "- Large files (videos, backups, archives)\n",
    "- Infrequent access\n",
    "- Data lake scenarios\n",
    "- Long-term archival\n",
    "\n",
    "**Cost Comparison:**\n",
    "\n",
    "- S3: Much cheaper storage, but higher latency\n",
    "- DynamoDB: More expensive storage, low latency access\n",
    "\n",
    "## Capacity Units\n",
    "\n",
    "DynamoDB uses capacity units to measure and bill for throughput.\n",
    "\n",
    "### Write Capacity Unit (WCU)\n",
    "\n",
    "**Definition**: One write per second for an item up to 1 KB in size\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "- Item size ≤ 1 KB = 1 WCU\n",
    "- Item size > 1 KB = Rounded up to nearest KB\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**Example 1: Write 10 items per second, item size 2 KB**\n",
    "\n",
    "```text\n",
    "Item size = 2 KB → 2 WCUs per item\n",
    "10 items/second × 2 WCUs = 20 WCUs required\n",
    "```\n",
    "\n",
    "**Example 2: Write 6 items per second, item size 4.5 KB**\n",
    "\n",
    "```text\n",
    "Item size = 4.5 KB → Round up to 5 KB → 5 WCUs per item\n",
    "6 items/second × 5 WCUs = 30 WCUs required\n",
    "```\n",
    "\n",
    "### Read Capacity Unit (RCU)\n",
    "\n",
    "**Definition**: One RCU represents:\n",
    "\n",
    "- 1 strongly consistent read per second, OR\n",
    "- 2 eventually consistent reads per second\n",
    "- For an item up to 4 KB in size\n",
    "\n",
    "**Read Consistency:**\n",
    "\n",
    "**1. Strongly Consistent Read**\n",
    "\n",
    "- Returns most up-to-date data\n",
    "- Reflects all successful writes\n",
    "- Higher RCU cost (1 RCU per 4 KB)\n",
    "- Slightly higher latency\n",
    "\n",
    "**2. Eventually Consistent Read (Default)**\n",
    "\n",
    "- May return stale data\n",
    "- Eventually reflects all writes\n",
    "- Lower RCU cost (0.5 RCU per 4 KB)\n",
    "- Better performance\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "For **Strongly Consistent Reads**:\n",
    "\n",
    "- Item size ≤ 4 KB = 1 RCU\n",
    "- Item size > 4 KB = Round up to nearest 4 KB\n",
    "\n",
    "For **Eventually Consistent Reads**:\n",
    "\n",
    "- Item size ≤ 4 KB = 0.5 RCU\n",
    "- Item size > 4 KB = Round up to nearest 4 KB, then divide by 2\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**Example 1: Read 10 items per second, item size 3 KB, strongly consistent**\n",
    "\n",
    "```text\n",
    "Item size = 3 KB → 1 RCU per item\n",
    "10 items/second × 1 RCU = 10 RCUs required\n",
    "```\n",
    "\n",
    "**Example 2: Read 10 items per second, item size 3 KB, eventually consistent**\n",
    "\n",
    "```text\n",
    "Item size = 3 KB → 0.5 RCU per item\n",
    "10 items/second × 0.5 RCU = 5 RCUs required\n",
    "```\n",
    "\n",
    "**Example 3: Read 5 items per second, item size 10 KB, strongly consistent**\n",
    "\n",
    "```text\n",
    "Item size = 10 KB → Round up to 12 KB (nearest 4 KB) → 3 RCUs per item\n",
    "5 items/second × 3 RCUs = 15 RCUs required\n",
    "```\n",
    "\n",
    "# Interview Questions and Coding Exercise\n",
    "\n",
    "## Q1: When Should You NOT Use DynamoDB?\n",
    "\n",
    "**Detailed Answer:**\n",
    "\n",
    "You should NOT use DynamoDB in the following scenarios:\n",
    "\n",
    "**1. Pre-written Applications Tied to Relational Databases**\n",
    "\n",
    "When your application is already built with SQL queries, joins, and relational schema, migrating to DynamoDB requires complete application rewrite. Use RDS instead to maintain compatibility.\n",
    "\n",
    "**2. Complex Joins Required**\n",
    "\n",
    "If your data model requires frequent multi-table joins, DynamoDB's lack of join support becomes a significant limitation. While you can denormalize data or perform application-level joins, this adds complexity. RDS with SQL is better suited for such use cases.\n",
    "\n",
    "**3. Complex Transactions**\n",
    "\n",
    "DynamoDB supports transactions but with limitations (100 items max, single region, performance overhead). Applications requiring complex multi-step ACID transactions across many entities should use RDS.\n",
    "\n",
    "**4. BLOB Data Storage**\n",
    "\n",
    "DynamoDB has a 400 KB item size limit, making it unsuitable for storing large binary objects like images, videos, or documents. Instead, store BLOBs in S3 and keep only metadata (S3 paths, file names) in DynamoDB.\n",
    "\n",
    "**5. Large Data with Low I/O Rate**\n",
    "\n",
    "For infrequently accessed large datasets (archives, backups, data lakes), S3 provides much lower storage costs than DynamoDB. DynamoDB is optimized for high-throughput, low-latency access patterns.\n",
    "\n",
    "**6. Ad-hoc Analytical Queries**\n",
    "\n",
    "DynamoDB requires predefined access patterns and doesn't support flexible SQL queries. For business intelligence and ad-hoc analytics, use Redshift or Athena with S3.\n",
    "\n",
    "## Q2: What Causes a Hot Partition in DynamoDB?\n",
    "\n",
    "**Detailed Answer:**\n",
    "\n",
    "A hot partition occurs when one partition receives significantly more traffic than others, causing performance bottlenecks.\n",
    "\n",
    "**Primary Causes:**\n",
    "\n",
    "**1. Poor Partition Key Design**\n",
    "\n",
    "Using low-cardinality attributes that create uneven data distribution:\n",
    "\n",
    "- **Example**: Using \"country\" as partition key when 80% of users are from one country\n",
    "- **Example**: Using \"status\" (active/inactive) as partition key\n",
    "- **Impact**: Most requests go to same partition, causing throttling\n",
    "\n",
    "**2. Time-Series Data Without Proper Design**\n",
    "\n",
    "Using current timestamp or date as partition key:\n",
    "\n",
    "- **Example**: Partition key = current_date, all today's data goes to one partition\n",
    "- **Example**: IoT sensor data with timestamp as partition key\n",
    "- **Impact**: All writes concentrated on single partition\n",
    "\n",
    "**3. Celebrity/Popular Item Problem**\n",
    "\n",
    "One item receiving disproportionate access:\n",
    "\n",
    "- **Example**: Viral social media post getting millions of views\n",
    "- **Example**: Popular product during flash sale\n",
    "- **Impact**: Single partition overwhelmed while others idle\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Use High-Cardinality Partition Keys**\n",
    "\n",
    "Choose attributes with many unique values that distribute evenly:\n",
    "\n",
    "```text\n",
    "Good: user_id, order_id, transaction_id\n",
    "Bad: country, status, category\n",
    "```\n",
    "\n",
    "**2. Add Write Sharding**\n",
    "\n",
    "Append random suffix to partition key:\n",
    "\n",
    "```text\n",
    "Original: date = \"2024-01-01\"\n",
    "Sharded: date = \"2024-01-01#0\", \"2024-01-01#1\", \"2024-01-01#2\"\n",
    "```\n",
    "\n",
    "**3. Use Composite Keys**\n",
    "\n",
    "Combine multiple attributes to increase cardinality:\n",
    "\n",
    "```text\n",
    "Partition Key: customer_id\n",
    "Sort Key: timestamp\n",
    "```\n",
    "\n",
    "**4. Calculate Shard Number**\n",
    "\n",
    "Use hash of attribute to determine shard:\n",
    "\n",
    "```text\n",
    "shard = hash(item_id) % num_shards\n",
    "partition_key = f\"{item_id}#{shard}\"\n",
    "```\n",
    "\n",
    "## Q3: How Does DynamoDB Scale Automatically?\n",
    "\n",
    "**Detailed Answer:**\n",
    "\n",
    "DynamoDB provides two scaling modes: provisioned and on-demand.\n",
    "\n",
    "**1. Provisioned Capacity with Auto Scaling**\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Set target utilization (e.g., 70% of provisioned capacity)\n",
    "- DynamoDB monitors actual usage\n",
    "- When usage approaches target, automatically increases capacity\n",
    "- Scales down during low traffic periods\n",
    "\n",
    "**Scaling Triggers:**\n",
    "\n",
    "- CloudWatch alarms based on consumed capacity\n",
    "- Application Auto Scaling adjusts capacity\n",
    "- Gradual scaling to prevent sudden changes\n",
    "\n",
    "**Configuration:**\n",
    "\n",
    "```text\n",
    "Minimum RCU: 5\n",
    "Maximum RCU: 100\n",
    "Target Utilization: 70%\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "Current: 50 RCUs provisioned\n",
    "Usage: 40 RCUs consumed (80% utilization)\n",
    "Action: Scale up to 60 RCUs\n",
    "```\n",
    "\n",
    "**2. On-Demand Capacity Mode**\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- No capacity planning required\n",
    "- DynamoDB automatically handles any amount of traffic\n",
    "- Instantly accommodates workload spikes\n",
    "- Pay per request rather than provisioned capacity\n",
    "\n",
    "**Automatic Scaling:**\n",
    "\n",
    "- Instantaneous scaling up to 2x previous peak within 30 minutes\n",
    "- No throttling for gradual traffic increases\n",
    "- Handles unpredictable workloads\n",
    "\n",
    "**3. Partition Splitting**\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- DynamoDB automatically splits partitions when:\n",
    "  - Storage exceeds 10 GB per partition\n",
    "  - Throughput exceeds partition limits\n",
    "- New partitions created automatically\n",
    "- Data redistributed across partitions\n",
    "- Completely transparent to applications\n",
    "\n",
    "**Partition Lifecycle:**\n",
    "\n",
    "```text\n",
    "Single Partition (10 GB reached)\n",
    "    ↓\n",
    "Split into 2 Partitions (5 GB each)\n",
    "    ↓\n",
    "Continue splitting as needed\n",
    "```\n",
    "\n",
    "**4. Adaptive Capacity**\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Responds to uneven workload patterns\n",
    "- Isolates frequently accessed items\n",
    "- Boosts capacity for hot partitions\n",
    "- Prevents throttling from hot partitions\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Handles temporary spikes\n",
    "- Protects against hot key issues\n",
    "- Automatic without configuration\n",
    "\n",
    "## Q4: Difference Between RDS and DynamoDB?\n",
    "\n",
    "**Comprehensive Comparison:**\n",
    "\n",
    "### Data Model\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Relational (SQL)\n",
    "- Fixed schema with tables, rows, columns\n",
    "- Relationships enforced via foreign keys\n",
    "- Normalization encouraged\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Non-relational (NoSQL)\n",
    "- Flexible schema\n",
    "- No foreign keys\n",
    "- Denormalization encouraged\n",
    "\n",
    "### Query Language\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- SQL (Structured Query Language)\n",
    "- Complex joins supported\n",
    "- Rich query capabilities\n",
    "- Aggregations, subqueries, window functions\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Key-value API\n",
    "- No joins (application-level required)\n",
    "- Limited query capabilities\n",
    "- Query and Scan operations\n",
    "\n",
    "### Scaling\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Vertical scaling (larger instance)\n",
    "- Read replicas for read scaling\n",
    "- Manual intervention often required\n",
    "- Limited by instance size\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Horizontal scaling (automatic partitioning)\n",
    "- Unlimited scaling capacity\n",
    "- Automatic with no intervention\n",
    "- Distributed architecture\n",
    "\n",
    "### Transactions\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Full ACID compliance\n",
    "- Complex multi-table transactions\n",
    "- Multiple isolation levels\n",
    "- Rollback capabilities\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Limited transactions (up to 100 items)\n",
    "- ACID for supported operations\n",
    "- Single-region transactions\n",
    "- Higher latency for transactions\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- OLTP workloads\n",
    "- Complex queries and joins\n",
    "- Traditional applications\n",
    "- Business intelligence\n",
    "- Data warehousing (Aurora)\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- High-scale web applications\n",
    "- Gaming leaderboards\n",
    "- IoT data storage\n",
    "- Mobile backends\n",
    "- Session management\n",
    "\n",
    "### Performance\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Millisecond to second latency\n",
    "- Depends on query complexity\n",
    "- Limited by instance resources\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Single-digit millisecond latency\n",
    "- Consistent performance at scale\n",
    "- Predictable performance\n",
    "\n",
    "### Pricing\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Pay for instance hours\n",
    "- Storage separately charged\n",
    "- Backup storage\n",
    "- Data transfer\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Pay per request (on-demand)\n",
    "- Or provisioned capacity\n",
    "- Storage per GB\n",
    "- Lower cost at high scale\n",
    "\n",
    "### Management\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- More management required\n",
    "- Backups, patches, upgrades\n",
    "- Performance tuning\n",
    "- Replication configuration\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Fully managed serverless\n",
    "- Automatic backups\n",
    "- No maintenance windows\n",
    "- Hands-off operation\n",
    "\n",
    "### Consistency\n",
    "\n",
    "**RDS:**\n",
    "\n",
    "- Strongly consistent by default\n",
    "- Eventual consistency for read replicas\n",
    "\n",
    "**DynamoDB:**\n",
    "\n",
    "- Eventually consistent by default\n",
    "- Strongly consistent reads available\n",
    "- Choice per read operation\n",
    "\n",
    "## Q5: Explain How the COPY Command Works in Redshift\n",
    "\n",
    "**Detailed Answer:**\n",
    "\n",
    "The COPY command is the most efficient method for loading large amounts of data into Amazon Redshift.\n",
    "\n",
    "### How COPY Works\n",
    "\n",
    "**1. Parallel Loading**\n",
    "\n",
    "COPY distributes the workload across all compute nodes:\n",
    "\n",
    "```text\n",
    "S3 (Multiple Files)\n",
    "    ↓\n",
    "Leader Node (coordinates)\n",
    "    ↓\n",
    "Compute Node 1, Compute Node 2, Compute Node 3, ...\n",
    "(Each node loads different files in parallel)\n",
    "```\n",
    "\n",
    "**Process:**\n",
    "\n",
    "- Leader node identifies all files to load\n",
    "- Distributes file list across compute nodes\n",
    "- Each compute node reads and loads its assigned files\n",
    "- All nodes work simultaneously\n",
    "\n",
    "**2. Automatic Compression Detection**\n",
    "\n",
    "- Detects compressed files automatically\n",
    "- Supports gzip, bzip2, lzo, zstd\n",
    "- Decompresses during load\n",
    "- No manual intervention required\n",
    "\n",
    "**3. Data Type Conversion**\n",
    "\n",
    "- Automatically converts data types\n",
    "- Validates data against table schema\n",
    "- Handles NULL values\n",
    "- Parses delimited files\n",
    "\n",
    "**4. Error Handling**\n",
    "\n",
    "- Can continue loading despite errors\n",
    "- Logs failed records\n",
    "- Configurable error thresholds\n",
    "- Allows investigation of bad data\n",
    "\n",
    "### COPY Command Syntax\n",
    "\n",
    "**Basic Syntax:**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 's3://bucket/prefix/'\n",
    "IAM_ROLE 'arn:aws:iam::account-id:role/role-name'\n",
    "[FORMAT]\n",
    "[OPTIONS];\n",
    "```\n",
    "\n",
    "**Common Options:**\n",
    "\n",
    "```text\n",
    "COPY orders\n",
    "FROM 's3://my-bucket/data/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftRole'\n",
    "FORMAT AS CSV\n",
    "DELIMITER ','\n",
    "IGNOREHEADER 1\n",
    "REGION 'us-east-1'\n",
    "MAXERROR 10;\n",
    "```\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "**1. Amazon S3:**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 's3://bucket/prefix/'\n",
    "IAM_ROLE 'role-arn'\n",
    "FORMAT AS PARQUET;\n",
    "```\n",
    "\n",
    "**2. DynamoDB:**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 'dynamodb://table-name'\n",
    "IAM_ROLE 'role-arn'\n",
    "READRATIO 50;\n",
    "```\n",
    "\n",
    "**3. Remote Host (SSH):**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 'ssh://host/path/file'\n",
    "IAM_ROLE 'role-arn';\n",
    "```\n",
    "\n",
    "### File Formats\n",
    "\n",
    "**Supported Formats:**\n",
    "\n",
    "- CSV (Comma-Separated Values)\n",
    "- TSV (Tab-Separated Values)\n",
    "- Fixed-width\n",
    "- JSON\n",
    "- Avro\n",
    "- Parquet\n",
    "- ORC\n",
    "\n",
    "**Optimized Formats:**\n",
    "\n",
    "Parquet and ORC provide best performance:\n",
    "\n",
    "- Columnar storage aligns with Redshift\n",
    "- Built-in compression\n",
    "- Predicate pushdown\n",
    "- Faster loads and queries\n",
    "\n",
    "### Manifest Files\n",
    "\n",
    "A manifest file lists S3 objects to load, providing precise control.\n",
    "\n",
    "**Why Use Manifest:**\n",
    "\n",
    "- Load specific files\n",
    "- Ensure exactly-once loading\n",
    "- Handle file updates\n",
    "- Mandatory objects list\n",
    "\n",
    "**Example Manifest:**\n",
    "\n",
    "```text\n",
    "{\n",
    "  \"entries\": [\n",
    "    {\"url\": \"s3://bucket/data/file1.csv\", \"mandatory\": true},\n",
    "    {\"url\": \"s3://bucket/data/file2.csv\", \"mandatory\": true}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**COPY with Manifest:**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 's3://bucket/manifest.json'\n",
    "IAM_ROLE 'role-arn'\n",
    "MANIFEST\n",
    "FORMAT AS CSV;\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Split Data into Multiple Files**\n",
    "\n",
    "- Enables parallel loading\n",
    "- Optimal file size: 1-125 MB after compression\n",
    "- Number of files = multiple of number of slices\n",
    "\n",
    "**2. Compress Files**\n",
    "\n",
    "- Reduces data transfer time\n",
    "- Saves S3 storage costs\n",
    "- Gzip provides good balance\n",
    "\n",
    "**3. Use Columnar Formats**\n",
    "\n",
    "- Parquet or ORC preferred\n",
    "- Faster loads\n",
    "- Better compression\n",
    "\n",
    "**4. Sort Data Before Loading**\n",
    "\n",
    "- Pre-sort by table's sort key\n",
    "- Reduces need for VACUUM\n",
    "- Improves query performance immediately\n",
    "\n",
    "**5. Use Appropriate Distribution Key**\n",
    "\n",
    "- Distribute data evenly\n",
    "- Minimize data movement during queries\n",
    "- Choose based on join patterns\n",
    "\n",
    "**6. Error Handling**\n",
    "\n",
    "```text\n",
    "COPY table_name\n",
    "FROM 's3://bucket/data/'\n",
    "IAM_ROLE 'role-arn'\n",
    "MAXERROR 100\n",
    "ACCEPTINVCHARS;\n",
    "```\n",
    "\n",
    "**7. Monitor Progress**\n",
    "\n",
    "Query system tables to monitor:\n",
    "\n",
    "```text\n",
    "SELECT * FROM stl_load_errors\n",
    "WHERE query = <query_id>;\n",
    "```\n",
    "\n",
    "## Coding Exercise: Latest Event Per User\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**Table Schema:**\n",
    "\n",
    "```text\n",
    "CREATE TABLE events (\n",
    "    event_id INT,\n",
    "    user_id INT,\n",
    "    event_type VARCHAR(50),\n",
    "    event_time TIMESTAMP\n",
    ");\n",
    "```\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Write a SQL query to get the latest event for each user.\n",
    "\n",
    "### Solution\n",
    "\n",
    "**Method 1: Using Window Function (Preferred)**\n",
    "\n",
    "```text\n",
    "SELECT \n",
    "    event_id,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    event_time\n",
    "FROM (\n",
    "    SELECT \n",
    "        event_id,\n",
    "        user_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_time DESC) as rn\n",
    "    FROM events\n",
    ") ranked\n",
    "WHERE rn = 1;\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. `ROW_NUMBER()` assigns a sequential number to each row within each user's partition\n",
    "2. `PARTITION BY user_id` creates separate row number sequences for each user\n",
    "3. `ORDER BY event_time DESC` orders events newest first\n",
    "4. Filter `rn = 1` keeps only the latest event for each user\n",
    "\n",
    "**Method 2: Using Subquery with MAX**\n",
    "\n",
    "```text\n",
    "SELECT e1.*\n",
    "FROM events e1\n",
    "INNER JOIN (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        MAX(event_time) as max_time\n",
    "    FROM events\n",
    "    GROUP BY user_id\n",
    ") e2 ON e1.user_id = e2.user_id \n",
    "    AND e1.event_time = e2.max_time;\n",
    "```\n",
    "\n",
    "**Method 3: Using Correlated Subquery**\n",
    "\n",
    "```text\n",
    "SELECT e1.*\n",
    "FROM events e1\n",
    "WHERE e1.event_time = (\n",
    "    SELECT MAX(e2.event_time)\n",
    "    FROM events e2\n",
    "    WHERE e2.user_id = e1.user_id\n",
    ");\n",
    "```\n",
    "\n",
    "### Follow-up: Why Expensive in Redshift & Optimization\n",
    "\n",
    "**Why This Query Can Be Expensive:**\n",
    "\n",
    "**1. Full Table Scan**\n",
    "\n",
    "- Query must read all rows from the `events` table\n",
    "- No filtering on partition key\n",
    "- Scanning across all compute nodes\n",
    "\n",
    "**2. Data Distribution Issues**\n",
    "\n",
    "- If table not distributed by `user_id`, data shuffling required\n",
    "- Events for same user may be on different nodes\n",
    "- Network transfer between nodes\n",
    "\n",
    "**3. Lack of Sort Key**\n",
    "\n",
    "- If `event_time` or `user_id` not in sort key, no zone maps to skip blocks\n",
    "- Can't eliminate blocks during scan\n",
    "- More disk I/O required\n",
    "\n",
    "**4. Window Function Overhead**\n",
    "\n",
    "- `ROW_NUMBER()` requires sorting within each partition\n",
    "- All user events must be collected and sorted\n",
    "- Temporary storage needed\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "**1. Use Appropriate Distribution Key**\n",
    "\n",
    "Distribute by `user_id` to colocate user events:\n",
    "\n",
    "```text\n",
    "CREATE TABLE events (\n",
    "    event_id INT,\n",
    "    user_id INT,\n",
    "    event_type VARCHAR(50),\n",
    "    event_time TIMESTAMP\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(user_id, event_time);\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- All events for a user on same node\n",
    "- No data shuffling required\n",
    "- Faster window function computation\n",
    "\n",
    "**2. Add Compound Sort Key**\n",
    "\n",
    "Sort by `user_id` and `event_time`:\n",
    "\n",
    "```text\n",
    "SORTKEY(user_id, event_time)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Zone maps help skip irrelevant blocks\n",
    "- Data physically ordered for better scans\n",
    "- Window function benefits from pre-sorted data\n",
    "\n",
    "**3. Partition Data by Date**\n",
    "\n",
    "If table is very large, partition by date:\n",
    "\n",
    "```text\n",
    "-- Query only recent partitions\n",
    "WHERE event_time >= DATEADD(day, -30, GETDATE())\n",
    "```\n",
    "\n",
    "**4. Materialized View**\n",
    "\n",
    "Create and maintain latest events:\n",
    "\n",
    "```text\n",
    "CREATE MATERIALIZED VIEW latest_user_events AS\n",
    "SELECT \n",
    "    event_id,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    event_time\n",
    "FROM (\n",
    "    SELECT \n",
    "        event_id,\n",
    "        user_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_time DESC) as rn\n",
    "    FROM events\n",
    ") ranked\n",
    "WHERE rn = 1;\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Pre-computed results\n",
    "- Query reads view instead of scanning full table\n",
    "- Refresh periodically or on-demand\n",
    "\n",
    "**5. Use Result Caching**\n",
    "\n",
    "Enable result caching in Redshift:\n",
    "\n",
    "- Subsequent identical queries return cached results\n",
    "- Dramatically faster for repeated queries\n",
    "- Automatic, no code changes needed\n",
    "\n",
    "**6. Column Compression**\n",
    "\n",
    "Ensure proper encoding:\n",
    "\n",
    "```text\n",
    "ANALYZE COMPRESSION events;\n",
    "```\n",
    "\n",
    "Then apply recommended encodings:\n",
    "\n",
    "```text\n",
    "ALTER TABLE events ALTER COLUMN event_type ENCODE LZO;\n",
    "ALTER TABLE events ALTER COLUMN event_time ENCODE AZ64;\n",
    "```\n",
    "\n",
    "**7. Query Pattern Optimization**\n",
    "\n",
    "If only need recent events, add time filter:\n",
    "\n",
    "```text\n",
    "SELECT \n",
    "    event_id,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    event_time\n",
    "FROM (\n",
    "    SELECT \n",
    "        event_id,\n",
    "        user_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_time DESC) as rn\n",
    "    FROM events\n",
    "    WHERE event_time >= DATEADD(day, -90, GETDATE())  -- Only last 90 days\n",
    ") ranked\n",
    "WHERE rn = 1;\n",
    "```\n",
    "\n",
    "**Performance Comparison:**\n",
    "\n",
    "**Before Optimization:**\n",
    "\n",
    "- Full table scan: 100M rows\n",
    "- Execution time: 45 seconds\n",
    "- Data shuffling: 20 GB transferred\n",
    "\n",
    "**After Optimization:**\n",
    "\n",
    "- Distributed by user_id, sorted by (user_id, event_time)\n",
    "- Zone maps eliminate 80% of blocks\n",
    "- Execution time: 5 seconds\n",
    "- No data shuffling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
