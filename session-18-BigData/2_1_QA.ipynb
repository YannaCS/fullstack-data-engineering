{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark & Data Engineering Questions\n",
    "\n",
    "## Part 1: Apache Spark Concepts\n",
    "\n",
    "### 1. What is Apache Spark, and how does it fit into the big data ecosystem (storage vs compute vs resource management)?\n",
    "\n",
    "Apache Spark is a **unified analytics engine for large-scale data processing**. It provides in-memory computing capabilities that make it significantly faster than traditional disk-based processing frameworks like MapReduce.\n",
    "\n",
    "In the big data ecosystem:\n",
    "- **Storage**: Spark does NOT store data. It reads from and writes to external storage systems like HDFS, S3, Azure Blob Storage, or databases.\n",
    "- **Compute**: Spark IS a compute engine. It processes data in memory across a distributed cluster, performing transformations and analytics.\n",
    "- **Resource Management**: Spark does NOT manage cluster resources itself. It relies on external resource managers like YARN, Kubernetes, or Mesos to allocate CPU, memory, and other resources.\n",
    "\n",
    "### 2. Explain lazy evaluation in Spark. Why does Spark delay execution until an action is called?\n",
    "\n",
    "**Lazy evaluation** means Spark does not execute transformations immediately. Instead, it builds a logical execution plan (DAG - Directed Acyclic Graph) and waits until an action is called.\n",
    "\n",
    "Why delay execution:\n",
    "- **Optimization**: Spark can analyze the entire computation plan and optimize it (e.g., predicate pushdown, combining operations, eliminating unnecessary steps)\n",
    "- **Reduced I/O**: By knowing the full plan, Spark minimizes data shuffling and disk reads\n",
    "- **Fault tolerance**: The DAG lineage allows Spark to recompute lost partitions without re-running the entire job\n",
    "- **Resource efficiency**: Only computes what's actually needed for the final result\n",
    "\n",
    "### 3. What is the difference between transformations and actions? What happens internally when an action is triggered?\n",
    "\n",
    "**Transformations**: Operations that create a new RDD/DataFrame from an existing one. They are lazy and only define the computation logic.\n",
    "- Examples: `map()`, `filter()`, `select()`, `groupBy()`, `join()`\n",
    "\n",
    "**Actions**: Operations that trigger actual computation and return results to the driver or write to external storage.\n",
    "- Examples: `collect()`, `count()`, `show()`, `write()`, `reduce()`\n",
    "\n",
    "When an action is triggered:\n",
    "1. Spark's DAG Scheduler analyzes the lineage of transformations\n",
    "2. It creates an optimized physical execution plan\n",
    "3. The plan is divided into stages based on shuffle boundaries\n",
    "4. Stages are broken into tasks (one per partition)\n",
    "5. Tasks are distributed to executors for parallel execution\n",
    "6. Results are collected or written as specified\n",
    "\n",
    "### 4. What is an RDD? Why are immutability and partitioning important in Spark's design?\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** is Spark's fundamental data abstraction - a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Immutability importance**:\n",
    "- Enables safe parallel processing without locks or synchronization\n",
    "- Supports fault tolerance through lineage - if a partition is lost, it can be recomputed from the original transformation chain\n",
    "- Allows Spark to cache intermediate results safely\n",
    "- Simplifies reasoning about distributed computations\n",
    "\n",
    "**Partitioning importance**:\n",
    "- Enables parallel processing - each partition can be processed independently on different nodes\n",
    "- Controls data distribution across the cluster\n",
    "- Affects performance - proper partitioning minimizes data shuffling\n",
    "- Determines the level of parallelism in computations\n",
    "\n",
    "### 5. Explain the execution hierarchy in Spark: Application → Job → Stage → Task. What determines stage boundaries?\n",
    "\n",
    "**Execution Hierarchy**:\n",
    "- **Application**: A complete Spark program with its own driver and executors\n",
    "- **Job**: Triggered by each action; represents all work needed to compute that action's result\n",
    "- **Stage**: A set of tasks that can run in parallel without shuffling data; jobs are divided into stages\n",
    "- **Task**: The smallest unit of work; one task processes one partition within a stage\n",
    "\n",
    "**Stage boundaries are determined by**:\n",
    "- **Shuffle operations** (wide transformations) like `groupBy()`, `reduceByKey()`, `join()`, `repartition()`\n",
    "- When data needs to be redistributed across partitions, Spark must complete all tasks in one stage before starting the next\n",
    "- Each shuffle creates a new stage boundary because all map-side tasks must finish before reduce-side tasks can begin\n",
    "\n",
    "### 6. What is the difference between narrow and wide transformations? Why are wide transformations usually more expensive?\n",
    "\n",
    "**Narrow Transformations**:\n",
    "- Each input partition contributes to at most one output partition\n",
    "- No data shuffling required\n",
    "- Examples: `map()`, `filter()`, `flatMap()`, `union()`\n",
    "- Can be pipelined together in a single stage\n",
    "\n",
    "**Wide Transformations**:\n",
    "- Input partitions contribute to multiple output partitions\n",
    "- Require data shuffling across the network\n",
    "- Examples: `groupByKey()`, `reduceByKey()`, `join()`, `repartition()`\n",
    "- Create new stage boundaries\n",
    "\n",
    "**Why wide transformations are more expensive**:\n",
    "- **Network I/O**: Data must be transferred between executors across the cluster\n",
    "- **Disk I/O**: Shuffle data is often written to disk as intermediate storage\n",
    "- **Synchronization**: All tasks in the previous stage must complete before the next stage can begin\n",
    "- **Serialization overhead**: Data must be serialized for transfer and deserialized on receipt\n",
    "\n",
    "### 7. Explain the roles of Driver and Executor. What kind of work happens on each side?\n",
    "\n",
    "**Driver**:\n",
    "- The main control process that runs the user's main() function\n",
    "- Responsibilities:\n",
    "  - Creates SparkContext/SparkSession\n",
    "  - Converts user code into a DAG of tasks\n",
    "  - Schedules tasks on executors\n",
    "  - Coordinates job execution\n",
    "  - Collects results from executors\n",
    "  - Maintains metadata about the application\n",
    "- Runs on: A single node (master node or client machine)\n",
    "\n",
    "**Executor**:\n",
    "- Worker processes that run on cluster nodes\n",
    "- Responsibilities:\n",
    "  - Execute tasks assigned by the driver\n",
    "  - Store data in memory or disk for caching\n",
    "  - Return computed results to the driver\n",
    "  - Report task status and metrics back to driver\n",
    "- Runs on: Multiple worker nodes (one or more executors per node)\n",
    "\n",
    "### 8. What is the Medallion Architecture (Bronze, Silver, Gold)? What type of data and logic belongs in each layer?\n",
    "\n",
    "**Medallion Architecture** is a data design pattern that organizes data into three layers based on quality and refinement level.\n",
    "\n",
    "**Bronze Layer (Raw)**:\n",
    "- Raw, unprocessed data exactly as ingested from sources\n",
    "- Maintains original format and schema\n",
    "- Logic: Minimal - just ingestion, timestamp addition, source tracking\n",
    "- Purpose: Data lineage, reprocessing capability, audit trail\n",
    "\n",
    "**Silver Layer (Cleaned/Conformed)**:\n",
    "- Cleaned, validated, and standardized data\n",
    "- Deduplication applied, data types enforced\n",
    "- Logic: Data quality checks, schema enforcement, joins with reference data, business rules validation\n",
    "- Purpose: Create a reliable, consistent dataset for analysis\n",
    "\n",
    "**Gold Layer (Business-Level Aggregates)**:\n",
    "- Aggregated, business-ready data optimized for consumption\n",
    "- Dimensional models, KPIs, metrics\n",
    "- Logic: Business aggregations, summary calculations, reporting transformations\n",
    "- Purpose: Power dashboards, reports, ML features, and business analytics\n",
    "\n",
    "### 9. Explain SCD Type 0, Type 1, and Type 2 in simple terms. When would you choose each one?\n",
    "\n",
    "**SCD (Slowly Changing Dimension)** strategies handle how to track changes in dimension data over time.\n",
    "\n",
    "**Type 0 - Retain Original**:\n",
    "- Never update the record; keep the original value forever\n",
    "- Use when: Original value must be preserved (e.g., original signup date, birth date, original credit score at loan application)\n",
    "\n",
    "**Type 1 - Overwrite**:\n",
    "- Simply overwrite the old value with the new value\n",
    "- No history is kept\n",
    "- Use when: Historical values don't matter, only current state is needed (e.g., fixing typos, updating phone numbers, current address for shipping)\n",
    "\n",
    "**Type 2 - Add New Row**:\n",
    "- Insert a new row for each change while keeping old rows\n",
    "- Includes effective dates (start_date, end_date) and/or a current flag\n",
    "- Full history is preserved\n",
    "- Use when: Historical tracking is critical for analysis (e.g., price changes, customer status changes, employee role changes for auditing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SQL Problem - 7-Day Moving Average\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "```text\n",
    "+---------------+---------+ \n",
    "| Column Name   | Type    | \n",
    "+---------------+---------+ \n",
    "| customer_id   | int     | \n",
    "| name          | varchar | \n",
    "| visited_on    | date    | \n",
    "| amount        | int     | \n",
    "+---------------+---------+ \n",
    "```\n",
    "\n",
    "In SQL,(customer_id, visited_on) is the primary key for this table. \n",
    "This table contains data about customer transactions in a restaurant. \n",
    "visited_on is the date on which the customer with ID (customer_id) has visited the restaurant.  \n",
    "\n",
    "amount is the total paid by a customer.   \n",
    "\n",
    "You are the restaurant owner and you want to analyze a possible expansion (there will be **at least one customer every day**).   \n",
    "\n",
    "Compute the **moving average** of how much the customer paid in a seven days window (i.e., **current day + 6 days before**). average_amount should be rounded to two decimal places.   \n",
    "\n",
    "Return the result table ordered by visited_on in ascending order.\n",
    "\n",
    "### Solution\n",
    "\n",
    "```sql\n",
    "-- postgresql\n",
    "-- all customer payments per day\n",
    "WITH daily_amount AS (\n",
    "    SELECT\n",
    "        visited_on,\n",
    "        SUM(amount) AS total_amount\n",
    "    FROM Customer\n",
    "    GROUP BY visited_on\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    visited_on,\n",
    "    SUM(total_amount) OVER (\n",
    "        ORDER BY visited_on\n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW  -- creates a 7-day rolling window\n",
    "    ) AS amount,\n",
    "    ROUND(\n",
    "        AVG(total_amount) OVER (\n",
    "            ORDER BY visited_on\n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ),\n",
    "        2\n",
    "    ) AS average_amount\n",
    "FROM daily_amount\n",
    "ORDER BY visited_on\n",
    "OFFSET 6;  -- removes the first 6 days since they don’t have a full window.\n",
    "-- or sql: LIMIT 18446744073709551615 OFFSET 6 -- a \"safe\" way to guarantee you get all rows no matter how big the table is\n",
    "-- or just use where: WHERE visited_on >= (SELECT MIN(visited_on) + INTERVAL 6 DAY FROM Customer)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "1. First, aggregate daily totals since multiple customers can visit on the same day\n",
    "2. Use window functions with `ROWS BETWEEN 6 PRECEDING AND CURRENT ROW` for the 7-day window\n",
    "3. Filter out the first 6 days since they don't have a complete 7-day history\n",
    "4. Round the average to 2 decimal places\n",
    "5. Order by visited_on ascending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
