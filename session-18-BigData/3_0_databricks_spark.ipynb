{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Databricks\n",
    "\n",
    "## From Apache Spark to Databricks\n",
    "\n",
    "**Apache Spark** is an open-source distributed computing framework designed for big data processing. However, after Spark became popular, several challenges emerged in production environments.\n",
    "\n",
    "### Problems with Running Spark in Production\n",
    "\n",
    "Engineers encountered significant **operational overhead**:\n",
    "\n",
    "- **Cluster Setup**: Manual configuration of distributed computing clusters was **complex and time-consuming**\n",
    "- **Resource Tuning**: Constant need to optimize memory, CPU, and storage allocation across nodes\n",
    "- **Job Failures**: Debugging failed jobs in distributed environments required deep expertise\n",
    "- **Infrastructure Management**: Engineers spent 60-70% of time on infrastructure rather than data logic\n",
    "\n",
    "**Result**: Data scientists and engineers had less time for actual data analytics and business logic.\n",
    "\n",
    "### Databricks Solution\n",
    "\n",
    "**Founded by**: Original creators of Apache Spark (from UC Berkeley AMPLab)\n",
    "\n",
    "**Mission**: Make Spark easy to use in production environments\n",
    "\n",
    "**Key Offerings**:\n",
    "\n",
    "1. **Managed Clusters**\n",
    "   - Automatic cluster provisioning and scaling\n",
    "   - No manual infrastructure setup required\n",
    "   - Auto-termination to save costs\n",
    "\n",
    "2. **Collaborative Notebooks**\n",
    "   - Interactive development environment\n",
    "   - Support for Python, Scala, SQL, and R\n",
    "   - Real-time collaboration features\n",
    "\n",
    "3. **Monitoring and Optimization**\n",
    "   - Built-in performance monitoring\n",
    "   - Automatic optimization suggestions\n",
    "   - Job scheduling and orchestration\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Databricks\n",
    "\n",
    "Step-by-Step Access Instructions:   \n",
    "\n",
    "## Step 1: Create Account\n",
    "- Navigate to: https://www.databricks.com/try-databricks\n",
    "- Sign up for a free community edition or enterprise trial\n",
    "- Choose your cloud provider (AWS, Azure, or GCP)\n",
    "\n",
    "## Step 2: Start a Spark Cluster\n",
    "\n",
    "1. Navigate to \"Compute\" in the sidebar\n",
    "2. Click \"Create Cluster\"\n",
    "3. Configure cluster settings:\n",
    "   - Cluster name\n",
    "   - Runtime version (e.g., 15.0 ML with Scala 2.12, Spark 3.5.0)\n",
    "   - Node type (e.g., Standard_DS3_v2: 14 GB Memory, 4 Cores)\n",
    "   - Autoscaling options\n",
    "4. Click \"Create Cluster\"\n",
    "\n",
    "> Community Edition has 1 free serverless limited cluster. \n",
    "> just go to step 3\n",
    "\n",
    "\n",
    "**Runtime Options**:\n",
    "- **Standard**: Basic Spark functionality\n",
    "- **ML (Machine Learning)**: Includes pre-installed ML libraries (scikit-learn, TensorFlow, etc.)\n",
    "- **LTS (Long Term Support)**: Stable versions for production\n",
    "\n",
    "**Node Types**:\n",
    "- **General Purpose**: Balanced compute and memory (Standard_DS3_v2, Standard_DS4_v2)\n",
    "- **GPU Accelerated**: For deep learning workloads\n",
    "\n",
    "## Step 3: Create a Notebook\n",
    "\n",
    "1. Navigate to \"Workspace\"\n",
    "2. Click \"Create\" → \"Notebook\"\n",
    "3. Name your notebook\n",
    "4. Select default language (Python recommended)\n",
    "5. Attach notebook to your running cluster\n",
    "\n",
    "\n",
    "## Step 4: Run Your First Spark Code\n",
    "```python\n",
    "# Create a simple DataFrame with 5 rows\n",
    "spark.range(5).show()\n",
    "```\n",
    "\n",
    "**Expected Output**:\n",
    "```\n",
    "+---+\n",
    "| id|\n",
    "+---+\n",
    "|  0|\n",
    "|  1|\n",
    "|  2|\n",
    "|  3|\n",
    "|  4|\n",
    "+---+\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Fundamentals\n",
    "\n",
    "## Understanding the Spark Architecture\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "1. **Driver Program**: Coordinates the execution, runs the main() function\n",
    "2. **Cluster Manager**: Allocates resources (YARN, Mesos, Kubernetes, or Standalone)\n",
    "3. **Executor Nodes**: Perform the actual data processing tasks\n",
    "4. **SparkSession**: Entry point for all Spark functionality\n",
    "\n",
    "<img src=\"./pic/3_spark_architecture.png\" width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "The unified entry point for working with Spark.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession (in Databricks, 'spark' is pre-created)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# In Databricks notebooks, simply use:\n",
    "spark  # Already available\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD vs DataFrame vs Dataset\n",
    "\n",
    "the complete Apache Spark ecosystem and how different components interact:  \n",
    "\n",
    "<img src='./pic/3_spark_api_architecture.jpg' width=500>\n",
    "\n",
    "**Vertical Flow** (Bottom to Top):\n",
    "\n",
    "```text\n",
    "Data Sources API (Storage, Where data comes from)\n",
    "\n",
    "    ↓reads/writes data\n",
    "\n",
    "RDDs (Low-level abstraction)\n",
    "\n",
    "    ↓ provides distributed data\n",
    "\n",
    "Spark Core: Execution engine  (What actually runs your code)\n",
    "\n",
    "    ↓ executes operations\n",
    "\n",
    "Catalyst Optimizer (Where you write query optimization)\n",
    "\n",
    "    ↓ optimizes queries\n",
    "\n",
    "APIs: Spark SQL, Dataset, DataFrame  (What you write code with)\n",
    "\n",
    "    ↓ provides high-level interface\n",
    "\n",
    "User Interfaces: JDBC, Console, Programs (How you interact with Spark)\n",
    "```\n",
    "\n",
    "**Horizontal Relationships:**   \n",
    "- All three APIs (Spark SQL, Dataset, DataFrame) use the same Catalyst Optimizer\n",
    "- All three APIs execute through the same Spark Core Engine\n",
    "- Traditional RDDs can bypass higher layers for low-level control\n",
    "\n",
    "**Key Takeaways**:   \n",
    "\n",
    "- **You work at the top** (Programs, DataFrame API)\n",
    "- **Middle layers optimize** automatically (Catalyst, Spark Core)\n",
    "- **Bottom layers** handle distribution and storage (RDDs, Data Sources)\n",
    "- **Everything is unified**: SQL, DataFrames, and Datasets all use the same engine\n",
    "- **You benefit from optimization** without doing anything special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD (Resilient Distributed Dataset) \n",
    "is the **fundamental data structure** of Apache Spark, it's the original low-level abstraction that Spark was built on. It is:  \n",
    "\n",
    "- An **immutable distributed** collection of objects\n",
    "- Split into multiple **partitions** across cluster nodes\n",
    "- Can be operated on in **parallel**\n",
    "- **Fault-tolerant**: automatically rebuilt if partitions are lost\n",
    "\n",
    "Think of it as: A distributed Python list that can be processed in parallel across multiple machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame \n",
    "is a **distributed collection** of data **organized into named columns**, similar to a **table** in a relational database or a spreadsheet in Excel, but designed to work with massive datasets across multiple computers. It is:  \n",
    "\n",
    "- ✅ Distributed table with named columns\n",
    "- ✅ Like Excel/SQL table but for big data\n",
    "- ✅ **Immutable** (can't change, only create new versions)\n",
    "- ✅ Lazy evaluation (builds execution plan)\n",
    "- ✅ Automatically optimized by Catalyst\n",
    "\n",
    "\n",
    "Think of it as Excel/Google Sheets: \n",
    "- Has rows and columns (like a spreadsheet)\n",
    "- Each column has a name and data type\n",
    "- But can handle HUGE data (terabytes)\n",
    "- Processes data across multiple computers in parallel\n",
    "\n",
    "\n",
    "compare to Python List, Pandas DataFrame, Spark DataFrame can scale to **billions of rows** across 100s of computers!\n",
    "\n",
    "Why Use DataFrames?\n",
    "- ✅ Easy to use - SQL-like operations\n",
    "- ✅ Fast - Automatic optimization\n",
    "- ✅ Scalable - Handles terabytes of data\n",
    "- ✅ Flexible - Reads many file formats\n",
    "- ✅ Industry standard - What companies actually use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "is a **strongly-typed, object-oriented API** available in **Scala and Java** (not Python/PySpark). It combines the benefits of RDDs (type safety) with the benefits of DataFrames (optimization).\n",
    "\n",
    "If you're using PySpark (Python):\n",
    "\n",
    "- ❌ **Datasets are NOT available in Python**\n",
    "- ✅ DataFrames are your Dataset equivalent\n",
    "- ✅ Focus on DataFrames, they're what you need\n",
    "\n",
    "**Why no Datasets in Python?**\n",
    "\n",
    "- **Python is dynamically typed** (no compile-time type checking)\n",
    "- Datasets require **static typing** (Scala/Java feature)\n",
    "- PySpark DataFrames already provide what you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "| Feature | RDD | DataFrame | Dataset |\n",
    "|---------|-----|-----------|---------|\n",
    "| **API Level** | Low-level | High-level | High-level |\n",
    "| **Type Safety** | No | No | Yes (Scala/Java) |\n",
    "| **Optimization** | Manual | Catalyst Optimizer | Catalyst Optimizer |\n",
    "| **Ease of Use** | Complex | Easy (SQL-like) | Easy |\n",
    "| **Performance** | Slower | Faster | Fastest |\n",
    "| **Language** | All | All | Scala/Java only |\n",
    "\n",
    "**Recommendation**: Use DataFrames for most PySpark work.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process with PySpark\n",
    "\n",
    "**ETL** stands for Extract, Transform, Load, which is the core process for data pipeline development.\n",
    "\n",
    "```text\n",
    "┌─────────┐     ┌───────────┐     ┌──────┐\n",
    "│ Extract │ --> │ Transform │ --> │ Load │\n",
    "└─────────┘     └───────────┘     └──────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Phase\n",
    "\n",
    "Reading data from various sources into Spark DataFrames.\n",
    "\n",
    "### Reading CSV Files\n",
    "```python\n",
    "# Basic CSV read\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Advanced CSV read with options\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"nullValue\", \"NA\") \\\n",
    "    .csv(\"path/to/file.csv\")\n",
    "```\n",
    "\n",
    "### Reading JSON Files\n",
    "```python\n",
    "# Single-line JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Multi-line JSON\n",
    "df = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"path/to/file.json\")\n",
    "```\n",
    "\n",
    "### Reading Parquet Files\n",
    "```python\n",
    "# Parquet is the preferred format for Spark\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Reading from Databases (JDBC)\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host:port/database\") \\\n",
    "    .option(\"dbtable\", \"schema.table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### Reading Delta Lake Tables\n",
    "```python\n",
    "# Delta Lake provides ACID transactions\n",
    "df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")\n",
    "\n",
    "# Or using table name\n",
    "df = spark.table(\"database.table_name\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform Phase\n",
    "\n",
    "Applying business logic and data transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations\n",
    "\n",
    "```python\n",
    "    # Select columns\n",
    "    df_selected = df.select(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "    # Filter rows\n",
    "    df_filtered = df.filter(df[\"age\"] > 18)\n",
    "    df_filtered = df.where(df[\"age\"] > 18)  # Same as filter\n",
    "\n",
    "    # Add new column\n",
    "    from pyspark.sql.functions import col, lit\n",
    "\n",
    "    df_with_new = df.withColumn(\"new_col\", col(\"existing_col\") * 2)\n",
    "    df_with_constant = df.withColumn(\"status\", lit(\"active\"))\n",
    "\n",
    "    # Rename column\n",
    "    df_renamed = df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "    # Drop columns\n",
    "    df_dropped = df.drop(\"col1\", \"col2\")\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_unique = df.dropDuplicates()\n",
    "    df_unique_subset = df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Null Values\n",
    "\n",
    "```python\n",
    "    # Drop rows with null values\n",
    "    df_no_nulls = df.dropna()  # Drop rows with any null\n",
    "    df_no_nulls = df.dropna(how=\"all\")  # Drop only if all columns are null\n",
    "    df_no_nulls = df.dropna(subset=[\"col1\", \"col2\"])  # Check specific columns\n",
    "\n",
    "    # Fill null values\n",
    "    df_filled = df.fillna(0)  # Fill all numeric nulls with 0\n",
    "    df_filled = df.fillna({\"col1\": 0, \"col2\": \"Unknown\"})  # Column-specific\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import count, sum, avg, max, min, stddev\n",
    "\n",
    "# Group by and aggregate\n",
    "df_agg = df.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        sum(\"amount\").alias(\"total_amount\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        max(\"amount\").alias(\"max_amount\"),\n",
    "        min(\"amount\").alias(\"min_amount\")\n",
    "    )\n",
    "\n",
    "# Multiple group by columns\n",
    "df_multi_group = df.groupBy(\"category\", \"region\") \\\n",
    "    .agg(sum(\"sales\").alias(\"total_sales\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "```python\n",
    "# Inner join (default)\n",
    "df_joined = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")\n",
    "\n",
    "# Left outer join\n",
    "df_left = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left\")\n",
    "\n",
    "# Right outer join\n",
    "df_right = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"right\")\n",
    "\n",
    "# Full outer join\n",
    "df_full = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"outer\")\n",
    "\n",
    "# Join on multiple columns\n",
    "df_joined = df1.join(df2, \n",
    "    (df1[\"id\"] == df2[\"id\"]) & (df1[\"date\"] == df2[\"date\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Join on column name (if same name in both DataFrames)\n",
    "df_joined = df1.join(df2, \"id\", \"inner\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "\n",
    "Window functions are special functions that **perform calculations across a set of rows that are related to the current row**, without collapsing the rows like groupBy() does.  \n",
    "\n",
    "Think of a **window** as a \"frame\" or \"view\" of related rows:\n",
    "```text\n",
    "Original DataFrame:\n",
    "┌──────────┬───────┬──────┐\n",
    "│department│ name  │salary│\n",
    "├──────────┼───────┼──────┤\n",
    "│Sales     │Alice  │ 5000 │  ┐\n",
    "│Sales     │Bob    │ 6000 │  ├─ Sales Window\n",
    "│Sales     │Charlie│ 5500 │  ┘\n",
    "│Marketing │Diana  │ 7000 │  ┐\n",
    "│Marketing │Eve    │ 6500 │  ┘─ Marketing Window\n",
    "└──────────┴───────┴──────┘\n",
    "\n",
    "with window: \n",
    "df_with_avg = df.withColumn('dept_avg', avg('salary').over(Window.partitionBy('department')))\n",
    "\n",
    "+----------+-------+------+--------+\n",
    "|department|   name|salary|dept_avg|\n",
    "+----------+-------+------+--------+\n",
    "| Marketing|  Diana|  7000|  6750.0|\n",
    "| Marketing|    Eve|  6500|  6750.0|\n",
    "|     Sales|  Alice|  5000|  5500.0|\n",
    "|     Sales|    Bob|  6000|  5500.0|\n",
    "|     Sales|Charlie|  5500|  5500.0|\n",
    "+----------+-------+------+--------+\n",
    "\n",
    "without window:\n",
    "dept_avg = df.groupBy('department').agg(avg('salary').alias('dept_avg'))\n",
    "\n",
    "+----------+--------+\n",
    "|department|dept_avg|\n",
    "+----------+--------+\n",
    "|     Sales|  5500.0|\n",
    "| Marketing|  6750.0|\n",
    "+----------+--------+\n",
    "```\n",
    "\n",
    "Window function calculates across each window separately\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lead, lag\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"amount\")\n",
    "\n",
    "# Add row number\n",
    "df_with_row = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Ranking functions\n",
    "df_ranked = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_dense = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "# Lead and lag (access next/previous row)\n",
    "df_with_lead = df.withColumn(\"next_amount\", lead(\"amount\", 1).over(window_spec))\n",
    "df_with_lag = df.withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_spec))\n",
    "\n",
    "# Cumulative sum\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "window_cumulative = Window.partitionBy(\"category\").orderBy(\"date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_cumsum = df.withColumn(\"cumulative_sum\", _sum(\"amount\").over(window_cumulative))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions (UDFs)\n",
    "allow you to create custom functions in Python and use them on Spark DataFrames when built-in Spark functions aren't enough.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Define Python function\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return \"Minor\"\n",
    "    elif age < 65:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "# Register as UDF\n",
    "categorize_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_categorized = df.withColumn(\"age_category\", categorize_udf(col(\"age\")))\n",
    "\n",
    "# Alternative: Use pandas UDF for better performance\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def categorize_age_pandas(age: pd.Series) -> pd.Series:\n",
    "    return age.apply(lambda x: \"Minor\" if x < 18 else (\"Adult\" if x < 65 else \"Senior\"))\n",
    "\n",
    "df_categorized = df.withColumn(\"age_category\", categorize_age_pandas(col(\"age\")))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Phase\n",
    "\n",
    "Writing transformed data to target destinations.\n",
    "\n",
    "### Writing to CSV\n",
    "```python\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"path/to/output\")\n",
    "```\n",
    "\n",
    "### Writing to Parquet\n",
    "```python\n",
    "# Single file (coalesce to 1 partition)\n",
    "df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"path/to/output\")\n",
    "\n",
    "# Partitioned by column\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"path/to/output\")\n",
    "```\n",
    "\n",
    "### Writing to Delta Lake\n",
    "```python\n",
    "# Overwrite\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/path/to/delta-table\")\n",
    "\n",
    "# Append\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/path/to/delta-table\")\n",
    "\n",
    "# Save as table\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"database.table_name\")\n",
    "```\n",
    "\n",
    "### Writing to Databases\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host:port/database\") \\\n",
    "    .option(\"dbtable\", \"schema.table_name\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "```\n",
    "\n",
    "**Write Modes**:\n",
    "- `overwrite`: Replace existing data\n",
    "- `append`: Add to existing data\n",
    "- `ignore`: Do nothing if data exists\n",
    "- `error` or `errorifexists` (default): Throw error if data exists\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "### From Python Lists\n",
    "```python\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "```\n",
    "\n",
    "### From Pandas DataFrame\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "```\n",
    "\n",
    "### With Schema Definition\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"age\", IntegerType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25, \"NYC\"), (\"Bob\", 30, \"LA\")]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting DataFrames\n",
    "\n",
    "```python\n",
    "# Show first n rows\n",
    "df.show(10)\n",
    "df.show(10, truncate=False)  # Don't truncate long strings\n",
    "\n",
    "# Display schema\n",
    "df.printSchema()\n",
    "\n",
    "# Get column names\n",
    "df.columns\n",
    "\n",
    "# Get number of rows and columns\n",
    "df.count()  # Number of rows\n",
    "len(df.columns)  # Number of columns\n",
    "\n",
    "# Summary statistics\n",
    "df.describe().show()\n",
    "df.summary().show()  # More detailed than describe()\n",
    "\n",
    "# Display first n rows as list of Row objects\n",
    "df.head(5)\n",
    "df.take(5)\n",
    "\n",
    "# First row\n",
    "df.first()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, expr, when, lit\n",
    "\n",
    "# Select columns\n",
    "df.select(\"name\", \"age\")\n",
    "df.select(col(\"name\"), col(\"age\"))\n",
    "\n",
    "# Select with expressions\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    (col(\"age\") + 5).alias(\"age_plus_5\"),\n",
    "    expr(\"age * 2 as age_doubled\")\n",
    ")\n",
    "\n",
    "# Conditional column\n",
    "df.withColumn(\"age_group\",\n",
    "    when(col(\"age\") < 18, \"Minor\")\n",
    "    .when((col(\"age\") >= 18) & (col(\"age\") < 65), \"Adult\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "# Cast column type\n",
    "df.withColumn(\"age\", col(\"age\").cast(\"double\"))\n",
    "df.withColumn(\"age\", col(\"age\").cast(DoubleType()))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    lower, upper, initcap, trim, ltrim, rtrim,\n",
    "    length, substring, concat, concat_ws,\n",
    "    split, regexp_replace, regexp_extract\n",
    ")\n",
    "\n",
    "# Case conversion\n",
    "df.withColumn(\"name_lower\", lower(col(\"name\")))\n",
    "df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
    "df.withColumn(\"name_title\", initcap(col(\"name\")))  # Title Case\n",
    "\n",
    "# Trim whitespace\n",
    "df.withColumn(\"name_trimmed\", trim(col(\"name\")))\n",
    "\n",
    "# String length\n",
    "df.withColumn(\"name_length\", length(col(\"name\")))\n",
    "\n",
    "# Substring (1-indexed)\n",
    "df.withColumn(\"name_first_3\", substring(col(\"name\"), 1, 3))\n",
    "\n",
    "# Concatenation\n",
    "df.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "df.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Split string into array\n",
    "df.withColumn(\"name_parts\", split(col(\"name\"), \" \"))\n",
    "\n",
    "# Regex replace\n",
    "df.withColumn(\"phone_clean\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\"))\n",
    "\n",
    "# Regex extract\n",
    "df.withColumn(\"area_code\", regexp_extract(col(\"phone\"), r\"(\\d{3})-\\d{3}-\\d{4}\", 1))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Operations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    current_date, current_timestamp, date_format,\n",
    "    year, month, dayofmonth, dayofweek, dayofyear,\n",
    "    hour, minute, second,\n",
    "    datediff, months_between, add_months, date_add, date_sub,\n",
    "    to_date, to_timestamp, unix_timestamp, from_unixtime\n",
    ")\n",
    "\n",
    "# Current date and timestamp\n",
    "df.withColumn(\"current_date\", current_date())\n",
    "df.withColumn(\"current_time\", current_timestamp())\n",
    "\n",
    "# Extract date parts\n",
    "df.withColumn(\"year\", year(col(\"date\")))\n",
    "df.withColumn(\"month\", month(col(\"date\")))\n",
    "df.withColumn(\"day\", dayofmonth(col(\"date\")))\n",
    "df.withColumn(\"day_of_week\", dayofweek(col(\"date\")))\n",
    "\n",
    "# Date arithmetic\n",
    "df.withColumn(\"date_plus_7\", date_add(col(\"date\"), 7))\n",
    "df.withColumn(\"date_minus_30\", date_sub(col(\"date\"), 30))\n",
    "df.withColumn(\"date_plus_2_months\", add_months(col(\"date\"), 2))\n",
    "\n",
    "# Date difference\n",
    "df.withColumn(\"days_diff\", datediff(col(\"end_date\"), col(\"start_date\")))\n",
    "df.withColumn(\"months_diff\", months_between(col(\"end_date\"), col(\"start_date\")))\n",
    "\n",
    "# String to date/timestamp\n",
    "df.withColumn(\"date\", to_date(col(\"date_string\"), \"yyyy-MM-dd\"))\n",
    "df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp_string\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Format date\n",
    "df.withColumn(\"formatted_date\", date_format(col(\"date\"), \"MMM dd, yyyy\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array and Map Operations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import (\n",
    "    array, array_contains, explode, size,\n",
    "    map_keys, map_values, explode as explode_map\n",
    ")\n",
    "\n",
    "# Create array column\n",
    "df.withColumn(\"numbers\", array(lit(1), lit(2), lit(3)))\n",
    "\n",
    "# Check if array contains value\n",
    "df.withColumn(\"has_value\", array_contains(col(\"array_col\"), \"value\"))\n",
    "\n",
    "# Array size\n",
    "df.withColumn(\"array_size\", size(col(\"array_col\")))\n",
    "\n",
    "# Explode array (creates new row for each element)\n",
    "df.withColumn(\"exploded\", explode(col(\"array_col\")))\n",
    "\n",
    "# Work with maps\n",
    "df.withColumn(\"keys\", map_keys(col(\"map_col\")))\n",
    "df.withColumn(\"values\", map_values(col(\"map_col\")))\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced PySpark Concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "Partitioning controls how data is distributed across the cluster.\n",
    "\n",
    "```python\n",
    "# Check number of partitions\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "# Repartition (full shuffle)\n",
    "df_repartitioned = df.repartition(10)\n",
    "df_repartitioned = df.repartition(10, \"column_name\")  # Partition by column\n",
    "\n",
    "# Coalesce (reduces partitions without full shuffle, more efficient)\n",
    "df_coalesced = df.coalesce(5)\n",
    "\n",
    "# Write with partitioning\n",
    "df.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"path/to/output\")\n",
    "```\n",
    "\n",
    "**When to use**:\n",
    "- Use `repartition()` when increasing partitions or need even distribution\n",
    "- Use `coalesce()` when decreasing partitions to save on shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and Persistence\n",
    "\n",
    "```python\n",
    "# Cache in memory (default storage level)\n",
    "df_cached = df.cache()\n",
    "\n",
    "# Persist with specific storage level\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Available storage levels:\n",
    "# - MEMORY_ONLY: Cache in memory only\n",
    "# - MEMORY_AND_DISK: Cache in memory, spill to disk if needed\n",
    "# - DISK_ONLY: Cache on disk only\n",
    "# - MEMORY_ONLY_SER: Serialize objects in memory\n",
    "# - MEMORY_AND_DISK_SER: Serialize in memory and disk\n",
    "\n",
    "# Unpersist (free up memory)\n",
    "df_cached.unpersist()\n",
    "\n",
    "# Check if cached\n",
    "df_cached.is_cached\n",
    "```\n",
    "\n",
    "**Best practices**:\n",
    "- Cache DataFrames that are reused multiple times\n",
    "- Cache after expensive transformations\n",
    "- Unpersist when no longer needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "\n",
    "Use for small datasets that need to be available on all nodes.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast small DataFrame in join\n",
    "large_df = spark.read.parquet(\"large_data.parquet\")\n",
    "small_df = spark.read.parquet(\"small_data.parquet\")\n",
    "\n",
    "# Broadcast join (more efficient)\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "\n",
    "# Broadcast Python variables\n",
    "broadcast_var = spark.sparkContext.broadcast({\"key\": \"value\"})\n",
    "# Access: broadcast_var.value\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "\n",
    "Shared variables for aggregating information across executors.\n",
    "\n",
    "```python\n",
    "# Create accumulator\n",
    "counter = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def process_row(row):\n",
    "    global counter\n",
    "    if row.age > 30:\n",
    "        counter.add(1)\n",
    "    return row\n",
    "\n",
    "# Use in transformations\n",
    "rdd = df.rdd.map(process_row)\n",
    "rdd.collect()\n",
    "\n",
    "print(f\"Count of rows with age > 30: {counter.value}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries\n",
    "\n",
    "```python\n",
    "# Register DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as count,\n",
    "        AVG(amount) as avg_amount\n",
    "    FROM people\n",
    "    WHERE age > 18\n",
    "    GROUP BY category\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "# Global temporary view (accessible across sessions)\n",
    "df.createGlobalTempView(\"global_people\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_people\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with JSON Data\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import from_json, to_json, get_json_object, json_tuple\n",
    "\n",
    "# Parse JSON string column\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", IntegerType())\n",
    "])\n",
    "\n",
    "df.withColumn(\"parsed\", from_json(col(\"json_string\"), json_schema))\n",
    "\n",
    "# Convert to JSON string\n",
    "df.withColumn(\"json_string\", to_json(struct(col(\"name\"), col(\"age\"))))\n",
    "\n",
    "# Extract specific field from JSON string\n",
    "df.withColumn(\"name\", get_json_object(col(\"json_string\"), \"$.name\"))\n",
    "\n",
    "# Extract multiple fields\n",
    "df.select(json_tuple(col(\"json_string\"), \"name\", \"age\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Nested Data\n",
    "\n",
    "```python\n",
    "# Select nested fields\n",
    "df.select(\"parent.child.grandchild\")\n",
    "df.select(col(\"parent.child.grandchild\"))\n",
    "\n",
    "# Flatten nested structure\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"nested.field1\").alias(\"field1\"),\n",
    "    col(\"nested.field2\").alias(\"field2\")\n",
    ")\n",
    "\n",
    "# Explode nested arrays\n",
    "df.select(\"id\", explode(\"array_field\").alias(\"item\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer\n",
    "\n",
    "Spark's Catalyst optimizer automatically optimizes logical and physical execution plans.\n",
    "\n",
    "**Optimization techniques**:\n",
    "1. Predicate pushdown: Push filters down to data source\n",
    "2. Projection pruning: Only read required columns\n",
    "3. Constant folding: Evaluate constant expressions at compile time\n",
    "4. Common subexpression elimination: Reuse computed values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tungsten Execution Engine\n",
    "\n",
    "Binary processing engine for improved performance:\n",
    "- Memory management and binary processing\n",
    "- Cache-aware computation\n",
    "- Code generation for improved CPU efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Use Built-in Functions\n",
    "```python\n",
    "# Good: Use built-in functions\n",
    "from pyspark.sql.functions import upper\n",
    "df.withColumn(\"name_upper\", upper(col(\"name\")))\n",
    "\n",
    "# Avoid: UDFs (slower due to serialization overhead)\n",
    "def upper_udf(s):\n",
    "    return s.upper()\n",
    "udf_upper = udf(upper_udf, StringType())\n",
    "df.withColumn(\"name_upper\", udf_upper(col(\"name\")))\n",
    "```\n",
    "\n",
    "### 2. Filter Early\n",
    "```python\n",
    "# Good: Filter before expensive operations\n",
    "df.filter(col(\"age\") > 18) \\\n",
    "  .join(other_df, \"id\") \\\n",
    "  .groupBy(\"category\").count()\n",
    "\n",
    "# Avoid: Filter after expensive operations\n",
    "df.join(other_df, \"id\") \\\n",
    "  .groupBy(\"category\").count() \\\n",
    "  .filter(col(\"age\") > 18)\n",
    "```\n",
    "\n",
    "### 3. Partition Pruning\n",
    "```python\n",
    "# Write data partitioned by commonly filtered columns\n",
    "df.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .parquet(\"path/to/data\")\n",
    "\n",
    "# Read with partition filter (only scans relevant partitions)\n",
    "df = spark.read.parquet(\"path/to/data\") \\\n",
    "    .filter((col(\"year\") == 2024) & (col(\"month\") == 12))\n",
    "```\n",
    "\n",
    "### 4. Avoid Shuffles When Possible\n",
    "```python\n",
    "# Shuffle operations (expensive):\n",
    "# - repartition()\n",
    "# - groupBy()\n",
    "# - join() (except broadcast join)\n",
    "# - distinct()\n",
    "# - sortBy()\n",
    "\n",
    "# Minimize shuffles by:\n",
    "# 1. Using broadcast joins for small tables\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "\n",
    "# 2. Pre-partitioning data\n",
    "df_partitioned = df.repartition(\"key\")\n",
    "# Subsequent groupBy on \"key\" won't shuffle\n",
    "```\n",
    "\n",
    "### 5. Choose Right File Format\n",
    "```python\n",
    "# Parquet (recommended):\n",
    "# - Columnar format\n",
    "# - Excellent compression\n",
    "# - Predicate pushdown support\n",
    "# - Schema evolution\n",
    "\n",
    "# Delta Lake (best for production):\n",
    "# - ACID transactions\n",
    "# - Time travel\n",
    "# - Schema enforcement\n",
    "# - Optimized writes\n",
    "\n",
    "# CSV (avoid for large data):\n",
    "# - No schema\n",
    "# - Poor compression\n",
    "# - Slow to parse\n",
    "```\n",
    "\n",
    "### 6. Optimize Joins\n",
    "```python\n",
    "# 1. Broadcast join for small tables (< 10MB)\n",
    "df1.join(broadcast(df2), \"key\")\n",
    "\n",
    "# 2. Bucket joins for large tables frequently joined\n",
    "df1.write \\\n",
    "    .bucketBy(10, \"key\") \\\n",
    "    .sortBy(\"key\") \\\n",
    "    .saveAsTable(\"bucketed_table1\")\n",
    "\n",
    "# 3. Sort-merge join (default for large tables)\n",
    "# Ensure data is pre-sorted and partitioned by join key\n",
    "```\n",
    "\n",
    "### 7. Monitor Query Plans\n",
    "```python\n",
    "# View physical plan\n",
    "df.explain()\n",
    "\n",
    "# View detailed plan with statistics\n",
    "df.explain(mode=\"extended\")\n",
    "\n",
    "# View cost-based optimization details\n",
    "df.explain(mode=\"cost\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Performance Issues\n",
    "\n",
    "```python\n",
    "# 1. Check data skew\n",
    "df.groupBy(\"partition_col\").count().show()\n",
    "\n",
    "# 2. Monitor Spark UI\n",
    "# - Access at port 4040 when job is running\n",
    "# - Check stages, tasks, and executors\n",
    "# - Look for stragglers (slow tasks)\n",
    "\n",
    "# 3. Enable adaptive query execution (Spark 3.0+)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "# 4. Tune memory settings\n",
    "spark.conf.set(\"spark.executor.memory\", \"4g\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"2g\")\n",
    "spark.conf.set(\"spark.memory.fraction\", \"0.8\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data Skew\n",
    "\n",
    "```python\n",
    "# Problem: Some partitions have much more data than others\n",
    "\n",
    "# Solution 1: Add salt to skewed keys\n",
    "from pyspark.sql.functions import rand, floor\n",
    "\n",
    "df_salted = df.withColumn(\"salt\", (floor(rand() * 10)).cast(\"int\"))\n",
    "df_salted = df_salted.withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), col(\"salt\")))\n",
    "\n",
    "# Solution 2: Broadcast join if one side is small\n",
    "result = large_skewed_df.join(broadcast(small_df), \"key\")\n",
    "\n",
    "# Solution 3: Repartition by multiple columns\n",
    "df.repartition(\"col1\", \"col2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common PySpark Patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Multiple Files\n",
    "```python\n",
    "# Read all CSV files in directory\n",
    "df = spark.read.csv(\"path/to/directory/*.csv\", header=True)\n",
    "\n",
    "# Read with wildcard pattern\n",
    "df = spark.read.parquet(\"path/*/year=2024/month=*/data.parquet\")\n",
    "\n",
    "# Union multiple DataFrames\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "dfs = [df1, df2, df3]\n",
    "combined = reduce(DataFrame.union, dfs)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Processing\n",
    "```python\n",
    "# Read only new data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df_new = spark.read.parquet(\"path/to/data\") \\\n",
    "    .filter(col(\"date\") >= yesterday)\n",
    "\n",
    "# Process and append\n",
    "df_processed = df_new.transform(my_transformation)\n",
    "df_processed.write.mode(\"append\").parquet(\"path/to/output\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "```python\n",
    "try:\n",
    "    df = spark.read.csv(\"path/to/file.csv\", header=True)\n",
    "    result = df.groupBy(\"category\").count()\n",
    "    result.write.mode(\"overwrite\").parquet(\"output/path\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    # Log error, send alert, etc.\n",
    "finally:\n",
    "    # Cleanup code\n",
    "    spark.catalog.clearCache()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Databricks** simplifies Spark operations with managed clusters, collaborative notebooks, and built-in optimization\n",
    "\n",
    "2. **PySpark DataFrames** are the primary abstraction for distributed data processing, offering high-level APIs with automatic optimization\n",
    "\n",
    "3. **ETL Pipeline** follows three phases:\n",
    "   - **Extract**: Read data from various sources\n",
    "   - **Transform**: Apply business logic and data transformations\n",
    "   - **Load**: Write results to target destinations\n",
    "\n",
    "4. **Performance** depends on:\n",
    "   - Using built-in functions over UDFs\n",
    "   - Filtering early and often\n",
    "   - Choosing appropriate file formats (Parquet/Delta)\n",
    "   - Managing partitions effectively\n",
    "   - Minimizing shuffles\n",
    "\n",
    "5. **Best Practices**:\n",
    "   - Cache frequently used DataFrames\n",
    "   - Use broadcast joins for small tables\n",
    "   - Monitor Spark UI for bottlenecks\n",
    "   - Handle data skew proactively\n",
    "   - Leverage Delta Lake for production workloads\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Practice with real datasets on Databricks Community Edition\n",
    "2. Explore Delta Lake for ACID transactions\n",
    "3. Learn Spark Streaming for real-time processing\n",
    "4. Study MLlib for machine learning at scale\n",
    "5. Master performance tuning and optimization techniques\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "- **Databricks Documentation**: https://docs.databricks.com/\n",
    "- **PySpark API Reference**: https://spark.apache.org/docs/latest/api/python/\n",
    "- **Apache Spark Guide**: https://spark.apache.org/docs/latest/\n",
    "- **Databricks Academy**: Free courses and certifications\n",
    "- **Community Forums**: https://community.databricks.com/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
