{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fda854-3e2c-44bf-8104-e67eadc3b2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interview Questions\n",
    "\n",
    "### Q1. What is the difference between a transformation and an action in Spark? Give at least 3 examples of each.\n",
    "\n",
    "**transformation**:  \n",
    "\n",
    "- It defines a new RDD or DataFrame from an existing one but **does not trigger execution immediately**. \n",
    "- Transformations are **lazy**, meaning Spark only records the lineage (execution plan) and waits until a result is actually needed. \n",
    "- This allows Spark to **optimize the execution plan before running it**. \n",
    "- Examples of transformations include `select()`, `filter()`, `map()`, `withColumn()`, and `groupBy()`.\n",
    "\n",
    "**action**:  \n",
    "\n",
    "- An **action**, on the other hand, **triggers the actual execution** of the Spark job \n",
    "- and **returns a result to the driver** or **writes data to external storage**. \n",
    "- Actions force Spark to materialize the computation defined by the transformations. \n",
    "- Examples of actions include `count()`, `collect()`, `show()`, `take()`, and `saveAsTextFile()`. \n",
    "\n",
    "In short, transformations build the plan, while actions execute it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07fd44e1-83c8-4ecd-bbe6-895c4c70307e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q2. Why does `spark.read.csv()` load all columns as `StringType` by default? What problems can this cause in aggregation or filtering?\n",
    "\n",
    "By default, `spark.read.csv()` loads all columns as `StringType` because \n",
    "- CSV files are schema-less text files, \n",
    "- and Spark avoids making incorrect assumptions about data types. \n",
    "- Inferring schema requires scanning the data, which can be expensive for large datasets, \n",
    "- so Spark prioritizes safety and performance unless `inferSchema=true` or an explicit schema is provided.\n",
    "\n",
    "This behavior can **cause issues in aggregation and filtering**.  \n",
    "\n",
    "For example, \n",
    "- numeric aggregations like `sum()` or `avg()` will fail or produce incorrect results if the column is treated as a string. \n",
    "- Similarly, filtering conditions can behave unexpectedly, such as `\"100\" < \"20\"` evaluating as true due to lexicographical comparison. \n",
    "\n",
    "To avoid these problems, it is best practice to \n",
    "- explicitly define a schema \n",
    "- or cast columns to the appropriate data types before performing analytics.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aed5d13-aa03-4a79-bfae-f5b7a67049ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q3. What does `explode()` do in Spark? In what scenario can `explode()` cause serious performance issues?\n",
    "\n",
    "The `explode()` function in Spark \n",
    "- takes an array or map column and **creates a new row for each element** in that collection. \n",
    "- Essentially, it “flattens” nested data so that each element becomes its own row while duplicating the values of the other columns.    \n",
    "\n",
    "This is commonly used when \n",
    "- working with JSON data, \n",
    "- nested arrays, \n",
    "- or event logs.\n",
    "\n",
    "However, `explode()` can cause serious performance issues when \n",
    "- applied to columns with **very large arrays or highly skewed data**. \n",
    "- In such cases, a single input row can generate thousands or millions of output rows, leading to data explosion, increased memory usage, and expensive shuffles downstream. \n",
    "- If not carefully managed, this can result in slow jobs or even out-of-memory errors on executors.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44055002-cab9-49cf-99ac-18e4885d6939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q4. Why is `groupBy()` considered a wide transformation? What happens under the hood when a wide transformation is executed?\n",
    "\n",
    "Unlike narrow transformations, where each output partition depends on a single input partition, wide transformations depend on multiple input partitions.   \n",
    "\n",
    "\n",
    "`groupBy()` is considered a **wide transformation** because \n",
    "- it requires data with the same key to be brought together, \n",
    "- which often means data must be redistributed across partitions. \n",
    "\n",
    "Under the hood, when a wide transformation like `groupBy()` is executed:  \n",
    "- Spark performs a **shuffle**. \n",
    "- This involves \n",
    "  - repartitioning the data based on the grouping key, \n",
    "  - writing intermediate data to disk, \n",
    "  - transferring it across the network, \n",
    "  - and then reading it back on the destination executors. \n",
    "- This shuffle process is expensive in terms of I/O, network, and latency, which is why wide transformations are typically the **main performance bottleneck** in Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35e9b17-3592-42d7-a316-0bc09016d3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Coding: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c136e2-2ba6-4150-afb5-7e04fe1c56f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 1: \n",
    "Assume you have a DataFrame peopleDF with the following schema:   \n",
    "name: string   \n",
    "age: integer  \n",
    "city: string   \n",
    "\n",
    "Task: \n",
    "1.  Remove duplicate rows  \n",
    "2.  Keep only people whose age is between 20 and 40  \n",
    "3.  Create a new column is_adult:  \n",
    "    - 1 if age ≥ 18  \n",
    "    - 0 otherwise  \n",
    "4.  Select only the following columns:  \n",
    "    - name  \n",
    "    - age  \n",
    "    - is_adult  \n",
    "5.  Sort the result by age in descending order  \n",
    "Use DataFrame API only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f042453-5412-46b1-9620-508e3ab61178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCB Original DataFrame:\n+-------+---+--------+\n|   name|age|    city|\n+-------+---+--------+\n|  Alice| 25|     NYC|\n|    Bob| 35|      LA|\n|Charlie| 15| Chicago|\n|  Diana| 28|  Boston|\n|    Eve| 45| Seattle|\n|  Frank| 22|  Austin|\n|  Grace| 38|  Denver|\n|  Henry| 50|   Miami|\n|  Alice| 25|     NYC|\n|    Bob| 35|      LA|\n|    Ivy| 19|Portland|\n|   Jack| 40| Phoenix|\n|   Kate| 17|  Dallas|\n|    Leo| 30| Houston|\n+-------+---+--------+\n\nTotal rows: 14\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Question1_Solution\").getOrCreate()\n",
    "\n",
    "# CREATE SAMPLE DATAFRAME\n",
    "data = [\n",
    "    ('Alice', 25, 'NYC'),\n",
    "    ('Bob', 35, 'LA'),\n",
    "    ('Charlie', 15, 'Chicago'),\n",
    "    ('Diana', 28, 'Boston'),\n",
    "    ('Eve', 45, 'Seattle'),\n",
    "    ('Frank', 22, 'Austin'),\n",
    "    ('Grace', 38, 'Denver'),\n",
    "    ('Henry', 50, 'Miami'),\n",
    "    ('Alice', 25, 'NYC'),      # Duplicate\n",
    "    ('Bob', 35, 'LA'),         # Duplicate\n",
    "    ('Ivy', 19, 'Portland'),\n",
    "    ('Jack', 40, 'Phoenix'),\n",
    "    ('Kate', 17, 'Dallas'),\n",
    "    ('Leo', 30, 'Houston')\n",
    "]\n",
    "\n",
    "peopleDF = spark.createDataFrame(data, ['name', 'age', 'city'])\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB Original DataFrame:\")\n",
    "peopleDF.show()\n",
    "print(f\"Total rows: {peopleDF.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55d5f63-e70b-4399-a6e9-c584b6e4225e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRows with duplicates (showing all occurrences):\n+-----+---+----+\n| name|age|city|\n+-----+---+----+\n|Alice| 25| NYC|\n|  Bob| 35|  LA|\n|Alice| 25| NYC|\n|  Bob| 35|  LA|\n+-----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "# before removing duplicates \n",
    "# Find duplicates: rows that appear more than once\n",
    "print(\"\\nRows with duplicates (showing all occurrences):\")\n",
    "\n",
    "# Group by all columns and count\n",
    "duplicates = peopleDF.groupBy('name', 'age', 'city') \\\n",
    "    .count() \\\n",
    "    .filter(col('count') > 1) \\\n",
    "    .drop('count')\n",
    "\n",
    "# Join back to get all duplicate rows\n",
    "duplicate_rows = peopleDF.join(duplicates, ['name', 'age', 'city'], 'inner')\n",
    "duplicate_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13c1a6d-602f-44f3-b388-5ce30cb7ce63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ After removing duplicates:\n+-------+---+--------+\n|   name|age|    city|\n+-------+---+--------+\n|  Alice| 25|     NYC|\n|    Bob| 35|      LA|\n|Charlie| 15| Chicago|\n|  Diana| 28|  Boston|\n|    Eve| 45| Seattle|\n|  Frank| 22|  Austin|\n|  Grace| 38|  Denver|\n|  Henry| 50|   Miami|\n|    Ivy| 19|Portland|\n|   Jack| 40| Phoenix|\n|   Kate| 17|  Dallas|\n|    Leo| 30| Houston|\n+-------+---+--------+\n\nTotal rows: 12\n"
     ]
    }
   ],
   "source": [
    "# 1. REMOVE DUPLICATE ROWS\n",
    "peopleDF_no_duplicates = peopleDF.dropDuplicates()\n",
    "\n",
    "print(\"\\n✅ After removing duplicates:\")\n",
    "peopleDF_no_duplicates.show()\n",
    "print(f\"Total rows: {peopleDF_no_duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c385326-7c4f-49b6-a075-f236494fccd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ After filtering (age between 20 and 40):\n+-----+---+-------+\n| name|age|   city|\n+-----+---+-------+\n|Alice| 25|    NYC|\n|  Bob| 35|     LA|\n|Diana| 28| Boston|\n|Frank| 22| Austin|\n|Grace| 38| Denver|\n| Jack| 40|Phoenix|\n|  Leo| 30|Houston|\n+-----+---+-------+\n\nTotal rows: 7\n"
     ]
    }
   ],
   "source": [
    "# 2. FILTER AGE BETWEEN 20 AND 40\n",
    "# ============================================\n",
    "peopleDF_filtered = peopleDF_no_duplicates.filter(\n",
    "    (col('age') >= 20) & (col('age') <= 40)\n",
    ")\n",
    "\n",
    "print(\"\\n✅ After filtering (age between 20 and 40):\")\n",
    "peopleDF_filtered.show()\n",
    "print(f\"Total rows: {peopleDF_filtered.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92502ed-393a-461c-94e9-fa8fcaf9c62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ After adding is_adult column:\n+-------+---+--------+--------+\n|   name|age|    city|is_adult|\n+-------+---+--------+--------+\n|  Alice| 25|     NYC|       1|\n|    Bob| 35|      LA|       1|\n|Charlie| 15| Chicago|       0|\n|  Diana| 28|  Boston|       1|\n|    Eve| 45| Seattle|       1|\n|  Frank| 22|  Austin|       1|\n|  Grace| 38|  Denver|       1|\n|  Henry| 50|   Miami|       1|\n|    Ivy| 19|Portland|       1|\n|   Jack| 40| Phoenix|       1|\n|   Kate| 17|  Dallas|       0|\n|    Leo| 30| Houston|       1|\n+-------+---+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. CREATE NEW COLUMN is_adult\n",
    "\n",
    "peopleDF_with_adult = peopleDF_no_duplicates.withColumn(\n",
    "    'is_adult',\n",
    "    when(col('age') >= 18, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\n✅ After adding is_adult column:\")\n",
    "peopleDF_with_adult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9a5848-bdd7-409f-870f-8933eadd3bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ After selecting columns:\n+-------+---+--------+\n|   name|age|is_adult|\n+-------+---+--------+\n|  Alice| 25|       1|\n|    Bob| 35|       1|\n|Charlie| 15|       0|\n|  Diana| 28|       1|\n|    Eve| 45|       1|\n|  Frank| 22|       1|\n|  Grace| 38|       1|\n|  Henry| 50|       1|\n|    Ivy| 19|       1|\n|   Jack| 40|       1|\n|   Kate| 17|       0|\n|    Leo| 30|       1|\n+-------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. SELECT SPECIFIC COLUMNS\n",
    "\n",
    "peopleDF_selected = peopleDF_with_adult.select('name', 'age', 'is_adult')\n",
    "\n",
    "print(\"\\n✅ After selecting columns:\")\n",
    "peopleDF_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f601ca9-5661-4f7d-865e-75ec1c8d6caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ FINAL RESULT:\n+-------+---+--------+\n|   name|age|is_adult|\n+-------+---+--------+\n|  Henry| 50|       1|\n|    Eve| 45|       1|\n|   Jack| 40|       1|\n|  Grace| 38|       1|\n|    Bob| 35|       1|\n|    Leo| 30|       1|\n|  Diana| 28|       1|\n|  Alice| 25|       1|\n|  Frank| 22|       1|\n|    Ivy| 19|       1|\n|   Kate| 17|       0|\n|Charlie| 15|       0|\n+-------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. SORT BY AGE DESCENDING\n",
    "\n",
    "final_result = peopleDF_selected.orderBy(col('age').desc())\n",
    "\n",
    "print(\"\\n✅ FINAL RESULT:\")\n",
    "final_result.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3f27c5-fe24-48fe-b7fc-7e36de6e61d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 2: \n",
    "You are given a CSV file users.csv with the following columns:    \n",
    "\n",
    "user_id,name,signup_date,score     \n",
    " \n",
    "Example data:   \n",
    "1,Alice,2023-01-01,90     \n",
    "2,Bob,2023-02-10,85    \n",
    " \n",
    "Task:    \n",
    "1.  Define an explicit schema using StructType  \n",
    "2.  Read the CSV using spark.read with:  \n",
    "    - header = true  \n",
    "    - your custom schema  \n",
    "3.  Print the schema  \n",
    "4.  Show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102af4a6-ab5c-4c25-a5bb-47d1a566d1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/Workspace/Users/shen.cheng@northeastern.edu/Drafts/3_1_QA.ipynb', name='3_1_QA.ipynb', size=18816, modificationTime=1766149670995),\n",
       " FileInfo(path='dbfs:/Workspace/Users/shen.cheng@northeastern.edu/Drafts/sparkSQL Intro.ipynb', name='sparkSQL Intro.ipynb', size=88376, modificationTime=1766145497474),\n",
       " FileInfo(path='dbfs:/Workspace/Users/shen.cheng@northeastern.edu/Drafts/users.csv', name='users.csv', size=246, modificationTime=1766147665752)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Question2_Solution\").getOrCreate()\n",
    "\n",
    "# users.csv has been uploaded by Catalog\n",
    "dbutils.fs.ls('/Workspace/Users/shen.cheng@northeastern.edu/Drafts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f138a-7227-4569-96ee-61c18049e16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- user_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- signup_date: date (nullable = true)\n |-- score: long (nullable = true)\n\n+-------+-------+-----------+-----+\n|user_id|   name|signup_date|score|\n+-------+-------+-----------+-----+\n|      1|  Alice| 2023-01-01|   90|\n|      2|    Bob| 2023-02-10|   85|\n|      3|Charlie| 2023-03-15|   78|\n|      4|  Diana| 2023-04-20|   92|\n|      5|    Eve| 2023-05-05|   88|\n+-------+-------+-----------+-----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# 1. DEFINE EXPLICIT SCHEMA\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"signup_date\", DateType(), nullable=True),\n",
    "    StructField(\"score\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# 2. Read from FileStore \n",
    "df = spark.table(\"workspace.default.users\")\n",
    "\n",
    "# Note: The table already has a schema, but we defined our expected schema above\n",
    "# In a real CSV scenario, would use:\n",
    "# df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"path/to/file.csv\")\n",
    "\n",
    "# 3. Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# 4. Show first 5 rows\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_1_QA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}