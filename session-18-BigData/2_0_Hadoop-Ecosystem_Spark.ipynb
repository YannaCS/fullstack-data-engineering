{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem\n",
    "\n",
    "Hadoop is an open-source framework for distributed storage and processing of large datasets across clusters of commodity hardware.\n",
    "\n",
    "## Core Hadoop Components\n",
    "\n",
    "<img src=\"./pic/2_HADOOP-ECOSYSTEM.png\" width=700>\n",
    "\n",
    "- Processing Layer: MapReduce / Spark / Hive / Pig\n",
    "- Resource Management: YARN (ResourceManager, NodeManager, ApplicationMaster)\n",
    "- Storage Layer: HDFS (NameNode, DataNode)\n",
    "- Data Ingestion: Kafka, Flume, Sqoop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS (Hadoop Distributed File System)\n",
    "\n",
    "### Why HDFS?\n",
    "\n",
    "**Problem**: How do you store a 1TB log file?\n",
    "- Single machine can't store it safely\n",
    "- Single machine isn't reliable (hardware failures)\n",
    "- Single machine can't read it fast enough\n",
    "\n",
    "**Solution**: HDFS **splits, distributes, and replicates data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Design Principles\n",
    "\n",
    "```text\n",
    "Original large File (1TB)\n",
    "       │\n",
    "       ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│        1. Split into Blocks (default 128 MB each)            │\n",
    "│  [Block 1] [Block 2] [Block 3] ... [Block 8192]              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "       │\n",
    "       ▼\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│     2. Blocks are distribute Across DataNodes (machines)       │\n",
    "│                                                                │\n",
    "│  DataNode 1    DataNode 2    DataNode 3    DataNode 4          │\n",
    "│  ┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐         │\n",
    "│  │Block 1  │   │Block 1  │   │Block 2  │   │Block 1  │ ←Replica│\n",
    "│  │Block 2  │   │Block 3  │   │Block 3  │   │Block 4  │         │\n",
    "│  │Block 5  │   │Block 4  │   │Block 6  │   │Block 5  │         │\n",
    "│  └─────────┘   └─────────┘   └─────────┘   └─────────┘         │\n",
    "│  3. Each block is stored with multiple replicas (default: 3)   │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Key Roles:   \n",
    "\n",
    "**1. NameNode (Master)**\n",
    "\n",
    "| Responsibility | Description |\n",
    "|---------------|-------------|\n",
    "| Metadata Management | Tracks which blocks make up each file |\n",
    "| Block Location | Knows where each block is stored |\n",
    "| Namespace Management | Maintains file system directory tree |\n",
    "| Client Coordination | Directs clients to appropriate DataNodes |\n",
    "\n",
    "\n",
    "NameNode Metadata Example:\n",
    "```text\n",
    "/user/data/sales.csv\n",
    "├── Block 1 → [DataNode1, DataNode3, DataNode5]\n",
    "├── Block 2 → [DataNode2, DataNode4, DataNode6]\n",
    "└── Block 3 → [DataNode1, DataNode2, DataNode4]\n",
    "```\n",
    "\n",
    "**2. DataNode (Worker)**\n",
    "\n",
    "| Responsibility | Description |\n",
    "|---------------|-------------|\n",
    "| Block Storage | Stores actual data blocks |\n",
    "| Block Operations | Handles read/write requests |\n",
    "| Heartbeat | Reports status to NameNode |\n",
    "| Replication | Participates in block replication |\n",
    "\n",
    "**3. Client**\n",
    "\n",
    "- Applications that read/write data (**Spark, MapReduce, Hive**)\n",
    "- Communicates with NameNode for metadata\n",
    "- Communicates with DataNodes for actual data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NameNode VS DataNode \n",
    "\n",
    "| Aspect | NameNode | DataNode |\n",
    "|------|---------|----------|\n",
    "| Role | Master / Controller | Worker / Storage |\n",
    "| Stores actual data | No | Yes |\n",
    "| Stores metadata | Yes | No |\n",
    "| Metadata includes | File names, directory structure, permissions, block locations | Block IDs only |\n",
    "| Client interaction | Provides block locations to clients | Serves data directly to clients |\n",
    "| Failure impact | Critical (cluster becomes unavailable) | Non-critical (data replicated elsewhere) |\n",
    "| Scalability | Vertical | Horizontal |\n",
    "| Typical count | 1 active (+ standby) | Many (hundreds or thousands) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDFS Configuration Defaults\n",
    "\n",
    "| Parameter | Default Value | Description |\n",
    "|-----------|---------------|-------------|\n",
    "| Block Size | 128 MB | Size of each data block |\n",
    "| Replication Factor | 3 | Number of copies of each block |\n",
    "| Heartbeat Interval | 3 seconds | DataNode health check frequency |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN (Yet Another Resource Negotiator)\n",
    "\n",
    "### What Does YARN Do?\n",
    "\n",
    "YARN is the **resource management** layer of Hadoop. It decides **who** runs what, \n",
    "**where** it runs, and with **how many** resources:    \n",
    "- Tracks available machines in the cluster\n",
    "- Manages CPU and memory resources\n",
    "- Schedules jobs\n",
    "- Monitors execution and retries failed tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YARN Components\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                          YARN Architecture                          │\n",
    "│  ┌────────────────────────────────────────────────────────────────┐ │\n",
    "│  │                    ResourceManager (RM)                        │ │\n",
    "│  │                    [Global Authority]                          │ │\n",
    "│  │    • Manages cluster-wide resources                            │ │\n",
    "│  │    • Accepts job submissions                                   │ │\n",
    "│  │    • Schedules applications                                    │ │\n",
    "│  └────────────────────────────────────────────────────────────────┘ │\n",
    "│                              │                                      │\n",
    "│              ┌───────────────┼───────────────┐                      │\n",
    "│              ▼               ▼               ▼                      │\n",
    "│  ┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐     │\n",
    "│  │  NodeManager 1   │ │  NodeManager 2   │ │  NodeManager 3   │     │\n",
    "│  │  [Worker Node]   │ │  [Worker Node]   │ │  [Worker Node]   │     │\n",
    "│  │                  │ │                  │ │                  │     │\n",
    "│  │  ┌────────────┐  │ │  ┌────────────┐  │ │  ┌────────────┐  │     │\n",
    "│  │  │ Container  │  │ │  │ Container  │  │ │  │ Container  │  │     │\n",
    "│  │  │  (App      │  │ │  │  (Task)    │  │ │  │  (Task)    │  │     │\n",
    "│  │  │  Master)   │  │ │  │            │  │ │  │            │  │     │\n",
    "│  │  └────────────┘  │ │  └────────────┘  │ │  └────────────┘  │     │\n",
    "│  └──────────────────┘ └──────────────────┘ └──────────────────┘     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Component Responsibilities\n",
    "\n",
    "| Component | Scope | Responsibilities |\n",
    "|-----------|-------|------------------|\n",
    "| **ResourceManager (RM)** | Cluster-wide | Global resource allocation, job scheduling |\n",
    "| **NodeManager (NM)** | Per-machine | Reports local resources, manages containers |\n",
    "| **ApplicationMaster (AM)** | Per-application | Requests resources, coordinates tasks |\n",
    "| Container | Per-task | Isolated execution environment |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YARN Job Execution Flow\n",
    "\n",
    "<img src=\"./pic/2_Job-execution-process-in-YARN.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <font color=brown>Client</font> **submits application** to <font color=orange>ResourceManager</font>,   \n",
    "\n",
    "   <font color=orange>ResourceManager</font> **allocates container** for <font color=blue>ApplicationMaster</font>\n",
    "\n",
    "2. <font color=blue>ApplicationMaster</font> starts and **registers** with <font color=orange>ResourceManager</font>\n",
    "\n",
    "3. <font color=blue>ApplicationMaster</font> **requests containers** for tasks\n",
    "\n",
    "4. <font color=orange>ResourceManager</font> **allocates containers** on <font color=green>NodeManagers</font>\n",
    "\n",
    "5. <font color=blue>ApplicationMaster</font> **launches tasks** in containers,    \n",
    "\n",
    "   **Tasks execute and report progress** to <font color=blue>ApplicationMaster</font>\n",
    "\n",
    "6. Application **completes**, <font color=blue>ApplicationMaster</font> **deregisters**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce vs Spark\n",
    "> talked each in previous notes\n",
    "\n",
    "### MapReduce (First Generation Hadoop compute engine)\n",
    "\n",
    "**Characteristics**:\n",
    "- **Disk-based** execution model\n",
    "- Data written to disk after each stage\n",
    "- High latency due to I/O overhead\n",
    "- Reliable and scalable\n",
    "- **Best For**: **Long-running batch jobs** where latency isn't critical\n",
    "\n",
    "**Execution Pattern**:\n",
    "```text\n",
    "Input → Map → [Disk] → Shuffle → [Disk] → Reduce → Output\n",
    "                ↑                    ↑\n",
    "           Disk I/O            Disk I/O\n",
    "```\n",
    "\n",
    "\n",
    "**Example**: Word Count in MapReduce\n",
    "```text\n",
    "Map Phase:    \"hello world\" → [(\"hello\", 1), (\"world\", 1)]\n",
    "Shuffle:      Group by key\n",
    "Reduce Phase: [(\"hello\", [1,1,1])] → [(\"hello\", 3)]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Spark (Next Generation Hadoop compute engine)\n",
    "\n",
    "Apache Spark running on a Hadoop cluster (specifically using YARN as its cluster manager and often HDFS for storage).   \n",
    "\n",
    "Designed to replace MapReduce for most workloads\n",
    "\n",
    "**Characteristics**:\n",
    "- **In-memory** execution\n",
    "- DAG-based optimization\n",
    "- Low latency, interactive analytics\n",
    "- Unified engine (supports batch, SQL, streaming, ML)\n",
    "- **Best For**: Interactive analytics, iterative algorithms, real-time processing\n",
    "\n",
    "**Execution Pattern**:\n",
    "```text\n",
    "Input → Transform → Transform → Transform → Output\n",
    "              ↑           ↑           ↑\n",
    "           Memory     Memory     Memory\n",
    "           (cached)   (cached)   (cached)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | MapReduce | Spark |\n",
    "|--------|-----------|-------|\n",
    "| Execution Model | Disk-based | In-memory |\n",
    "| Speed | Slower | 10-100x faster |\n",
    "| Ease of Use | Complex (Java) | Simple (Python/Scala/SQL) |\n",
    "| Iterative Algorithms | Poor | Excellent |\n",
    "| Interactive Queries | Not suitable | Excellent |\n",
    "| Streaming | Not native | Native support |\n",
    "| Machine Learning | Limited | MLlib integrated |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is a **unified, distributed data processing engine** designed for large-scale data analytics.   \n",
    "\n",
    "Confusion with 'Hadoop Spark':   \n",
    "- Apache Spark is the actual **distributed computing framework**, the open-source project maintained by the Apache Software Foundation. \n",
    "- It's a standalone engine for large-scale data processing that can run on **various cluster managers**. \n",
    "- It was originally developed to work with Hadoop's ecosystem to replace MapReduce.\n",
    "- But Spark itself is independent and can run on other platforms like Kubernetes, Mesos, or even standalone mode without any Hadoop components.\n",
    "- So there's only one Spark (Apache Spark)\n",
    "\n",
    "### Core Capabilities\n",
    "\n",
    "```text\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                        Apache Spark                                │\n",
    "│                                                                    │\n",
    "│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐   │\n",
    "│  │  Spark SQL  │ │  Spark      │ │   MLlib     │ │  GraphX     │   │\n",
    "│  │  (SQL &     │ │  Streaming  │ │  (Machine   │ │  (Graph     │   │\n",
    "│  │  DataFrames)│ │  (Real-time)│ │  Learning)  │ │  Processing)│   │\n",
    "│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘   │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │                     Spark Core (RDD)                        │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │              Cluster Manager (YARN / K8s / Standalone)      │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "- Large-scale batch data processing\n",
    "- SQL analytics via Spark SQL\n",
    "- Structured Streaming for real-time data\n",
    "- Graph processing\n",
    "- Machine Learning via MLlib\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "| Characteristic | Description |\n",
    "|---------------|-------------|\n",
    "| Distributed | Processes data across multiple machines |\n",
    "| Fault-tolerant | Automatically recovers from failures |\n",
    "| In-memory–first execution mode | Keeps data in RAM for speed |\n",
    "| Highly scalable across clusters | Handles petabytes of data |\n",
    "| Multi-language | Scala, Java, Python, R |\n",
    "|One of the largest open-source data projects||\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Application\n",
    "\n",
    "<img src=\"./pic/2_spark_app.png\" width=500>\n",
    "\n",
    "### Component Responsibilities\n",
    "\n",
    "| Component | Responsibilities |\n",
    "|-----------|------------------|\n",
    "| **Driver** | Runs main program, builds DAG, splits into stages/tasks, schedules work, collects results |\n",
    "| **Cluster Manager** | Allocates resources for executors (YARN, Kubernetes, Standalone) |\n",
    "| **Executors** (on worker nodes) | Execute tasks in parallel, store cached data in memory, report status to driver |\n",
    "| **Tasks** | Smallest unit of work, typically 1 task per partition per stage |\n",
    "\n",
    "### Execution Flow\n",
    "\n",
    "```text\n",
    "User Code → SparkContext → DAG → Stages → Tasks → Executors\n",
    "```\n",
    "\n",
    "1. User writes transformations and actions\n",
    "2. SparkContext builds **DAG (Directed Acyclic Graph)**\n",
    "3. DAG is divided into **stages** (at shuffle boundaries)\n",
    "4. Stages are divided into **tasks** (one per partition)\n",
    "5. Tasks are sent to executors for **parallel execution**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark is Fast\n",
    "\n",
    "### 1. RDD-Based Data Abstraction\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)**:\n",
    "- **Immutable** distributed collections\n",
    "- Data split into logical partitions\n",
    "- Parallel processing across cluster nodes\n",
    "\n",
    "```python\n",
    "# RDD partitioning example\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Distributed across 3 partitions\n",
    "# Partition 1: [1, 2, 3, 4]\n",
    "# Partition 2: [5, 6, 7]\n",
    "# Partition 3: [8, 9, 10]\n",
    "\n",
    "rdd = sc.parallelize(data, 3)  # 3 partitions\n",
    "```\n",
    "\n",
    "### 2. In-Memory Computing\n",
    "\n",
    "- Intermediate results cached in RAM\n",
    "- Reduced disk I/O\n",
    "- Optimized for iterative workloads\n",
    "\n",
    "```text\n",
    "Traditional Disk-Based Processing:\n",
    "Step 1 → Write to Disk → Read from Disk → Step 2 → Write to Disk → Step 3\n",
    "         ~100ms          ~100ms                    ~100ms\n",
    "\n",
    "Spark In-Memory Processing:\n",
    "Step 1 → Memory Cache → Step 2 → Memory Cache → Step 3\n",
    "         ~0.1ms         ~0.1ms\n",
    "         \n",
    "Speed Improvement: 100-1000x for iterative workloads\n",
    "```\n",
    "\n",
    "\n",
    "### 3. Execution Efficiency\n",
    "\n",
    "- **Pipeline execution**: Multiple operations combined within stages\n",
    "- **Lazy evaluation**: Operations only execute when results are needed\n",
    "- **Minimized data movement**: Intelligent task placement\n",
    "- **Optimized scheduling**: Tasks run where data resides\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "### Method 1: Parallelizing Existing Collections\n",
    "\n",
    "```python\n",
    "# Convert in-memory Python data to RDD\n",
    "# Best for: testing, learning, small datasets\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# With specified partitions\n",
    "rdd = sc.parallelize(data, numSlices=4)\n",
    "```\n",
    "\n",
    "### Method 2: Loading from External Storage\n",
    "\n",
    "```python\n",
    "# From local file system\n",
    "rdd = sc.textFile(\"file:///path/to/data.txt\")\n",
    "\n",
    "# From HDFS\n",
    "rdd = sc.textFile(\"hdfs://namenode:9000/data/file.txt\")\n",
    "\n",
    "# From Amazon S3\n",
    "rdd = sc.textFile(\"s3a://bucket-name/data/file.txt\")\n",
    "\n",
    "# Multiple files (wildcard)\n",
    "rdd = sc.textFile(\"hdfs:///data/logs/*.log\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐️ Transformations and Actions\n",
    "\n",
    "### Lazy Evaluation\n",
    "\n",
    "Key Rule: **No action → No execution**\n",
    "\n",
    "Transformations build a plan (DAG)   \n",
    "Actions trigger the plan execution -> stages -> tasks   \n",
    "\n",
    "Example:\n",
    "```python\n",
    "rdd.map(...)      # Just builds plan, nothing executes\n",
    "   .filter(...)   # Just builds plan, nothing executes\n",
    "   .count()       # ACTION! Now everything executes\n",
    "```\n",
    "\n",
    "### Transformations (Lazy, build a plan)\n",
    "\n",
    "**Return a new RDD (or DataFrame) without executing immediately**.\n",
    "\n",
    "```python\n",
    "# Common transformations\n",
    "rdd.map(lambda x: x * 2)           # Apply function to each element\n",
    "rdd.filter(lambda x: x > 5)        # Keep elements matching condition\n",
    "rdd.flatMap(lambda x: x.split())   # Map + flatten results\n",
    "rdd.distinct()                      # Remove duplicates\n",
    "rdd.union(other_rdd)               # Combine two RDDs\n",
    "```\n",
    "\n",
    "### Actions (Trigger Execution)\n",
    "\n",
    "**Start a job and return results to driver or storage**.\n",
    "\n",
    "```python\n",
    "# Common actions\n",
    "rdd.count()                        # Count elements\n",
    "rdd.collect()                      # Return all elements to driver\n",
    "rdd.take(n)                        # Return first n elements\n",
    "rdd.first()                        # Return first element\n",
    "rdd.reduce(lambda a, b: a + b)     # Aggregate elements\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow vs Wide Transformations\n",
    "\n",
    "### Narrow Transformations\n",
    "\n",
    "**Definition**: **Each output partitio**n depends on **ONE input partition**\n",
    "\n",
    "**Characteristics**:\n",
    "- No shuffle required\n",
    "- No network data movement\n",
    "- Can execute within same stage\n",
    "- Fast and efficient\n",
    "\n",
    "```text\n",
    "Narrow Transformation (map):\n",
    "\n",
    "Partition 1 ──map──▶ Partition 1'\n",
    "Partition 2 ──map──▶ Partition 2'\n",
    "Partition 3 ──map──▶ Partition 3'\n",
    "\n",
    "Each output depends only on its corresponding input\n",
    "```\n",
    "\n",
    "**Examples**: `map`, `filter`, `select`, `withColumn`, `union`\n",
    "\n",
    "### Wide Transformations\n",
    "\n",
    "**Definition**: Output partitions depend on **MULTIPLE input partitions**\n",
    "\n",
    "**Characteristics**:\n",
    "- Requires shuffle across network\n",
    "- Creates stage boundary\n",
    "- More expensive (I/O and network)\n",
    "\n",
    "```text\n",
    "Wide Transformation (groupBy):\n",
    "\n",
    "Partition 1 ─┐\n",
    "             ├──shuffle──▶ Partition 1' (all keys \"A\")\n",
    "Partition 2 ─┤\n",
    "             ├──shuffle──▶ Partition 2' (all keys \"B\")\n",
    "Partition 3 ─┘\n",
    "             └──shuffle──▶ Partition 3' (all keys \"C\")\n",
    "\n",
    "Data must move between partitions based on keys\n",
    "```\n",
    "\n",
    "**Examples**: `groupBy`, `reduceByKey`, `join`, `distinct`, `repartition`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Challenges\n",
    "\n",
    "### System Dependencies\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| No built-in storage | Relies on external storage (HDFS, S3) |\n",
    "| Cluster manager required | Needs YARN, Kubernetes, or Standalone for scale |\n",
    "| External coordination | Depends on ZooKeeper for some features |\n",
    "\n",
    "### Security\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| Limited native security | Basic authentication and authorization |\n",
    "| Platform-dependent | Security typically from: Kerberos, IAM, Ranger |\n",
    "| Network security | Requires proper firewall configuration |\n",
    "\n",
    "### Performance & Cost\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| Performance tuning is hard | Complex: partitions, shuffle, skew, memory |\n",
    "| Capacity management | Difficult to right-size clusters |\n",
    "| Cost optimization | Challenging in cloud environments |\n",
    "| Data skew | Uneven data distribution causes bottlenecks |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Spark Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data (Transformation - lazy)\n",
    "df = spark.read.parquet(\"s3://data/sales/\")\n",
    "\n",
    "# Apply transformations (all lazy)\n",
    "result = (\n",
    "    df\n",
    "    .filter(col(\"year\") == 2024)              # Narrow\n",
    "    .filter(col(\"amount\") > 0)                 # Narrow\n",
    "    .groupBy(\"region\", \"product_category\")     # Wide (shuffle)\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        avg(\"amount\").alias(\"avg_sale\"),\n",
    "        count(\"*\").alias(\"num_transactions\")\n",
    "    )\n",
    "    .orderBy(col(\"total_sales\").desc())        # Wide (shuffle)\n",
    ")\n",
    "\n",
    "# Action - triggers execution\n",
    "result.show(10)\n",
    "\n",
    "# Another action - write results\n",
    "result.write.mode(\"overwrite\").parquet(\"s3://output/sales_summary/\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
