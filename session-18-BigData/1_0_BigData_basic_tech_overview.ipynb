{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Big Data](#introduction-to-big-data)\n",
    "   - [The 4 Vs of Big Data](#the-4-vs-of-big-data)\n",
    "   - [Limitations of Traditional Databases](#limitations-of-traditional-databases)\n",
    "2. [Big Data System](#big-data-system)\n",
    "   - [Three Core Challenges](#three-core-challenges)\n",
    "3. [MapReduce](#5-mapreduce)\n",
    "4. [Apache Spark](#6-apache-spark)\n",
    "5. [Batch vs Streaming Processing](#7-batch-vs-streaming-processing)\n",
    "\n",
    "6.  [Data Lake](#10-data-lake)\n",
    "7.  [ETL vs ELT](#11-etl-vs-elt)\n",
    "8.  [Data Warehouse](#12-data-warehouse)\n",
    "9.  [Data Mart](#13-data-mart)\n",
    "\n",
    "10. [Cloud Platforms & AWS Services](#15-cloud-platforms--aws-services)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data\n",
    "\n",
    "## [What is Big Data](https://www.upgrad.com/blog/what-is-big-data-types-characteristics-benefits-and-examples/)\n",
    "\n",
    "datasets that are **so large, complex, or fast-moving** that they cannot be managed or analyzed using traditional data processing tools.  \n",
    "\n",
    "As Gartner defines it, big data is \"**high-volume, high-velocity, and high-variety information assets** that require innovative processing for enhanced decision-making, insight discovery, and process optimization.\"\n",
    "\n",
    "\n",
    "<img src=\"./pic/1_What_Is_Big_Data.jpg\" width=400>  \n",
    "\n",
    "<img src=\"./pic/1_how_big_data_works.webp\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Big Data Matters\n",
    "\n",
    "Big Data has become essential in modern computing due to several converging factors:\n",
    "\n",
    "| Driver | Description |\n",
    "|--------|-------------|\n",
    "| **Explosive Data Growth** | Global data doubles every 2-3 years; by 2025, estimated 175+ zettabytes worldwide |\n",
    "| **Digital Interactions** | Every click, swipe, purchase, and social media post generates data |\n",
    "| **IoT Devices & Sensors** | Smart devices, wearables, industrial sensors produce continuous data streams |\n",
    "| **Real-time Analytics** | Businesses need instant insights for competitive advantage |\n",
    "| **ML/AI Requirements** | Machine learning models require massive datasets for training |\n",
    "| **Cheap Storage** | Cost per GB has dropped dramatically, making it economical to store everything |\n",
    "\n",
    "**Example: Daily Data Generation**    \n",
    "\n",
    "- 500 million tweets per day\n",
    "- 4 petabytes of data created on Facebook daily\n",
    "- 720,000 hours of video uploaded to YouTube daily\n",
    "- 6 billion Google searches per day\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Traditional Databases\n",
    "\n",
    "#### Why a Single Database Isn't Enough\n",
    "\n",
    "Traditional RDBMS (like MySQL, PostgreSQL) face fundamental limitations at scale:\n",
    "\n",
    "| Limitation | Explanation | Impact |\n",
    "|------------|-------------|--------|\n",
    "| **Cannot Scale to TB/PB** | Vertical scaling has physical limits | Cannot handle Big Data volumes |\n",
    "| **Limited CPU/Memory** | Single server bottleneck | Processing becomes the constraint |\n",
    "| **Single Point of Failure** | One server = one failure point | Downtime affects entire system |\n",
    "| **Slow for Real-time Analytics** | Designed for transactions, not analytics | Cannot meet velocity requirements |\n",
    "| **Tight Coupling** | Storage and compute bound together | Cannot scale independently |\n",
    "\n",
    "### Example: The Scaling Problem\n",
    "\n",
    "```text\n",
    "Scenario: E-commerce company with 100M users\n",
    "\n",
    "Traditional Approach:\n",
    "├── Single PostgreSQL server\n",
    "├── 500GB RAM (maximum practical)\n",
    "├── 10TB storage\n",
    "└── Result: Cannot handle Black Friday traffic spike\n",
    "\n",
    "Big Data Approach:\n",
    "├── Distributed cluster (100 nodes)\n",
    "├── 50TB total RAM\n",
    "├── 1PB distributed storage\n",
    "└── Result: Scales horizontally to meet demand\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4 Vs of Big Data\n",
    "\n",
    "The **four defining characteristics of Big Data** form the foundation for understanding its challenges:\n",
    "\n",
    "<img src=\"./pic/1_4Vs-of-Big-Data-Infographic.png\" width=500>\n",
    "\n",
    "### Volume\n",
    "**Definition:** The **sheer size of data** being generated and stored.\n",
    "\n",
    "| Scale | Size | Example |\n",
    "|-------|------|---------|\n",
    "| Gigabyte (GB) | 10⁹ bytes | A few hours of HD video |\n",
    "| Terabyte (TB) | 10¹² bytes | Large enterprise database |\n",
    "| Petabyte (PB) | 10¹⁵ bytes | Netflix's entire video library |\n",
    "| Exabyte (EB) | 10¹⁸ bytes | All words ever spoken by humans |\n",
    "\n",
    "**Example:** Walmart processes 2.5 petabytes of customer transaction data per hour.\n",
    "\n",
    "### Velocity\n",
    "**Definition:** The **speed** at which *data flows in, is processed, and analyzed*.\n",
    "\n",
    "| Type | Latency | Use Case |\n",
    "|------|---------|----------|\n",
    "| **Batch** | Hours/Days | Monthly reports, historical analysis |\n",
    "| **Near Real-time** | Minutes | Inventory updates, email campaigns |\n",
    "| **Real-time** | Milliseconds | Fraud detection, stock trading |\n",
    "\n",
    "**Example:** Credit card fraud detection must analyze transactions in <100ms to block fraudulent purchases.\n",
    "\n",
    "### Variety\n",
    "**Definition:** Different **types and formats of data** from multiple sources.\n",
    "\n",
    "| Data Type | Format | Examples |\n",
    "|-----------|--------|----------|\n",
    "| **Structured** | Tables, rows, columns | SQL databases, spreadsheets |\n",
    "| **Semi-structured** | Has organization but flexible | JSON, XML, CSV, logs |\n",
    "| **Unstructured** | No predefined format | Images, videos, emails, PDFs |\n",
    "\n",
    "**Example:** A hospital processes structured patient records, semi-structured lab results (JSON), and unstructured MRI images.\n",
    "\n",
    "### Veracity\n",
    "**Definition:** The **trustworthiness, accuracy, and quality** of data.\n",
    "\n",
    "| Challenge | Description | Solution |\n",
    "|-----------|-------------|----------|\n",
    "| **Inconsistency** | Same entity represented differently | Data normalization |\n",
    "| **Incompleteness** | Missing values | Imputation, validation rules |\n",
    "| **Ambiguity** | Unclear meaning | Data governance, metadata |\n",
    "| **Noise** | Erroneous data points | Outlier detection, cleaning |\n",
    "\n",
    "**Example:** Customer addresses may be entered in different formats: \"123 Main St\", \"123 Main Street\", \"123 Main St.\"\n",
    "\n",
    "\n",
    "### Value - the fifth V\n",
    "Value is widely considered the most critical of the 5 Vs because it represents the ultimate purpose of any big data system. \n",
    "\n",
    "While the other Vs (Volume, Velocity, Variety, Veracity) describe the **technical challenges** of the data, Value focuses on the **actionable insights and tangible benefits** derived from it.\n",
    "\n",
    "Value is often viewed as the result of successfully managing the other four characteristics:   \n",
    "$$\\text{Volume + Velocity + Variety + Veracity = Value}$$\n",
    "\n",
    "<img src=\"./pic/1_sources-of-big-data.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Big Data System\n",
    "An integrated **framework** of software and hardware designed to **collect, store, process, and analyze datasets** that are too massive or complex for traditional database systems. \n",
    "\n",
    "As of 2025, these systems are essential for handling the \"5 Vs\"—Volume (petabytes of data), Velocity (real-time speed), Variety (text, video, sensors), Veracity (accuracy), and Value.  \n",
    "\n",
    "Big Data -> The Asset  \n",
    "Big Data System -> The Framework\n",
    "\n",
    "### Core Architecture Layers\n",
    "Modern big data systems are typically organized into several functional layers:\n",
    "- **Ingestion**: The entry point where raw data is captured from sources like IoT sensors, social media, or financial transactions using tools like **Apache Kafka**.\n",
    "- **Storage**: **Distributed environments**, such as **Data Lakes** (for raw data) or **Lakehouses**, that store information across many servers using systems like HDFS or cloud object storage (Amazon S3, Google Cloud Storage).\n",
    "- **Processing**: Frameworks like [**Apache Spark**](https://spark.apache.org/) that transform raw data into usable formats through **parallel computing**, often handling both **real-time streams** and **historical batches**.\n",
    "- **Analysis & Visualization**: The final stage where data scientists use **machine learning models or BI tools** (like Tableau) to extract patterns and present actionable insights. \n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                        BIG DATA LIFECYCLE                               │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌─────────┐│\n",
    "│  │  INGEST  │──▶│  STORE   │──▶│ PROCESS  │──▶│ ANALYZE  │──▶│VISUALIZE││\n",
    "│  └──────────┘   └──────────┘   └──────────┘   └──────────┘   └─────────┘│\n",
    "│  \"Collect\"      \"Persist\"      \"Transform\"    \"Understand\"   \"Present\"  │\n",
    "│                                                                         │\n",
    "│  Kafka          S3/HDFS        Spark          SQL/Python     Tableau    │\n",
    "│  Kinesis        Snowflake      Flink          ML Models      Power BI   │\n",
    "│  Fivetran       PostgreSQL     dbt            Statistics     Looker     │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Characteristics in 2025\n",
    "- **Distributed Computing**: Instead of one large computer, big data systems use \"clusters\" of connected machines to **process data in parallel**.\n",
    "- **AI & Machine Learning Integration**: These systems now serve as the primary training ground for large language models (LLMs) and predictive AI.\n",
    "- **Scalability**: They are designed to **scale \"horizontally\"** by simply adding more commodity servers as data grows.\n",
    "- **Fault Tolerance**: To prevent data loss, they **automatically replicate** data across multiple nodes so the system continues to work even if a server fails.\n",
    "\n",
    "### Common Use Cases\n",
    "| Industry \t| Big Data System Application| \n",
    "| ----------| --------------------------| \n",
    "| Finance\t| Detecting fraudulent transactions in milliseconds.| \n",
    "| Healthcare\t| Predicting disease outbreaks and personalizing patient care.| \n",
    "| Retail\t| Optimizing inventory and delivering hyper-personalized recommendations.| \n",
    "| Manufacturing\t| Using sensor data for predictive maintenance to reduce equipment downtime.| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do Big Data Systems Solve\n",
    "Big data systems solve the problem of processing and extracting value from information that is too large, complex, or fast-moving for traditional database tools. By addressing the \"5 Vs\" (Volume, Velocity, Variety, Veracity, and Value), these systems transform raw, unorganized data into actionable intelligence.\n",
    "\n",
    "### Three Core Challenges\n",
    "\n",
    "Big Data systems must solve three fundamental problems:\n",
    "\n",
    "1. **Distributed Compute (Scaling Processing)**: split workloads across many machines    \n",
    "   \n",
    "    **The Problem**:  \n",
    "    A single processor cannot analyze petabytes of data within a useful timeframe.   \n",
    "    \n",
    "    **The Solution**: \n",
    "    - Systems use parallelism to split a large job into thousands of small tasks executed simultaneously across different machines. \n",
    "    - Frameworks like **Apache Spark** also prioritize data locality, moving code to the data rather than vice versa to minimize network traffic.\n",
    "2. **Distributed Storage (Scaling Volume)**: partition and replicate data     \n",
    "    \n",
    "    **The Problem**:   \n",
    "    Data grows faster than the disk capacity of any individual server.    \n",
    "    \n",
    "    **The Solution**: \n",
    "    - **Distributed File Systems (DFS)** like **HDFS** partition large files into smaller blocks (shards) and scatter them across multiple nodes. \n",
    "    - This allows storage to **scale horizontally** by simply adding more commodity hardware to the cluster.\n",
    "3. **Fault Tolerance (Maintaining Reliability)**: recover automatically when nodes fail  \n",
    "  \n",
    "   \n",
    "    **The Problem**:    \n",
    "    In a system with thousands of components, hardware failure is a daily occurrence, not a rarity.     \n",
    "    \n",
    "    **The Solution**: \n",
    "    - Systems use **replication** to store multiple copies of the same data across different nodes. \n",
    "    - If a node fails, the system automatically redirects tasks to a healthy replica and initiates automatic recovery to restore the lost copy elsewhere.\n",
    "\n",
    "### Key Design Questions\n",
    "\n",
    "| Question | Consideration | Common Approaches |\n",
    "|----------|---------------|-------------------|\n",
    "| **How to split data?** | Partitioning strategy.  Ensuring even distribution to avoid \"hotspots.\" | **Hash partitioning** (for balance) or **Range partitioning** (for sorted access). |\n",
    "| **How to schedule tasks?** | Resource allocation | YARN, Kubernetes(containerized orchestration), Mesos |\n",
    "| **How to handle failures?** | Fault tolerance. Balancing cost vs. recovery speed. | Replication (fatest), checkpointing vs. Erasure Coding(cheaper storage)|\n",
    "| **How to aggregate results?** | Combining partial results. Minimizing data \"shuffling\" over the network. | Reduce operations, shuffling. MapReduce workflows or Vectorized execution in modern engines. |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Technologies\n",
    "[Big data technology](https://www.datacamp.com/blog/big-data-technologies) refers to the tools and frameworks that process, store, and analyze complex and large datasets. \n",
    "\n",
    "To understand the Big Data Ecosystem, instead of thinking in isolated categories, think of big data as a **pipeline with layers**:\n",
    "\n",
    "<img src=\"./pic/1_data-pipeline.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Data Ingestion\n",
    "\n",
    "Collect data from **various sources** and move it into storage systems.\n",
    "\n",
    "There are two Modes of Ingestion:  \n",
    "\n",
    "<img src=\"./pic/1_ingestion_modes.png\" width=500>\n",
    "\n",
    "\n",
    "### Key Technologies\n",
    "\n",
    "| Technology | Type | Best For |\n",
    "|------------|------|----------|\n",
    "| **Apache Kafka** | Streaming | High-throughput event streaming |\n",
    "| **AWS Kinesis** | Streaming | AWS-native streaming |\n",
    "| **Debezium** | CDC (Change Data Capture) | Database change capture |\n",
    "| **Fivetran** | Batch | Managed ELT connectors |\n",
    "| **Airbyte** | Batch | Open-source ELT |\n",
    "| **Apache NiFi** | Both | Complex data routing |\n",
    "\n",
    "### Kafka Deep Dive\n",
    "\n",
    "<img src=\"./pic/1_kafka-architecture.png\" width=500>\n",
    "\n",
    "Key Concepts:                                                \n",
    "- Topics: Categories of messages                             \n",
    "- Partitions: Parallel lanes for scalability                 \n",
    "- Consumer Groups: Load balance consumption                  \n",
    "- Retention: How long messages are kept\n",
    "- Producers: could be applications, IoT Sensors\n",
    "- Consumers: could be Spark Streaming, Flink, S3 (Archive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Data Storage\n",
    "\n",
    "**Overview**:  \n",
    "\n",
    "<img src=\"./pic/1_big_data_storage_landscape.png\" width=600>  \n",
    "\n",
    "Traditional databases (PostgreSQL, MySQL, MongoDB) serve as operational sources. **Some NoSQL (Cassandra, HBase) can scale to big data levels**.\n",
    "\n",
    "**Storage Types Comparison**   \n",
    "\n",
    "| Type | Examples | Data Format | Best For | Latency | Cost |\n",
    "|------|----------|-------------|----------|---------|------|\n",
    "| **OLTP Database** | PostgreSQL, MySQL | Structured | Transactions | ms | $$$ |\n",
    "| **NoSQL** | MongoDB, Cassandra | Flexible | Scale, Speed | ms | $$ |\n",
    "| **Data Lake** | S3, HDFS | Any | Raw storage, ML | sec | $ |\n",
    "| **Data Warehouse** | Snowflake, BigQuery | Structured | Analytics | sec | $$$ |\n",
    "| **Lakehouse** | Delta Lake, Iceberg | Any + ACID | Modern analytics | sec | $$ |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lakes\n",
    "   \n",
    "A Data Lake is a centralized repository that stores **all** your data in its **raw, native format** at any scale.  \n",
    "\n",
    "**PURPOSE**: Store everything cheaply, process later.\n",
    "\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        DATA LAKE ZONES                           │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   S3 Bucket: s3://company-data-lake/                            │\n",
    "│   │                                                              │\n",
    "│   ├── raw/                    ◄── BRONZE: Landing zone (as-is)  │\n",
    "│   │   ├── sales/                                                │\n",
    "│   │   │   └── 2024/01/01/transactions.json                     │\n",
    "│   │   ├── logs/                                                 │\n",
    "│   │   │   └── 2024/01/01/app.log.gz                            │\n",
    "│   │   ├── images/                                               │\n",
    "│   │   │   └── product_photos/                                   │\n",
    "│   │   └── social/                                               │\n",
    "│   │       └── twitter_feed.json                                 │\n",
    "│   │                                                              │\n",
    "│   ├── processed/              ◄── SILVER: Cleaned & validated   │\n",
    "│   │   ├── sales/                                                │\n",
    "│   │   │   └── daily_transactions.parquet                       │\n",
    "│   │   └── customers/                                            │\n",
    "│   │       └── profiles_deduplicated.parquet                    │\n",
    "│   │                                                              │\n",
    "│   └── curated/                ◄── GOLD: Business-ready          │\n",
    "│       ├── marketing/                                            │\n",
    "│       │   └── campaign_metrics.parquet                         │\n",
    "│       ├── finance/                                              │\n",
    "│       │   └── revenue_by_region.parquet                        │\n",
    "│       └── ml_features/                                          │\n",
    "│           └── customer_features.parquet                        │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Data Lake Characteristics:**\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Raw Data Storage** | Data stored as-is, without transformation |\n",
    "| **Any Format** | JSON, CSV, Parquet, ORC, Avro, images, video |\n",
    "| **Schema-on-Read** | Define structure when querying, not when writing |\n",
    "| **Cheap Storage** | ~$0.02/GB/month for standard, less for infrequent |\n",
    "| **Unlimited Scale** | Exabytes of data possible |\n",
    "| **Decoupled Compute** | Any engine can read (Spark, Trino, Athena) |\n",
    "| **Foundation for ML** | Data scientists access raw data for exploration |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organization \n",
    "\n",
    "The architecture used above is called **Medallion Architecture (Bronze/Silver/Gold)**, it is a popular design pattern. But it is not the only way to organize a data lake. While highly recommended for its clear separation of data quality stages, many organizations in 2025 use hybrid, alternative or modified frameworks depending on their specific needs:  \n",
    "\n",
    "| Architecture \t| Best For\t| Focus| \n",
    "| ---------------| --------| -----| \n",
    "| Medallion (Bronze → Silver → Gold zones)| \tGeneral-purpose lakehouse\t| Progressive data refinement & quality| \n",
    "| Data Vault\t| Complex, highly regulated audits| \tLong-term history and auditability| \n",
    "| Data Mesh\t| Large, global organizations| \tDomain autonomy and scalability| \n",
    "| Lambda\t| High-velocity streaming + batch| \tBalancing real-time and historical accuracy| \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Formats in Data Lakes\n",
    "\n",
    "| Format | Type | Compression | Splittable | Best For |\n",
    "|--------|------|-------------|------------|----------|\n",
    "| **CSV** | Row | Poor | Yes | Simple interchange, small data |\n",
    "| **Avro** | Row | Good | Yes | Streaming, schema evolution |\n",
    "| **JSON** | Semi-structured | Moderate | Yes (line-delimited) | APIs, nested data |\n",
    "| **Parquet** | Columnar | Excellent | Yes | Analytics, Spark |\n",
    "| **ORC** | Columnar | Excellent | Yes | Hive workloads |\n",
    "\n",
    "\n",
    "**Parquet**:   \n",
    "A **columnar** storage **file format** designed for efficient analytics. Think of it as a better CSV for analytical workloads.\n",
    "\n",
    "<img src=\"./pic/1_parquet_file_format.webp\" width=500>\n",
    "\n",
    "*Key characteristics*\n",
    "\n",
    "- Columnar: Data stored by column, not row—great for queries that only need specific columns\n",
    "- Compressed: Built-in compression (snappy, gzip, zstd)—files are much smaller than CSV/JSON\n",
    "- Schema-embedded: The file carries its own schema (types, column names)\n",
    "- Splittable: Can be read in parallel chunks—important for distributed processing\n",
    "\n",
    "*Why Parquet Dominates*    \n",
    "- more columnar ideas in storage formats  \n",
    "\n",
    "*Benefits*                                                     \n",
    "- Read only columns needed (projection pushdown)             \n",
    "- Skip row groups using statistics (predicate pushdown)      \n",
    "- Excellent compression (same data types together)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Lake Challenges\n",
    "| Challenge | Description | Solution |\n",
    "|-----------|-------------|----------|\n",
    "| **No ACID** | Concurrent writes can corrupt | Use Lakehouse (Delta/Iceberg) |\n",
    "| **Data Swamp** | Ungoverned, unusable data | Data catalog, governance |\n",
    "| **No Schema** | Hard to understand data | Metadata management |\n",
    "| **Performance** | Small files problem | Compaction, partitioning |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foundation Storage \n",
    "where files physically live   \n",
    "\n",
    "- 2010s: HDFS was THE foundation                                      \n",
    "  Data Lake = HDFS + Hive + MapReduce/Spark                            \n",
    "                                                                \n",
    "- 2020s: Cloud Object Storage is THE foundation                                            \n",
    "  Data Lake = S3/GCS + Spark/Trino + Delta Lake/Iceberg                \n",
    "                                                                \n",
    "HDFS still exists but is legacy/declining                           \n",
    "Most new projects use S3/GCS      \n",
    "\n",
    "| Aspect| HDFS| S3/GCS/Azure Blob| \n",
    "| ------| ----| ------------------| \n",
    "| Era| 2006+ (Hadoop era)| 2010s+ (Cloud era)| \n",
    "| Deployment| On-premise (deploy locally), self-managed| Cloud, fully managed| \n",
    "| Scaling| Add nodes manually| Unlimited, automatic| \n",
    "| Cost| Hardware + ops team| Pay per GB stored| \n",
    "| Ecosystem| Hadoop (MapReduce, Hive)| Everything (Spark, Snowflake, etc.)| \n",
    "| Current Trend| Declining| Dominant| \n",
    "\n",
    "##### HDFS (on-premise, legacy)\n",
    "\n",
    "**What it is:** Hadoop Distributed File System, the primary storage system for Hadoop ecosystem, designed to store very large files across multiple machines.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "<img src=\"./pic/1_HDFS-Architecture.webp\" width=500>\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Block Storage** | Files split into 128MB blocks (configurable) |\n",
    "| **Replication** | Each block replicated **3x by default** |\n",
    "| **Rack Awareness** | Places replicas on different racks for fault tolerance |\n",
    "| **Write Once** | Optimized for **append-only**, **sequential reads** |\n",
    "| **Data Locality** | Moves compute to data, not data to compute |\n",
    "\n",
    "**Use Cases:**\n",
    "- Log file storage and analysis\n",
    "- Data lake foundation for Hadoop ecosystem\n",
    "- Batch processing with MapReduce/Spark\n",
    "- Long-term archival storage\n",
    "\n",
    "**Example Commands:**\n",
    "```bash\n",
    "# List files in HDFS\n",
    "hdfs dfs -ls /user/data/\n",
    "\n",
    "# Copy local file to HDFS\n",
    "hdfs dfs -put localfile.csv /user/data/\n",
    "\n",
    "# Copy from HDFS to local\n",
    "hdfs dfs -get /user/data/file.csv ./local/\n",
    "\n",
    "# Check replication factor\n",
    "hdfs dfs -stat %r /user/data/myfile.csv\n",
    "\n",
    "# Set replication factor\n",
    "hdfs dfs -setrep 5 /user/data/important_file.csv\n",
    "\n",
    "# Check disk usage\n",
    "hdfs dfs -du -h /user/data/\n",
    "\n",
    "# Delete file\n",
    "hdfs dfs -rm /user/data/oldfile.csv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cloud Object Storage (modern standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon S3 (Simple Storage Service)**\n",
    "\n",
    "**What it is:** Highly scalable, durable **object storage** service that serves as the backbone for cloud data lakes.\n",
    "\n",
    "**Structure:**\n",
    "```text\n",
    "S3 Organization:\n",
    "\n",
    " AWS Account                                      \n",
    " └── Bucket: my-company-data-lake                 \n",
    "     ├── raw/                                     \n",
    "     │   ├── sales/2024/01/transactions.parquet  \n",
    "     │   ├── logs/2024/01/01/app.log.gz           \n",
    "     │   └── images/product_001.jpg              \n",
    "     ├── processed/                               \n",
    "     │   └── sales/aggregated_daily.parquet      \n",
    "     └── analytics/                               \n",
    "         └── reports/monthly_summary.csv         \n",
    "\n",
    "```\n",
    "\n",
    "**Storage Classes:**\n",
    "| Class | Use Case | Retrieval | Cost | Min Duration |\n",
    "|-------|----------|-----------|------|--------------|\n",
    "| **S3 Standard** | Frequently accessed | Instant | $$$ | None |\n",
    "| **S3 Intelligent-Tiering** | Unknown patterns | Instant | $$ | 30 days |\n",
    "| **S3 Standard-IA** | Infrequent access | Instant | $$ | 30 days |\n",
    "| **S3 One Zone-IA** | Infrequent, non-critical | Instant | $ | 30 days |\n",
    "| **S3 Glacier Instant** | Archive, instant access | Instant | $ | 90 days |\n",
    "| **S3 Glacier Flexible** | Archive | Minutes-12hrs | ¢ | 90 days |\n",
    "| **S3 Glacier Deep Archive** | Long-term archive | 12-48hrs | ¢ | 180 days |\n",
    "\n",
    "**Key Features:**\n",
    "- 99.999999999% (11 9s) durability\n",
    "- 99.99% availability\n",
    "- Unlimited storage capacity\n",
    "- Built-in versioning and lifecycle policies\n",
    "- Event notifications (trigger Lambda on upload)\n",
    "- Cross-region replication\n",
    "- Server-side encryption\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Storage (GCS)**\n",
    "\n",
    "**Storage Classes:**\n",
    "| Class | Use Case | Retrieval |\n",
    "|-------|----------|-----------|\n",
    "| **Standard** | Frequently accessed | Instant |\n",
    "| **Nearline** | Once per month | Instant |\n",
    "| **Coldline** | Once per quarter | Instant |\n",
    "| **Archive** | Once per year | Instant |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure Blob Storage**\n",
    "\n",
    "**Access Tiers:**\n",
    "| Tier | Use Case | Retrieval |\n",
    "|------|----------|-----------|\n",
    "| **Hot** | Frequently accessed | Instant |\n",
    "| **Cool** | Infrequent (30+ days) | Instant |\n",
    "| **Cold** | Rarely accessed (90+ days) | Instant |\n",
    "| **Archive** | Long-term (180+ days) | Hours |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cloud Storage Comparison**\n",
    "\n",
    "| Feature | S3 (AWS) | GCS (Google) | Azure Blob |\n",
    "|---------|----------|--------------|------------|\n",
    "| **Durability** | 11 9s | 11 9s | 11 9s |\n",
    "| **Availability** | 99.99% | 99.99% | 99.99% |\n",
    "| **Min Object Size** | 0 bytes | 0 bytes | 0 bytes |\n",
    "| **Max Object Size** | 5TB | 5TB | 4.75TB |\n",
    "| **Versioning** | Yes | Yes | Yes |\n",
    "| **Lifecycle Policies** | Yes | Yes | Yes |\n",
    "| **Best Integration** | AWS services | BigQuery, Dataflow | Azure services |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warehouses \n",
    "A Data Warehouse is a centralized repository of **integrated, cleaned, structured data** optimized for high-performance reporting and business intelligence. DW serve as the \"refined\" analytical layer of a big data system.\n",
    "\n",
    "**PURPOSE**: Fast analytics on clean, structured data  \n",
    "\n",
    "**Characteristics**:                                       \n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Subject-Oriented** | Organized by business subjects (sales, customers) |\n",
    "| **Integrated** | Unified from multiple source systems |\n",
    "| **Time-Variant** | Maintains historical data with timestamps |\n",
    "| **Non-Volatile** | Data is stable, not frequently updated |\n",
    "| **Columnar Storage** | Optimized for analytical read patterns |\n",
    "| **Schema-on-Write**\t| Requires a strictly defined structure (schema) before data is loaded, ensuring high data quality and predictability.| \n",
    "| **Pre-computed Aggregations** | Materialized views for common queries |\n",
    "| **MPP Architecture**\t| Uses Massive Parallel Processing to distribute queries across multiple nodes for sub-second responses on millions of rows.| \n",
    "| **Decoupled Design**\t| Modern 2025 DWs (like Snowflake/BigQuery) separate storage from compute, allowing you to pay for each independently.|             \n",
    "\n",
    "**Pros**:\n",
    "- ✅ Speed: Optimized for complex SQL joins and pre-computed aggregations (materialized views).\n",
    "- ✅ Accessibility: Easy for non-technical business users to query via standard SQL or BI tools.\n",
    "- ✅ Governance: High levels of security, access control, and data lineage.\n",
    "\n",
    "**Cons**:\n",
    "- ❌ Cost: Generally more expensive than raw Data Lake storage (S3/GCS).\n",
    "- ❌ Inflexibility: Struggles with unstructured data (videos, raw logs) and requires rigorous ETL (Extract, Transform, Load) pipelines to ingest data.\n",
    "\n",
    "#### Data Warehouse vs Data Lake\n",
    "\n",
    "| Aspect | Data Warehouse | Data Lake |\n",
    "|--------|----------------|-----------|\n",
    "| **Data Type** | Structured only | All types |\n",
    "| **Schema** | Schema-on-write | Schema-on-read |\n",
    "| **Users** | Business analysts | Data scientists |\n",
    "| **Data Quality** | High (cleaned) | Variable (raw) |\n",
    "| **Cost** | Higher | Lower |\n",
    "| **Query Performance** | Fast (optimized) | Variable |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowflake \n",
    "[Snowflake](https://www.linkedin.com/pulse/snowflake-architecture-overview-minzhen-yang/) is architected with **three independent layers**: \n",
    "\n",
    "**Cloud Service Layer**\n",
    "- Authentication & Access Control               \n",
    "- Infrastructure Management                     \n",
    "- Metadata Management                           \n",
    "- Query Parsing & Optimization\n",
    "\n",
    "**Compute Layer**  \n",
    "- Scale up/down instantly                      \n",
    "- Auto-suspend when idle                       \n",
    "- Multi-cluster for concurrency\n",
    "\n",
    "**Data Storage Layer**\n",
    "- Compressed **columnar** format                     \n",
    "- Stored on **cloud object storage (S3/Azure/GCS)**  \n",
    "- Automatic clustering and optimization          \n",
    "- Pay only for storage used          \n",
    "\n",
    "<img src=\"./pic/1_snowflake_layers.png\" width=500>\n",
    "\n",
    "**Snowflake Key Features:**\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Separation of Storage/Compute** | Scale independently, pay for what you use |\n",
    "| **Zero-Copy Cloning** | Instant database copies without storage cost |\n",
    "| **Time Travel** | Query historical data (up to 90 days) |\n",
    "| **Data Sharing** | Share live data across accounts securely |\n",
    "| **Multi-Cloud** | Run on AWS, Azure, or GCP |\n",
    "| **Auto-Scaling** | Automatically scale compute up/down |\n",
    "| **Snowpipe** | Continuous data loading |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Redshift \n",
    "\n",
    "<img src=\"./pic/1_redshift_clusters.jpg\" width=400>\n",
    "\n",
    "**Key Features**:                                        \n",
    "- Massively Parallel Processing (MPP)                \n",
    "- Columnar storage with compression                  \n",
    "- Redshift Spectrum: Query S3 directly               \n",
    "- Concurrency Scaling: Handle traffic spikes         \n",
    "- Redshift ML: In-database machine learning          \n",
    "- Redshift Serverless: No cluster management  \n",
    "\n",
    "**Leader Node**: \n",
    "- Query planning & aggregation\n",
    "- Coordinates compute nodes       \n",
    "\n",
    "**Compute Nodes**:\n",
    "- Parallel processing\n",
    "\n",
    "\n",
    "<img src=\"./pic/1_redshift.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google BigQuery\n",
    "\n",
    "**What it is:** Serverless, highly scalable enterprise data warehouse.\n",
    "\n",
    "<img src=\"./pic/1_bigquery-architecture-diagram.svg\" width=500>\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Serverless** | No infrastructure to manage |\n",
    "| **Pay-per-Query** | $5 per TB scanned (on-demand) |\n",
    "| **Slots** | Reserved compute for predictable pricing |\n",
    "| **BigQuery ML** | Train ML models with SQL |\n",
    "| **BI Engine** | In-memory analysis for sub-second queries |\n",
    "| **Streaming Insert** | Real-time data ingestion |\n",
    "| **Geospatial** | Native GIS support |\n",
    "\n",
    "<img src=\"./pic/1_GoogleBigQuery_Explained.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Warehouse Comparison\n",
    "\n",
    "| Warehouse | Cloud | Key Differentiator | Best For |\n",
    "|-----------|-------|-------------------|----------|\n",
    "| **Snowflake** | Multi-cloud | Data sharing, separation | Multi-cloud, collaboration |\n",
    "| **BigQuery** | GCP | Serverless, BigQuery ML | No-ops, ML in SQL |\n",
    "| **Redshift** | AWS | Deep AWS integration | AWS-centric teams |\n",
    "| **Azure Synapse** | Azure | Unified analytics | Microsoft ecosystem |\n",
    "| **Databricks SQL** | Multi-cloud | Lakehouse-native | Existing Databricks users |\n",
    "| **ClickHouse** | Any | Fastest for real-time | Real-time OLAP |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Modeling\n",
    "\n",
    "Data modeling in warehouses typically uses dimensional modeling with Fact and Dimension tables.\n",
    "\n",
    "| Table Type | Purpose | Example |\n",
    "|------------|---------|---------|\n",
    "| **Fact Table** | Stores measurable, quantitative data | Sales transactions, clicks, orders |\n",
    "| **Dimension Table** | Stores descriptive attributes | Products, customers, dates, locations |\n",
    "\n",
    "\n",
    "**Dimensional modeling** ends up with a dimensional model as a:   \n",
    "\n",
    "- **Star schema** with one fact surrounded by conformed and shared dimensions.\n",
    "- **Constellation schema** with few facts sharing conformed dimensions.\n",
    "- **Snowflake schema** with one or few dimensions have become snowflaking.   \n",
    "\n",
    "> Snowflake schema is not Snowflake product/service/tech talked before or after\n",
    "\n",
    "**Design Methodology**:    \n",
    "- [Inmon VS the Kimball](https://www.astera.com/type/blog/data-warehouse-concepts/) \n",
    "- [Dimensional modeling – architecture and methodology (including data warehouse definition)](./reading/Dimensional%20modeling_Data_Warehouse.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slowly Changing Dimensions (SCD)\n",
    "\n",
    "Slowly Changing Dimensions (SCD) are techniques for **tracking changes** in dimension data over time in **data warehouses**.\n",
    "\n",
    "**Why SCD Matters**\n",
    "\n",
    "  Dimension attributes change over time (e.g., customer address, employee department), and we need strategies to handle these changes while maintaining historical accuracy.\n",
    "\n",
    "**1. SCD Type 0 — No Change (Retain Original)**\n",
    "\n",
    "  **Strategy**: Original value is NEVER changed\n",
    "\n",
    "  **Characteristics**:\n",
    "  - No updates allowed\n",
    "  - No history tracking\n",
    "  - Data treated as static/immutable\n",
    "\n",
    "  **Use Cases**:\n",
    "  - Original registration date\n",
    "  - Birth date\n",
    "  - Social Security Number\n",
    "  - Original customer ID\n",
    "\n",
    "  **Example**:\n",
    "  ```sql\n",
    "  -- Customer table with Type 0 attribute\n",
    "  CREATE TABLE customers (\n",
    "      customer_id INT PRIMARY KEY,\n",
    "      original_signup_date DATE,  -- Type 0: Never changes\n",
    "      current_email VARCHAR(100)  -- Other attributes may change\n",
    "  );\n",
    "\n",
    "  -- Even if customer re-registers, original_signup_date stays the same\n",
    "  ```\n",
    "\n",
    "  ```text\n",
    "  Initial State:\n",
    "  customer_id | original_signup_date | current_email\n",
    "  1           | 2020-01-15          | john@old.com\n",
    "\n",
    "  After \"Update\":\n",
    "  customer_id | original_signup_date | current_email\n",
    "  1           | 2020-01-15          | john@new.com  ← email changed\n",
    "              ↑ signup date UNCHANGED\n",
    "  ```\n",
    "\n",
    "**2. SCD Type 1 — Overwrite (No History)**\n",
    "\n",
    "  **Strategy**: Update attribute in place, replacing old value\n",
    "\n",
    "  **Characteristics**:\n",
    "  - Old value is permanently lost\n",
    "  - Table always reflects current state only\n",
    "  - Simplest to implement\n",
    "  - No storage overhead\n",
    "\n",
    "  **Use Cases**:\n",
    "  - Correcting data entry errors\n",
    "  - Attributes where history doesn't matter\n",
    "  - Non-critical descriptive attributes\n",
    "\n",
    "  **Example**:\n",
    "  ```sql\n",
    "  -- Before update\n",
    "  SELECT * FROM customers WHERE customer_id = 1;\n",
    "  -- customer_id | name  | city\n",
    "  -- 1           | John  | Boston\n",
    "\n",
    "  -- Update: Customer moved to New York\n",
    "  UPDATE customers SET city = 'New York' WHERE customer_id = 1;\n",
    "\n",
    "  -- After update (history lost!)\n",
    "  SELECT * FROM customers WHERE customer_id = 1;\n",
    "  -- customer_id | name  | city\n",
    "  -- 1           | John  | New York\n",
    "  ```\n",
    "\n",
    "  ```text\n",
    "  Timeline with Type 1:\n",
    "  Jan 2023: city = 'Boston'\n",
    "  Jun 2023: city = 'New York' (Boston is GONE)\n",
    "  Dec 2023: Query \"Where did customer live in Feb 2023?\" → Cannot answer!\n",
    "  ```\n",
    "\n",
    "**3. SCD Type 2 — Full History Tracking**\n",
    "\n",
    "  **Strategy**: Insert new row for each change, maintaining complete history\n",
    "\n",
    "  **Characteristics**:\n",
    "  - Old records are NOT overwritten\n",
    "  - New row inserted when data changes\n",
    "  - Multiple records per business key\n",
    "  - Tracks complete history with validity periods\n",
    "\n",
    "  **Required Columns**:\n",
    "  - Surrogate Key (unique per row)\n",
    "  - Business Key (identifies the entity)\n",
    "  - Effective Date / Start Date\n",
    "  - Expiration Date / End Date\n",
    "  - Current Flag (optional, for convenience)\n",
    "\n",
    "  **Example**:\n",
    "  ```sql\n",
    "  -- Type 2 Dimension Table Structure\n",
    "  CREATE TABLE dim_customer (\n",
    "      surrogate_key INT PRIMARY KEY AUTO_INCREMENT,\n",
    "      customer_id INT,              -- Business key\n",
    "      name VARCHAR(100),\n",
    "      city VARCHAR(100),\n",
    "      effective_date DATE,\n",
    "      expiration_date DATE,\n",
    "      is_current BOOLEAN\n",
    "  );\n",
    "  ```\n",
    "\n",
    "  ```sql\n",
    "  -- Initial record (Jan 2023)\n",
    "  INSERT INTO dim_customer VALUES \n",
    "  (1, 101, 'John', 'Boston', '2023-01-01', '9999-12-31', TRUE);\n",
    "\n",
    "  -- Customer moves to New York (Jun 2023)\n",
    "  -- Step 1: Close current record\n",
    "  UPDATE dim_customer \n",
    "  SET expiration_date = '2023-06-14', is_current = FALSE\n",
    "  WHERE customer_id = 101 AND is_current = TRUE;\n",
    "\n",
    "  -- Step 2: Insert new record\n",
    "  INSERT INTO dim_customer VALUES \n",
    "  (2, 101, 'John', 'New York', '2023-06-15', '9999-12-31', TRUE);\n",
    "  ```\n",
    "\n",
    "  **Resulting Table State**:\n",
    "\n",
    "  | surrogate_key | customer_id | name | city     | effective_date | expiration_date | is_current |\n",
    "  |--------------|------------|------|----------|----------------|-----------------|------------|\n",
    "  | 1            | 101        | John | Boston   | 2023-01-01     | 2023-06-14      | FALSE      |\n",
    "  | 2            | 101        | John | New York | 2023-06-15     | 9999-12-31      | TRUE       |\n",
    "\n",
    "  **Querying Type 2 Dimensions**:\n",
    "  ```sql\n",
    "  -- Get current state\n",
    "  SELECT * FROM dim_customer WHERE is_current = TRUE;\n",
    "\n",
    "  -- Get state as of specific date (point-in-time query)\n",
    "  SELECT * FROM dim_customer \n",
    "  WHERE customer_id = 101 \n",
    "    AND '2023-03-15' BETWEEN effective_date AND expiration_date;\n",
    "  -- Returns: Boston (historical state)\n",
    "\n",
    "  -- Get complete history\n",
    "  SELECT * FROM dim_customer WHERE customer_id = 101 ORDER BY effective_date;\n",
    "  ```\n",
    "\n",
    "**SCD Type Comparison**\n",
    "\n",
    "  | Aspect | Type 0 | Type 1 | Type 2 |\n",
    "  |--------|--------|--------|--------|\n",
    "  | History | None (immutable) | None (overwritten) | Full |\n",
    "  | Storage | Minimal | Minimal | Higher |\n",
    "  | Complexity | Lowest | Low | High |\n",
    "  | Query Performance | Fast | Fast | Slower |\n",
    "  | Point-in-Time Analysis | N/A | Not possible | Fully supported |\n",
    "  | Use Case | Static attributes | Current state only | Audit/History required |\n",
    "\n",
    "**Additional SCD Types (Brief Overview)**  \n",
    "\n",
    "  | Type | Strategy | Description |\n",
    "  |------|----------|-------------|\n",
    "  | Type 3 | Limited History | Add columns for previous value (e.g., `previous_city`) |\n",
    "  | Type 4 | History Table | Separate current and history tables |\n",
    "  | Type 6 | Hybrid | Combines Types 1, 2, and 3 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mart\n",
    "\n",
    "A Data Mart is a **subset of a Data Warehouse** focused on a specific business department, subject area, or team.\n",
    "\n",
    "\n",
    "```text\n",
    "┌──────────────────────────────────────────────────────────┐\n",
    "│                ENTERPRISE DATA WAREHOUSE                 │\n",
    "│  ┌─────────────────────────────────────────────────────┐ │\n",
    "│  │               All Company Data                      │ │\n",
    "│  │┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐│ │\n",
    "│  ││  Sales  │   │ Finance │   │Marketing│   │   HR    ││ │\n",
    "│  ││  Mart   │   │  Mart   │   │  Mart   │   │  Mart   ││ │\n",
    "│  │└─────────┘   └─────────┘   └─────────┘   └─────────┘│ │\n",
    "│  │     ↓             ↓             ↓             ↓     │ │\n",
    "│  │Sales Team   Finance Team    Marketing     HR Team   │ │\n",
    "│  │                                Team                 │ │\n",
    "│  └─────────────────────────────────────────────────────┘ │\n",
    "└──────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Example: Sales Data Mart**\n",
    "\n",
    "```sql\n",
    "-- Sales Mart focuses only on sales-related data\n",
    "Sales Data Mart:\n",
    "├── fact_sales (transactions)\n",
    "├── dim_product\n",
    "├── dim_customer\n",
    "├── dim_store\n",
    "├── dim_date\n",
    "└── dim_salesperson\n",
    "\n",
    "-- Excludes HR, Finance, Marketing data\n",
    "-- Pre-aggregated for common sales queries:\n",
    "   - Daily sales by store\n",
    "   - Monthly revenue by product category\n",
    "   - Salesperson performance metrics\n",
    "```\n",
    "\n",
    "**Benefits of Data Marts**\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Performance** | Smaller dataset = faster queries |\n",
    "| **Simplicity** | Reduced schema complexity for end users |\n",
    "| **Security** | Department-level access control |\n",
    "| **Autonomy** | Teams manage their own data |\n",
    "| **Cost** | Lower compute requirements per mart |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lakehouse (Modern Approach)\n",
    "\n",
    "PURPOSE: Combine lake flexibility with warehouse reliability\n",
    "\n",
    "\n",
    "| Traditional Data Lake Problems | Lakehouse Solutions            |\n",
    "|--------------------------------|-------------------------------  |\n",
    "| No ACID transactions           | Full ACID support               |\n",
    "| No schema enforcement          | Schema evolution & enforcement  |\n",
    "| Data corruption on failure     | Atomic commits                  |\n",
    "| No versioning                  | Time travel (versioned data)    |\n",
    "| Slow metadata operations       | Optimized metadata handling     |\n",
    "| No upserts/deletes             | Full DML support                |\n",
    "\n",
    "```text\n",
    "┌───────────────────────────────────────────────────┐\n",
    "│                   LAKEHOUSE                       │\n",
    "├───────────────────────────────────────────────────┤\n",
    "│ ┌───────────────────────────────────────────────┐ │\n",
    "│ │             TABLE FORMAT LAYER                │ │\n",
    "│ │     (Delta Lake / Apache Iceberg / Hudi)      │ │\n",
    "│ └───────────────────────────────────────────────┘ │\n",
    "│                        │                          │\n",
    "│                        ▼                          │\n",
    "│ ┌───────────────────────────────────────────────┐ │\n",
    "│ │               OBJECT STORAGE                  │ │\n",
    "│ │              (S3 / GCS / ADLS)                │ │\n",
    "│ └───────────────────────────────────────────────┘ │\n",
    "└───────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Lake\n",
    "\n",
    "**What it is:** Open-source **storage layer** that brings **ACID** transactions to data lakes. Created by Databricks.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "<img src=\"./pic/1_delta-lake.png\" width=600>\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    DELTA LAKE ON DISK                           │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   s3://bucket/orders/                                           │\n",
    "│   ├── _delta_log/                    ◄── Transaction log        │\n",
    "│   │   ├── 00000000000000000000.json  ◄── Version 0 (create)     │\n",
    "│   │   ├── 00000000000000000001.json  ◄── Version 1 (insert)     │\n",
    "│   │   ├── 00000000000000000002.json  ◄── Version 2 (update)     │\n",
    "│   │   └── 00000000000000000003.json  ◄── Version 3 (delete)     │\n",
    "│   ├── part-00000-xxx.parquet         ◄── Data files             │\n",
    "│   ├── part-00001-xxx.parquet                                    │\n",
    "│   └── part-00002-xxx.parquet                                    │\n",
    "│                                                                 │\n",
    "│   Transaction Log Entry Example:                                │\n",
    "│   {                                                             │\n",
    "│     \"add\": {                                                    │\n",
    "│       \"path\": \"part-00003-xxx.parquet\",                         │\n",
    "│       \"size\": 1024000,                                          │\n",
    "│       \"modificationTime\": 1704067200000,                        │\n",
    "│       \"dataChange\": true,                                       │\n",
    "│       \"stats\": \"{\\\"numRecords\\\":10000,\\\"minValues\\\":{...}}\"     │\n",
    "│     }                                                           │\n",
    "│   }                                                             │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Delta Lake Code Examples:**\n",
    "```python\n",
    "# Write Delta table\n",
    "df.write.format(\"delta\").save(\"/data/events\")\n",
    "\n",
    "# Write with partitioning\n",
    "df.write.format(\"delta\") \\\n",
    "    .partitionBy(\"date\", \"country\") \\\n",
    "    .save(\"/data/events\")\n",
    "\n",
    "# Read Delta table\n",
    "df = spark.read.format(\"delta\").load(\"/data/events\")\n",
    "\n",
    "# Time Travel - Read specific version\n",
    "df_v5 = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 5) \\\n",
    "    .load(\"/data/events\")\n",
    "\n",
    "# Time Travel - Read as of timestamp\n",
    "df_historical = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-01\") \\\n",
    "    .load(\"/data/events\")\n",
    "\n",
    "# MERGE (Upsert) operation\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/data/events\")\n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    updates.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"value\": \"source.value\",\n",
    "    \"updated_at\": \"source.updated_at\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"id\": \"source.id\",\n",
    "    \"value\": \"source.value\",\n",
    "    \"created_at\": \"source.created_at\"\n",
    "}).execute()\n",
    "\n",
    "# Delete operation\n",
    "deltaTable.delete(\"date < '2023-01-01'\")\n",
    "\n",
    "# Update operation\n",
    "deltaTable.update(\n",
    "    condition=\"country = 'USA'\",\n",
    "    set={\"region\": \"'North America'\"}\n",
    ")\n",
    "\n",
    "# Optimize (compaction)\n",
    "deltaTable.optimize().executeCompaction()\n",
    "\n",
    "# Z-Order (co-locate related data)\n",
    "deltaTable.optimize().executeZOrderBy(\"customer_id\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Iceberg\n",
    "\n",
    "**What it is:** Open **table format** for **huge analytic datasets**. Created by Netflix.\n",
    "\n",
    "**Key Features:**\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Hidden Partitioning** | No partition columns in queries |\n",
    "| **Schema Evolution** | Add, drop, rename columns without rewrite |\n",
    "| **Partition Evolution** | Change partitioning without rewrite |\n",
    "| **Time Travel** | Query historical snapshots |\n",
    "| **Multi-Engine** | Works with Spark, Trino, Flink, Hive |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Hudi\n",
    "\n",
    "**What it is:** Data lake **storage layer** with incremental processing. Created by Uber.\n",
    "\n",
    "**Key Features:**\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Incremental Processing** | Process only changed data |\n",
    "| **Upserts/Deletes** | Record-level mutations |\n",
    "| **Two Table Types** | Copy-on-Write, Merge-on-Read |\n",
    "| **Streaming Ingestion** | Built for real-time data |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lakehouse Technologies Comparison\n",
    "\n",
    "| Feature | Delta Lake | Apache Iceberg | Apache Hudi |\n",
    "|---------|------------|----------------|-------------|\n",
    "| **Created By** | Databricks | Netflix | Uber |\n",
    "| **ACID** | Yes | Yes | Yes |\n",
    "| **Time Travel** | Yes | Yes | Yes |\n",
    "| **Schema Evolution** | Yes | Yes (best) | Yes |\n",
    "| **Partition Evolution** | Limited | Yes (best) | Yes |\n",
    "| **Streaming** | Good | Good | Best |\n",
    "| **Best Engine** | Spark/Databricks | Trino, multi-engine | Spark, Flink |\n",
    "| **Cloud Support** | All | All | All |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star/ Snowflake Schema\n",
    "\n",
    "Star/Snowflake Schema is for STRUCTURED ANALYTICAL DATA. It requires: Tables, Relationships, Schema-on-write.   \n",
    "\n",
    "Do they need to consider it:  \n",
    "- **DATA LAKE: ✗ NO** - Not typically                                                                    \n",
    "  - Raw data, any format                                        \n",
    "  - Schema-on-read                                              \n",
    "  - No enforced structure                                       \n",
    "  - Just files: JSON, CSV, Parquet, images...                   \n",
    "                                                                \n",
    "    (But if you add structure → it becomes a Lakehouse)\n",
    "- **DATA WAREHOUSE: ✓ YES** - Primary use case\n",
    "  - Structured data                                     \n",
    "  - Schema-on-write                                     \n",
    "  - Designed for dimensional modeling                   \n",
    "  - fact_sales, dim_customer, dim_product, etc. \n",
    "- **LAKEHOUSE: ✓ YES** - In the **Gold/Curated layer**\n",
    "  - Bronze (Raw)    → No schema, just raw files                        \n",
    "  - Silver (Clean)  → Some structure, cleaned data                     \n",
    "  - Gold (Curated)  → Star/Snowflake schema for analytics ✓\n",
    "- **STREAMING STORAGE: ✗ NO** - Different model\n",
    "  - Event-based, not dimensional                                      \n",
    "  - Time-series data                                                  \n",
    "  - Append-only logs                                                  \n",
    "  - No fact/dimension concept  \n",
    "- **VECTOR STORAGE: ✗ NO** - Completely different \n",
    "  - Stores embeddings (high-dimensional vectors)                      \n",
    "  - Similarity search, not SQL joins                                  \n",
    "  - No relational model   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Storage Technology Comparison\n",
    "\n",
    "| Technology | Category | Type | Data Format | Scale | Latency | Cost |\n",
    "|------------|----------|------|-------------|-------|---------|------|\n",
    "| **HDFS** | File System | Distributed | Any | PB | High | $$ |\n",
    "| **S3** | Object Storage | Cloud | Any | EB | Medium | $ |\n",
    "| **GCS** | Object Storage | Cloud | Any | EB | Medium | $ |\n",
    "| **Azure Blob** | Object Storage | Cloud | Any | EB | Medium | $ |\n",
    "| **PostgreSQL** | RDBMS | Relational | Structured | TB | Low | $$ |\n",
    "| **MySQL** | RDBMS | Relational | Structured | TB | Low | $$ |\n",
    "| **Oracle** | RDBMS | Relational | Structured | TB | Low | $$$$ |\n",
    "| **MongoDB** | NoSQL | Document | JSON-like | TB | Low | $ |\n",
    "| **Cassandra** | NoSQL | Wide-Column | Flexible | PB | Low | $ |\n",
    "| **DynamoDB** | NoSQL | Key-Value | Flexible | PB | Very Low | $$ |\n",
    "| **Redis** | NoSQL | In-Memory | Any | GB-TB | Sub-ms | $$ |\n",
    "| **Neo4j** | NoSQL | Graph | Graph | TB | Low | $$ |\n",
    "| **Snowflake** | Data Warehouse | Cloud DW | Structured | PB | Medium | $$ |\n",
    "| **BigQuery** | Data Warehouse | Serverless DW | Structured | PB | Medium | $ |\n",
    "| **Redshift** | Data Warehouse | Cloud DW | Structured | PB | Medium | $$ |\n",
    "| **ClickHouse** | Data Warehouse | OLAP | Structured | PB | Very Low | $ |\n",
    "| **Delta Lake** | Lakehouse | Table Format | Any + ACID | PB | Medium | $ |\n",
    "| **Apache Iceberg** | Lakehouse | Table Format | Any + ACID | PB | Medium | $ |\n",
    "| **Apache Hudi** | Lakehouse | Table Format | Any + ACID | PB | Medium | $ |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Data Processing & Compute\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│   STORAGE ≠ COMPUTE                                             │\n",
    "│   ═══════════════════                                           │\n",
    "│   STORAGE = Where data lives      COMPUTE = How data transforms │\n",
    "│   (Passive)                       (Active)                      │\n",
    "│   ┌─────────────┐                ┌─────────────┐                │\n",
    "│   │    HDFS     │                │   Spark     │                │\n",
    "│   │    S3       │◄──── read ────▶│   Flink     │                │\n",
    "│   │  Snowflake  │◄──── write ───▶│   dbt       │                │\n",
    "│   └─────────────┘                └─────────────┘                │\n",
    "│   Think of it like:                                             │\n",
    "│   Storage = Hard drive / Filing cabinet                         │\n",
    "│   Compute = CPU / Workers processing files                      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Paradigms: Batch vs Stream\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   PROCESSING PARADIGMS                           │\n",
    "├──────────────────────────────┬────────────────────────────────────┤\n",
    "│      BATCH PROCESSING        │      STREAM PROCESSING             │\n",
    "├──────────────────────────────┼────────────────────────────────────┤\n",
    "│                              │                                     │\n",
    "│  Process bounded dataset     │  Process unbounded stream          │\n",
    "│                              │                                     │\n",
    "│  ┌──────────────────────┐    │  ────▶────▶────▶────▶────▶        │\n",
    "│  │  Yesterday's Data    │    │  event event event event event    │\n",
    "│  │  [████████████████]  │    │    │     │     │     │     │      │\n",
    "│  └──────────┬───────────┘    │    ▼     ▼     ▼     ▼     ▼      │\n",
    "│             │                │  ┌─────────────────────────────┐   │\n",
    "│             ▼                │  │    Process immediately      │   │\n",
    "│  ┌──────────────────────┐    │  └─────────────────────────────┘   │\n",
    "│  │     Process All      │    │                                     │\n",
    "│  │     At Once          │    │                                     │\n",
    "│  └──────────────────────┘    │                                     │\n",
    "│                              │                                     │\n",
    "│  Latency: Minutes-Hours      │  Latency: Milliseconds-Seconds     │\n",
    "│                              │                                     │\n",
    "│  Use Cases:                  │  Use Cases:                        │\n",
    "│  - Daily/weekly report       │  - Fraud detection                 │\n",
    "│  - ML model training         │  - Real-time dashboards            │\n",
    "│  - Historical analysis       │  - IoT monitoring                  │\n",
    "│  - Data warehouse loads(ETL) │  - Live recommendations            │\n",
    "│  - Historical trend analysis │  - Social media trend detection    │\n",
    "│                              │                                    │\n",
    "│  Tools:                      │  Tools:                            │\n",
    "│  - Spark (batch mode)        │  - Spark Streaming                 │\n",
    "│  - dbt                       │  - Apache Flink                    │\n",
    "│  - AWS Glue                  │  - Kafka Streams                   │\n",
    "│  - Hadoop MapReduce          │  - Apache Storm                    │\n",
    "│                              │                                     │\n",
    "└──────────────────────────────┴────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Comparison**:   \n",
    "\n",
    "| Aspect | Batch Processing | Streaming Processing |\n",
    "|--------|------------------|----------------------|\n",
    "| **Data Scope** | Large historical datasets | Individual events/micro-batches |\n",
    "| **Timing** | Scheduled (hourly/daily/weekly) | Continuous (real-time) |\n",
    "| **Latency** | High (minutes to hours) | Low (milliseconds to seconds) |\n",
    "| **Throughput** | Very high | Moderate |\n",
    "| **Complexity** | Lower | Higher (state management, ordering) |\n",
    "| **Use Cases** | ETL, reporting, warehousing | Monitoring, fraud detection, IoT |\n",
    "\n",
    "#### Batch Processing Examples\n",
    "\n",
    "```text\n",
    "Daily Sales Aggregation Pipeline:\n",
    "┌───────────┐    ┌───────────┐    ┌───────────┐    ┌───────────┐\n",
    "│Raw Sales  │───▶│ Clean &   │───▶│ Aggregate │───▶│  Daily    │\n",
    "│   Logs    │    │ Transform │    │ by Region │    │  Report   │\n",
    "└───────────┘    └───────────┘    └───────────┘    └───────────┘\n",
    "     ↑\n",
    "  Runs at midnight daily\n",
    "```\n",
    "\n",
    "#### Streaming Processing Examples\n",
    "\n",
    "```text\n",
    "Fraud Detection Pipeline:\n",
    "┌───────────┐    ┌───────────┐    ┌───────────┐    ┌───────────┐\n",
    "│Credit Card│───▶│  Feature  │───▶│   ML      │───▶│  Alert/   │\n",
    "│Transaction│    │Extraction │    │Inference  │    │  Block    │\n",
    "└───────────┘    └───────────┘    └───────────┘    └───────────┘\n",
    "     ↑                                                    ↓\n",
    "  Each transaction                               < 100ms latency\n",
    "```\n",
    "\n",
    "\n",
    "#### Popular Frameworks Comparison\n",
    "\n",
    "| Framework | Type | Strengths |\n",
    "|-----------|------|-----------|\n",
    "| **Apache Spark** | Batch + Micro-batch streaming | Unified API, mature ecosystem |\n",
    "| **Apache Flink** | True streaming + Batch | Low latency, exactly-once semantics |\n",
    "| **Apache Kafka Streams** | Streaming | Lightweight, Kafka-native |\n",
    "| **Apache Storm** | Streaming | Real-time, low latency |\n",
    "| **AWS Kinesis** | Streaming | Managed, AWS integration |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL vs ELT\n",
    "\n",
    "<img src=\"./pic/1_ETLandELT.png\" width=500>\n",
    "\n",
    "the choice between ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) is primarily defined by the location of the \"heavy lifting\" (transformation) and the intended use of the data.  \n",
    "\n",
    "The fundamental difference is **where and when data is transformed**:  \n",
    "\n",
    "- **ETL**: Data is transformed on a **secondary processing server** (**external** to the storage) before it is loaded.\n",
    "- **ELT**: Raw data is loaded directly into the destination system (**data lake or cloud warehouse**), where transformations occur using that system's **internal compute power**.\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│   TRADITIONAL ETL               MODERN ELT                      │\n",
    "│   ═══════════════               ══════════                      │\n",
    "│   ┌────────┐                    ┌────────┐                      │\n",
    "│   │ Source │                    │ Source │                      │\n",
    "│   └───┬────┘                    └───┬────┘                      │\n",
    "│       ▼                             ▼                           │\n",
    "│   ┌────────┐                    ┌────────┐                      │\n",
    "│   │Extract │                    │Extract │                      │\n",
    "│   └───┬────┘                    │   &    │                      │\n",
    "│       │                         │ Load   │ (Fivetran, Airbyte)  │\n",
    "│       ▼                         └───┬────┘                      │\n",
    "│   ┌─────────┐                       │                           │\n",
    "│   │Transform│ ◄── External          │                           │\n",
    "│   │ Server  │                       ▼                           │\n",
    "│   └───┬─────┘                  ┌─────────────┐                  │\n",
    "│       │                        │   Data      │                  │\n",
    "│       ▼                        │ Warehouse   │                  │\n",
    "│   ┌────────┐                   │             │                  │\n",
    "│   │ Load   │                   │ ┌─────────┐ │                  │\n",
    "│   └───┬────┘                   │ │Transform│ │◄── dbt           │\n",
    "│       │                        │ │ (SQL)   │ │                  │\n",
    "│       ▼                        │ └─────────┘ │                  │\n",
    "│   ┌────────┐                   └─────────────┘                  │\n",
    "│   │  DW    │                                                    │\n",
    "│   └────────┘                                                    │\n",
    "│   Pros: Control                Pros: Simpler, faster            │\n",
    "│   Cons: Complex, slow          Cons: Warehouse compute cost     │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "#### Detailed Comparison\n",
    "\n",
    "| Aspect | ETL | ELT |\n",
    "|--------|-----|-----|\n",
    "| **Processing Location** | External ETL server | Inside target data warehouse |\n",
    "| **Data Movement** | Raw → ETL Tool → DW | Raw → DW (Staging) → DW (Final) |\n",
    "| **Best For** | Structured data, smaller volumes | Large volumes, cloud DWs |\n",
    "| **Compute Cost** | Dedicated ETL infrastructure | Uses DW compute power |\n",
    "| **Flexibility** | Rigid, pre-defined transformations | Flexible, can re-transform |\n",
    "| **Historical Data** | Only transformed data kept | Raw data preserved |\n",
    "| **Speed** | Slower (extra hop) | Faster (direct load) |\n",
    "| **Tools** | Informatica, Talend, SSIS | dbt, Snowflake, BigQuery |\n",
    "\n",
    "#### When to Use Each\n",
    "\n",
    "**Use ETL when:**\n",
    "- Data needs extensive cleaning before loading\n",
    "- Target system has limited compute\n",
    "- Sensitive data must be masked before storage\n",
    "- Working with on-premise legacy systems\n",
    "\n",
    "**Use ELT when:**\n",
    "- Using cloud data warehouses (Snowflake, BigQuery, Redshift)\n",
    "- Need to preserve raw data\n",
    "- Transformations may change over time\n",
    "- High data volumes require parallel processing\n",
    "\n",
    "#### Modern ELT Example with dbt\n",
    "\n",
    "```sql\n",
    "-- models/staging/stg_orders.sql\n",
    "WITH source AS (\n",
    "    SELECT * FROM {{ source('raw', 'orders') }}\n",
    "),\n",
    "\n",
    "cleaned AS (\n",
    "    SELECT\n",
    "        order_id,\n",
    "        customer_id,\n",
    "        CAST(order_date AS DATE) AS order_date,\n",
    "        ROUND(amount, 2) AS amount,\n",
    "        LOWER(status) AS status\n",
    "    FROM source\n",
    "    WHERE order_id IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT * FROM cleaned\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce - Batch Processing Only\n",
    "\n",
    "- MapReduce is a fundamental programming model and distributed execution framework, designed for processing large datasets in parallel across a distributed cluster on disk.\n",
    "- Introduced by Google (2004)\n",
    "- Implemented in Hadoop (2006) \n",
    "- NOT for real-time or streaming\n",
    "\n",
    "<img src=\"./pic/1_map-reduce-mode.png\" width=500>\n",
    "\n",
    "**Detailed Phases**:  \n",
    "\n",
    "| Phase | Function | Description |\n",
    "|-------|----------|-------------|\n",
    "| **Split** | Divide input | Break large file into chunks (typically 64-128MB) |\n",
    "| **Map** | Transform | Apply function to each record, emit key-value pairs |\n",
    "| **Shuffle** | Redistribute | Group all values by key across the cluster |\n",
    "| **Sort** | Order | Sort intermediate data by key |\n",
    "| **Reduce** | Aggregate | Combine values for each key into final result |\n",
    "\n",
    "Classic Example: Word Count   \n",
    "\n",
    "\n",
    "<img src=\"./pic/1_mapreduce_eg_word_count.png\" width=500>\n",
    "\n",
    "#### Automatic Handling\n",
    "\n",
    "MapReduce automatically manages:\n",
    "- **Partitioning:** Distributes data across nodes\n",
    "- **Retries:** Re-executes failed tasks\n",
    "- **Shuffle:** Moves intermediate data between map and reduce\n",
    "- **Scheduling:** Assigns tasks to available workers\n",
    "\n",
    "#### Limitations of MapReduce\n",
    "\n",
    "| Limitation | Explanation | Impact |\n",
    "|------------|-------------|--------|\n",
    "| **Disk-heavy** | Writes intermediate results to disk | Slow I/O operations |\n",
    "| **Poor for Iterative Jobs** | Must read/write disk between iterations | ML algorithms suffer |\n",
    "| **High Latency** | Batch-oriented, not real-time | Minutes to hours for results |\n",
    "| **Hard to Debug** | Distributed nature complicates debugging | Development overhead |\n",
    "| **Rigid Model** | Only Map and Reduce operations | Complex logic is awkward |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark - The Swiss Army Knife\n",
    "\n",
    "Spark is a **unified analytics engine** that **overcomes MapReduce limitations** through in-memory processing.\n",
    "\n",
    "#### Spark Architecture\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      APACHE SPARK                               │\n",
    "│              \"Unified Analytics Engine\"                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                    SPARK CORE                           │   │\n",
    "│   │            (Distributed Compute Engine)                 │   │\n",
    "│   │                                                         │   │\n",
    "│   │   • In-memory processing (100x faster than MapReduce)   │   │\n",
    "│   │   • DAG execution (optimizes query plans)               │   │\n",
    "│   │   • Fault tolerance via lineage                         │   │\n",
    "│   │   • Runs on: YARN, Kubernetes, Standalone, Mesos        │   │\n",
    "│   └─────────────────────────────────────────────────────────┘   │\n",
    "│        ┌────────────────────┼────────────────────┐              │\n",
    "│        ▼                    ▼                    ▼              │\n",
    "│   ┌──────────┐        ┌──────────┐        ┌──────────┐          │\n",
    "│   │Spark SQL │        │  MLlib   │        │Structured│          │\n",
    "│   │          │        │          │        │Streaming │          │\n",
    "│   │DataFrames│        │  ML at   │        │Real-time │          │\n",
    "│   │& SQL     │        │  Scale   │        │Processing│          │\n",
    "│   └──────────┘        └──────────┘        └──────────┘          │\n",
    "│   │                   │                   │                     │\n",
    "│   │ Analytics         │ Mining            │ Streaming           │\n",
    "│   │ Layer             │ Layer             │ Layer               │\n",
    "│                                                                 │\n",
    "│   READS FROM:               WRITES TO:                          │\n",
    "│   • HDFS                    • HDFS                              │\n",
    "│   • S3/GCS/Azure            • S3/GCS/Azure                      │\n",
    "│   • JDBC databases          • JDBC databases                    │\n",
    "│   • Kafka                   • Kafka                             │\n",
    "│   • Delta Lake/Iceberg      • Delta Lake/Iceberg                │\n",
    "│   • Cassandra, HBase        • Data Warehouses                   │\n",
    "│                                                                 │\n",
    "│   ════════════════════════════════════════════════════════════  │\n",
    "│   SPARK IS COMPUTE, NOT STORAGE!                                │\n",
    "│   ════════════════════════════════════════════════════════════  │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      SPARK APPLICATION                      │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌──────────────┐                                           │\n",
    "│  │    Driver    │ ◄── SparkContext, DAG Scheduler           │\n",
    "│  │   Program    │                                           │\n",
    "│  └──────┬───────┘                                           │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌──────────────┐                                           │\n",
    "│  │   Cluster    │ ◄── YARN / Mesos / Kubernetes / Standalone│\n",
    "│  │   Manager    │                                           │\n",
    "│  └──────┬───────┘                                           │\n",
    "│         │                                                   │\n",
    "│    ┌────┴────┬────────┬────────┐                            │\n",
    "│    ▼         ▼        ▼        ▼                            │\n",
    "│ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐                         │\n",
    "│ │Worker│ │Worker│ │Worker│ │Worker│                         │\n",
    "│ │Node 1│ │Node 2│ │Node 3│ │Node N│                         │\n",
    "│ └──────┘ └──────┘ └──────┘ └──────┘                         │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "#### Spark APIs Comparison\n",
    "\n",
    "| API | Abstraction Level | Type Safety | Use Case |\n",
    "|-----|-------------------|-------------|----------|\n",
    "| **RDD** | Low-level | Compile-time | Fine-grained control |\n",
    "| **DataFrame** | High-level | Runtime | SQL-like operations |\n",
    "| **Dataset** | High-level | Compile-time | Type-safe structured data |\n",
    "| **Spark SQL** | Highest | Runtime | SQL queries on structured data |\n",
    "\n",
    "#### Key Advantages\n",
    "\n",
    "| Feature | Description | Benefit |\n",
    "|---------|-------------|---------|\n",
    "| **In-memory Processing** | Keeps data in **RAM** between operations | Up to 100x faster than MapReduce |\n",
    "| **DAG Execution** | Directed Acyclic Graph optimizer | Efficient query planning |\n",
    "| **Rich APIs** | RDD, DataFrame, Dataset, SQL | Flexible programming models |\n",
    "| **Unified Engine** | Batch + Streaming in one framework | Simplified architecture |\n",
    "| **Built-in Fault Tolerance** | Lineage-based recovery | Automatic failure handling |\n",
    "\n",
    "\n",
    "\n",
    "**Example - Word Count in Spark (Python):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Read text file\n",
    "text_df = spark.read.text(\"input.txt\")\n",
    "\n",
    "# Word count using DataFrame API\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "word_counts = text_df \\\n",
    "    .select(explode(split(col(\"value\"), \" \")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "word_counts.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Flink - True Streaming\n",
    "\n",
    "```text\n",
    "    ┌─────────────────────────────────────────────────────────────────┐\n",
    "    │                      APACHE FLINK                               │\n",
    "    │              \"Stateful Stream Processing\"                       │\n",
    "    ├─────────────────────────────────────────────────────────────────┤\n",
    "    │                                                                 │\n",
    "    │  Flink processes data as STREAMS first (batch = bounded stream) │\n",
    "    │                                                                 │\n",
    "    │   ┌─────────────────────────────────────────────────────────┐   │\n",
    "    │   │                    DATA STREAM                          │   │\n",
    "    │   │                                                         │   │\n",
    "    │   │  ──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶──▶       │   │\n",
    "    │   │  event event event event event event event event        │   │\n",
    "    │   │                                                         │   │\n",
    "    │   └───────────────────────┬─────────────────────────────────┘   │\n",
    "    │                           ▼                                     │\n",
    "    │   ┌─────────────────────────────────────────────────────────┐   │\n",
    "    │   │                  FLINK CLUSTER                          │   │\n",
    "    │   │                                                         │   │\n",
    "    │   │  ┌──────────┐    ┌──────────┐    ┌──────────┐           │   │\n",
    "    │   │  │   Job    │    │   Task   │    │   Task   │           │   │\n",
    "    │   │  │ Manager  │───▶│ Manager  │    │ Manager  │           │   │\n",
    "    │   │  │(Master)  │    │(Worker)  │    │(Worker)  │           │   │\n",
    "    │   │  └──────────┘    └──────────┘    └──────────┘           │   │\n",
    "    │   │                                                         │   │\n",
    "    │   │  Features:                                              │   │\n",
    "    │   │  • Exactly-once semantics                               │   │\n",
    "    │   │  • Event time processing                                │   │\n",
    "    │   │  • Stateful computations                                │   │\n",
    "    │   │  • Low latency (milliseconds)                           │   │\n",
    "    │   │  • High throughput                                      │   │\n",
    "    │   │                                                         │   │\n",
    "    │   └─────────────────────────────────────────────────────────┘   │\n",
    "    └─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbt (Data Build Tool)\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                           dbt                                   │\n",
    "│              \"Transform Data in Warehouse\"                      │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  dbt = SQL-based transformation framework                       │\n",
    "│  Runs INSIDE your data warehouse (ELT, not ETL)                 │\n",
    "│                                                                 │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                  dbt Project                            │   │\n",
    "│   │                                                         │   │\n",
    "│   │   models/                                               │   │\n",
    "│   │   ├── staging/           ◄── Clean raw data             │   │\n",
    "│   │   │   ├── stg_orders.sql                                │   │\n",
    "│   │   │   └── stg_customers.sql                             │   │\n",
    "│   │   ├── intermediate/      ◄── Business logic             │   │\n",
    "│   │   │   └── int_orders_enriched.sql                       │   │\n",
    "│   │   └── marts/             ◄── Final tables               │   │\n",
    "│   │       ├── dim_customers.sql                             │   │\n",
    "│   │       └── fct_orders.sql                                │   │\n",
    "│   │                                                         │   │\n",
    "│   │   tests/                 ◄── Data quality               │   │\n",
    "│   │   └── assert_positive_amounts.sql                       │   │\n",
    "│   │                                                         │   │\n",
    "│   └─────────────────────────────────────────────────────────┘   │\n",
    "│   Example dbt model (stg_orders.sql):                           │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │  SELECT                                                 │   │\n",
    "│   │      order_id,                                          │   │\n",
    "│   │      customer_id,                                       │   │\n",
    "│   │      CAST(order_date AS DATE) AS order_date,            │   │\n",
    "│   │      ROUND(amount, 2) AS amount,                        │   │\n",
    "│   │      LOWER(status) AS status                            │   │\n",
    "│   │  FROM {{ source('raw', 'orders') }}                     │   │\n",
    "│   │  WHERE order_id IS NOT NULL                             │   │\n",
    "│   └─────────────────────────────────────────────────────────┘   │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Technology Comparison\n",
    "\n",
    "| Technology | Type | Latency | Throughput | Best For |\n",
    "|------------|------|---------|------------|----------|\n",
    "| **MapReduce** | Batch | Very High | High | Legacy Hadoop jobs |\n",
    "| **Apache Spark** | Batch + Micro-batch | Medium | Very High | General purpose, ML |\n",
    "| **Apache Flink** | True Streaming | Very Low | High | Real-time, event-driven |\n",
    "| **dbt** | Batch (SQL) | Medium | Medium | SQL transformations |\n",
    "| **Kafka Streams** | Streaming | Low | High | Kafka-native apps |\n",
    "| **AWS Glue** | Batch | Medium | High | Serverless ETL on AWS |\n",
    "| **Dataflow** | Both | Low | High | GCP-native |\n",
    "| **Presto/Trino** | Interactive | Low | Medium | Ad-hoc queries |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Data Mining & Machine Learning\n",
    "\n",
    "### Where Mining Fits\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                                                                  │\n",
    "│   Data Mining = Finding patterns you didn't know existed        │\n",
    "│   Analytics   = Answering questions you already have            │\n",
    "│                                                                  │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                    DATA                                  │   │\n",
    "│   └─────────────────────────┬───────────────────────────────┘   │\n",
    "│                             │                                    │\n",
    "│           ┌─────────────────┴─────────────────┐                 │\n",
    "│           │                                   │                 │\n",
    "│           ▼                                   ▼                 │\n",
    "│   ┌───────────────┐                   ┌───────────────┐         │\n",
    "│   │  DATA MINING  │                   │   ANALYTICS   │         │\n",
    "│   │               │                   │               │         │\n",
    "│   │ \"What patterns│                   │ \"How much did │         │\n",
    "│   │  exist in     │                   │  we sell last │         │\n",
    "│   │  customer     │                   │  month?\"      │         │\n",
    "│   │  behavior?\"   │                   │               │         │\n",
    "│   │               │                   │ (Known        │         │\n",
    "│   │ (Unknown      │                   │  questions)   │         │\n",
    "│   │  discoveries) │                   │               │         │\n",
    "│   └───────────────┘                   └───────────────┘         │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Mining Techniques Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    DATA MINING TECHNIQUES                        │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   SUPERVISED LEARNING            UNSUPERVISED LEARNING          │\n",
    "│   (Have labels/answers)          (No labels)                    │\n",
    "│                                                                  │\n",
    "│   ┌─────────────────┐            ┌─────────────────┐            │\n",
    "│   │ CLASSIFICATION  │            │   CLUSTERING    │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ Predict category│            │ Find groups     │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ • Spam/Not spam │            │ • Customer      │            │\n",
    "│   │ • Fraud/Legit   │            │   segments      │            │\n",
    "│   │ • Churn/Stay    │            │ • Similar docs  │            │\n",
    "│   │ • Disease/No    │            │ • Anomalies     │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ Algorithms:     │            │ Algorithms:     │            │\n",
    "│   │ • Random Forest │            │ • K-Means       │            │\n",
    "│   │ • XGBoost       │            │ • DBSCAN        │            │\n",
    "│   │ • Neural Nets   │            │ • Hierarchical  │            │\n",
    "│   └─────────────────┘            └─────────────────┘            │\n",
    "│                                                                  │\n",
    "│   ┌─────────────────┐            ┌─────────────────┐            │\n",
    "│   │   REGRESSION    │            │  ASSOCIATION    │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ Predict number  │            │ Find rules      │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ • House price   │            │ • Market basket │            │\n",
    "│   │ • Sales forecast│            │ • \"Customers    │            │\n",
    "│   │ • CLV           │            │   who bought X  │            │\n",
    "│   │ • Stock price   │            │   also buy Y\"   │            │\n",
    "│   │                 │            │                 │            │\n",
    "│   │ Algorithms:     │            │ Algorithms:     │            │\n",
    "│   │ • Linear Reg    │            │ • Apriori       │            │\n",
    "│   │ • Gradient Boost│            │ • FP-Growth     │            │\n",
    "│   │ • Neural Nets   │            │                 │            │\n",
    "│   └─────────────────┘            └─────────────────┘            │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### ML/Mining Tools at Scale\n",
    "\n",
    "| Tool | Scale | Best For | Integration |\n",
    "|------|-------|----------|-------------|\n",
    "| **Scikit-learn** | Single machine (GB) | Prototyping, small data | Python |\n",
    "| **Spark MLlib** | Distributed (PB) | Large-scale ML | Spark ecosystem |\n",
    "| **TensorFlow** | Single/Distributed | Deep learning | TFX, Vertex AI |\n",
    "| **PyTorch** | Single/Distributed | Deep learning, research | TorchServe |\n",
    "| **XGBoost** | Single/Distributed | Tabular data, competitions | Spark, Dask |\n",
    "| **Amazon SageMaker** | Managed | End-to-end ML | AWS |\n",
    "| **Vertex AI** | Managed | End-to-end ML | GCP |\n",
    "| **Databricks ML** | Distributed | Unified ML platform | Spark, MLflow |\n",
    "\n",
    "### Example: Customer Segmentation with Spark MLlib\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load data from warehouse (STORAGE)\n",
    "customers = spark.read.table(\"analytics.customer_features\")\n",
    "\n",
    "# Prepare features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"recency\", \"frequency\", \"monetary\"],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# K-Means clustering (MINING)\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Fit model\n",
    "model = pipeline.fit(customers)\n",
    "\n",
    "# Get predictions\n",
    "segmented = model.transform(customers)\n",
    "\n",
    "# Analyze segments\n",
    "segmented.groupBy(\"prediction\").agg(\n",
    "    F.avg(\"recency\").alias(\"avg_recency\"),\n",
    "    F.avg(\"frequency\").alias(\"avg_frequency\"),\n",
    "    F.avg(\"monetary\").alias(\"avg_monetary\"),\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").show()\n",
    "\n",
    "# Save results back to warehouse (STORAGE)\n",
    "segmented.write.saveAsTable(\"analytics.customer_segments\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: Data Analytics\n",
    "\n",
    "### Analytics Maturity Levels\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    ANALYTICS SPECTRUM                            │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   VALUE & COMPLEXITY                                            │\n",
    "│        ▲                                                         │\n",
    "│        │                                    ┌───────────────┐   │\n",
    "│        │                             ┌─────▶│ PRESCRIPTIVE  │   │\n",
    "│        │                             │      │ \"What should  │   │\n",
    "│        │                    ┌────────┴──┐   │  we do?\"      │   │\n",
    "│        │             ┌─────▶│ PREDICTIVE │   │               │   │\n",
    "│        │             │      │ \"What will │   │ • Optimization│   │\n",
    "│        │    ┌────────┴──┐   │  happen?\"  │   │ • Simulation  │   │\n",
    "│        │ ┌─▶│DIAGNOSTIC │   │            │   │ • Recommend   │   │\n",
    "│        │ │  │ \"Why did  │   │ • ML Models│   └───────────────┘   │\n",
    "│   ┌────┴─┴┐ │ it happen?\"│   │ • Forecast │                      │\n",
    "│   │DESCRIP│ │            │   └────────────┘                      │\n",
    "│   │-TIVE  │ │ • Drill-down│                                       │\n",
    "│   │\"What  │ │ • Root cause│                                       │\n",
    "│   │happened│ └────────────┘                                       │\n",
    "│   │?\"     │                                                       │\n",
    "│   │       │                                                       │\n",
    "│   │• Reports                                                      │\n",
    "│   │• Dashboards                                                  │\n",
    "│   │• KPIs                                                        │\n",
    "│   └───────┘                                                       │\n",
    "│   └───────────────────────────────────────────────────────▶     │\n",
    "│                            TIME                                  │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Analytics Tools\n",
    "\n",
    "| Type | Tools | Use Case |\n",
    "|------|-------|----------|\n",
    "| **Descriptive** | SQL, Excel, Tableau | Reports, dashboards |\n",
    "| **Diagnostic** | SQL, Python | Root cause analysis |\n",
    "| **Predictive** | Python, Spark MLlib | Forecasting, ML |\n",
    "| **Prescriptive** | OR tools, simulation | Optimization |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Data Visualization & Consumption\n",
    "\n",
    "### How Data Reaches End Users\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   DATA CONSUMPTION METHODS                       │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                    DATA WAREHOUSE                        │   │\n",
    "│   └───────────────────────────┬─────────────────────────────┘   │\n",
    "│                               │                                  │\n",
    "│       ┌───────────────────────┼───────────────────────┐         │\n",
    "│       │                       │                       │         │\n",
    "│       ▼                       ▼                       ▼         │\n",
    "│   ┌───────────┐         ┌───────────┐         ┌───────────┐    │\n",
    "│   │DASHBOARDS │         │  REPORTS  │         │   APIs    │    │\n",
    "│   │           │         │           │         │           │    │\n",
    "│   │ Interactive│        │ Scheduled │         │Programmatic│   │\n",
    "│   │ exploration│        │ delivery  │         │ access    │    │\n",
    "│   │           │         │           │         │           │    │\n",
    "│   │ Tableau   │         │ PDF/Email │         │ REST/     │    │\n",
    "│   │ Power BI  │         │ Exports   │         │ GraphQL   │    │\n",
    "│   │ Looker    │         │           │         │           │    │\n",
    "│   └─────┬─────┘         └─────┬─────┘         └─────┬─────┘    │\n",
    "│         │                     │                     │           │\n",
    "│         ▼                     ▼                     ▼           │\n",
    "│   ┌───────────┐         ┌───────────┐         ┌───────────┐    │\n",
    "│   │ Analysts  │         │ Executives│         │Applications│   │\n",
    "│   │ Explore   │         │ Consume   │         │ Integrate  │    │\n",
    "│   └───────────┘         └───────────┘         └───────────┘    │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Visualization Tools\n",
    "\n",
    "| Tool | Type | Best For | Users |\n",
    "|------|------|----------|-------|\n",
    "| **Tableau** | Enterprise BI | Beautiful visuals | Analysts |\n",
    "| **Power BI** | Enterprise BI | Microsoft ecosystem | Business users |\n",
    "| **Looker** | Semantic layer | Governed metrics | Data teams |\n",
    "| **Metabase** | Open-source | Self-service | Everyone |\n",
    "| **Grafana** | Monitoring | Time-series, real-time | DevOps |\n",
    "| **Apache Superset** | Open-source | SQL-based exploration | Analysts |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: Orchestration & Governance\n",
    "\n",
    "### Orchestration\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    APACHE AIRFLOW DAG                            │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│   Daily Sales Pipeline:                                         │\n",
    "│                                                                  │\n",
    "│   ┌──────────┐    ┌──────────┐    ┌──────────┐                 │\n",
    "│   │ Extract  │───▶│Transform │───▶│  Load    │                 │\n",
    "│   │ from DB  │    │ (Spark)  │    │   (DW)   │                 │\n",
    "│   └──────────┘    └──────────┘    └────┬─────┘                 │\n",
    "│                                        │                        │\n",
    "│                           ┌────────────┼────────────┐          │\n",
    "│                           │            │            │          │\n",
    "│                           ▼            ▼            ▼          │\n",
    "│                    ┌──────────┐ ┌──────────┐ ┌──────────┐      │\n",
    "│                    │  Update  │ │  Train   │ │  Send    │      │\n",
    "│                    │Dashboard │ │ ML Model │ │  Alert   │      │\n",
    "│                    └──────────┘ └──────────┘ └──────────┘      │\n",
    "│                                                                  │\n",
    "│   Schedule: Daily at 2:00 AM                                    │\n",
    "│   Retries: 3 with exponential backoff                          │\n",
    "│   Alerts: On failure → PagerDuty                               │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Tool | Best For |\n",
    "|------|----------|\n",
    "| **Apache Airflow** | Complex pipelines, Python DAGs |\n",
    "| **Dagster** | Modern, asset-centric |\n",
    "| **Prefect** | Python-native, cloud-first |\n",
    "| **dbt Cloud** | SQL transformations |\n",
    "\n",
    "### Data Governance\n",
    "\n",
    "| Component | Purpose | Tools |\n",
    "|-----------|---------|-------|\n",
    "| **Data Catalog** | What data exists? | DataHub, Atlan, Glue Catalog |\n",
    "| **Data Quality** | Is data accurate? | Great Expectations, dbt tests |\n",
    "| **Data Lineage** | Where did data come from? | OpenLineage, Marquez |\n",
    "| **Data Security** | Who can access what? | IAM, column-level security |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech Summary\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    KEY TAKEAWAYS                                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  1. Big Data is a PIPELINE: Ingest → Store → Process → Analyze │\n",
    "│                                                                  │\n",
    "│  2. Storage ≠ Compute                                           │\n",
    "│     • HDFS, S3 = Storage                                        │\n",
    "│     • Spark, Flink = Compute                                    │\n",
    "│                                                                  │\n",
    "│  3. Modern Trends:                                              │\n",
    "│     • Lakehouse (Delta Lake, Iceberg)                          │\n",
    "│     • ELT over ETL (transform in warehouse)                    │\n",
    "│     • SQL-first (dbt + Snowflake)                              │\n",
    "│     • Managed services (less ops)                              │\n",
    "│                                                                  │\n",
    "│  4. Choose based on:                                            │\n",
    "│     • Scale (GB vs TB vs PB)                                    │\n",
    "│     • Latency (batch vs real-time)                             │\n",
    "│     • Team skills (SQL vs Python)                              │\n",
    "│     • Cloud strategy (single vs multi)                         │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Technologies Mapping\n",
    "\n",
    "\n",
    "| Layer | Open Source | AWS | GCP | Azure |\n",
    "|-------|-------------|-----|-----|-------|\n",
    "| **Ingestion** | Kafka, Airbyte | Kinesis, DMS | Pub/Sub, Dataflow | EventHub |\n",
    "| **Lake Storage** | HDFS, MinIO | S3 | GCS | ADLS |\n",
    "| **Databases** | PostgreSQL, Cassandra | RDS, DynamoDB | Cloud SQL, Bigtable | Cosmos DB |\n",
    "| **Warehouse** | ClickHouse | Redshift | BigQuery | Synapse |\n",
    "| **Lakehouse** | Delta, Iceberg | Lake Formation | BigLake | Delta on Azure |\n",
    "| **Processing** | Spark, Flink | EMR, Glue | Dataproc, Dataflow | HDInsight |\n",
    "| **ML** | MLflow, Kubeflow | SageMaker | Vertex AI | Azure ML |\n",
    "| **Visualization** | Superset, Metabase | QuickSight | Looker | Power BI |\n",
    "| **Orchestration** | Airflow, Dagster | MWAA, Step Functions | Composer | Data Factory |\n",
    "| **Governance** | DataHub, Atlas | Glue Catalog | Data Catalog | Purview |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Patterns\n",
    "\n",
    "#### Modern Data Stack (Most Companies)\n",
    "\n",
    "```text\n",
    "Fivetran → Snowflake → dbt → Tableau\n",
    "```\n",
    "\n",
    "#### Lakehouse (Data-Intensive)\n",
    "\n",
    "```text\n",
    "Kafka → S3 + Delta Lake → Spark → Databricks SQL → BI Tools\n",
    "```\n",
    "\n",
    "#### Real-Time Analytics\n",
    "\n",
    "```text\n",
    "Kafka → Flink → ClickHouse → Grafana\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Platforms & AWS Services\n",
    "\n",
    "## Major Cloud Providers\n",
    "\n",
    "| Provider | Strengths | Data Services |\n",
    "|----------|-----------|---------------|\n",
    "| **AWS** | Largest market share, most services | S3, Redshift, EMR, Glue, Athena |\n",
    "| **Azure** | Enterprise integration, Microsoft ecosystem | Blob, Synapse, Databricks, Data Factory |\n",
    "| **Google Cloud** | BigQuery, AI/ML leadership | GCS, BigQuery, Dataflow, Dataproc |\n",
    "\n",
    "## AWS Data Services\n",
    "\n",
    "<Img src=\"./pic/1_aws_overview.jpg\" >\n",
    "\n",
    "### Key AWS Services for Big Data\n",
    "\n",
    "| Service | Category | Purpose |\n",
    "|---------|----------|---------|\n",
    "| **S3** | Storage | Object storage, data lake foundation |\n",
    "| **Redshift** | Data Warehouse | Petabyte-scale columnar analytics |\n",
    "| **EMR** | Processing | Managed Spark, Hadoop, Presto clusters |\n",
    "| **Athena** | Query | Serverless SQL queries on S3 |\n",
    "| **Glue** | ETL | Serverless data integration |\n",
    "| **Kinesis** | Streaming | Real-time data ingestion |\n",
    "| **DynamoDB** | Database | NoSQL for OLTP workloads |\n",
    "| **RDS/Aurora** | Database | Managed relational databases |\n",
    "| **QuickSight** | BI | Dashboards and visualization |\n",
    "| **SageMaker** | ML | Machine learning platform |\n",
    "\n",
    "### AWS Service Categories\n",
    "\n",
    "**Compute:**\n",
    "| Service | Type | Use Case |\n",
    "|---------|------|----------|\n",
    "| EC2 | Virtual Machines | Custom workloads, legacy apps |\n",
    "| Lambda | Serverless | Event-driven, short tasks |\n",
    "| ECS/EKS | Containers | Microservices, Kubernetes |\n",
    "\n",
    "**Databases:**\n",
    "| Service | Type | Use Case |\n",
    "|---------|------|----------|\n",
    "| RDS | Relational | Traditional OLTP |\n",
    "| Aurora | Relational | High-performance MySQL/PostgreSQL |\n",
    "| DynamoDB | NoSQL | Key-value, high scale |\n",
    "| DocumentDB | NoSQL | MongoDB-compatible |\n",
    "| ElastiCache | Cache | Redis/Memcached |\n",
    "\n",
    "**Analytics:**\n",
    "| Service | Type | Use Case |\n",
    "|---------|------|----------|\n",
    "| Redshift | Data Warehouse | BI, complex analytics |\n",
    "| Athena | Serverless Query | Ad-hoc S3 queries |\n",
    "| OpenSearch | Search | Log analytics, full-text search |\n",
    "| QuickSight | BI Tool | Dashboards, visualization |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Big Data Ecosystem\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      BIG DATA ECOSYSTEM                              │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│   SOURCES              INGESTION           STORAGE                  │\n",
    "│   ────────             ─────────           ───────                  │\n",
    "│   Databases            Kinesis             S3 (Data Lake)           │\n",
    "│   Applications         Kafka               HDFS                     │\n",
    "│   IoT/Sensors          Flume               Cloud Storage            │\n",
    "│   Logs                 NiFi                                         │\n",
    "│                                                                      │\n",
    "│   PROCESSING           TRANSFORMATION      SERVING                  │\n",
    "│   ──────────           ──────────────      ───────                  │\n",
    "│   Spark (Batch)        dbt                 Redshift (OLAP)          │\n",
    "│   Flink (Stream)       Glue                PostgreSQL (OLTP)        │\n",
    "│   EMR                  Dataflow            Elasticsearch            │\n",
    "│                                                                      │\n",
    "│   ORCHESTRATION        VISUALIZATION       GOVERNANCE               │\n",
    "│   ─────────────        ─────────────       ──────────               │\n",
    "│   Airflow              Tableau             Glue Catalog             │\n",
    "│   Step Functions       Power BI            Atlas                    │\n",
    "│   Prefect              Looker              DataHub                  │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Reference Card\n",
    "\n",
    "## The 4 Vs\n",
    "- **Volume:** Terabytes to Petabytes\n",
    "- **Velocity:** Batch to Real-time\n",
    "- **Variety:** Structured, Semi-structured, Unstructured\n",
    "- **Veracity:** Data quality and reliability\n",
    "\n",
    "## Key Comparisons\n",
    "\n",
    "| Batch | Streaming |\n",
    "|-------|-----------|\n",
    "| High latency | Low latency |\n",
    "| Historical data | Real-time events |\n",
    "| Scheduled | Continuous |    \n",
    "\n",
    "\n",
    "--  \n",
    "\n",
    "\n",
    "| OLTP | OLAP |\n",
    "|------|------|\n",
    "| Transactions | Analytics |\n",
    "| Row-based | Column-based |\n",
    "| Current data | Historical data |\n",
    "     \n",
    "--\n",
    "\n",
    "| ETL | ELT |\n",
    "|-----|-----|\n",
    "| Transform first | Load first |\n",
    "| External processing | In-warehouse processing |\n",
    "| On-premise | Cloud-native |\n",
    "\n",
    "--\n",
    "\n",
    "| Star Schema | Snowflake Schema |\n",
    "|-------------|------------------|\n",
    "| Denormalized | Normalized |\n",
    "| Faster queries | Less storage |\n",
    "| Simpler | Complex |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
