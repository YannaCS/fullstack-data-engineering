{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Conceptual Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1. Context Window:** What is the context window in an LLM, and why does it matter in real-world applications?  \n",
    "\n",
    "The context window of an LLM refers to the **maximum number of tokens** the model can process **at once**, including both **the prompt and its generated output**.  \n",
    "\n",
    "It matters because  \n",
    "- anything outside this window is effectively forgotten or ignored by the model. \n",
    "- In real-world applications, this determines how much history, instructions, or conversation you can provide before the model begins losing earlier information. \n",
    "- A larger context window improves continuity in long chats and enables processing of large documents. \n",
    "- However, longer contexts also increase cost, latency, and may dilute attention across irrelevant information. \n",
    "- Developers must balance context length, relevance, and retrieval strategies for reliable performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2. Tokens & Token Efficiency:** Why do LLM APIs price by tokens instead of characters or words? What does token efficiency mean in practice? \n",
    "\n",
    "LLM APIs price by tokens because \n",
    "- tokens represent the actual computational workload: \n",
    "  - models operate on token-level embeddings, not characters or words. \n",
    "- Characters are too granular and words too inconsistent across languages, whereas tokens provide a standardized unit across all text. \n",
    "\n",
    "Token efficiency refers to  \n",
    "- minimizing unnecessary tokens in both prompts and model outputs to reduce cost and latency. \n",
    "- In practice, this means writing concise prompts, removing redundant text, and using formats like JSON instead of verbose prose when appropriate. \n",
    "- Efficient token use helps reduce API expenses, improve throughput, and prevent hitting context window limits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3. Sampling Controls:** Explain what temperature, top-k, and top-p do during generation, and how you would tune them for: (a) a factual RAG-based Q&A bot, and (b) a creative brainstorming assistant.  \n",
    "- Temperature controls randomness: low values produce deterministic answers, while high values encourage creative variation. \n",
    "- Top-k limits sampling to the k most likely next tokens, reducing noise and making generation more controlled. \n",
    "- Top-p (nucleus sampling) chooses from the smallest probability mass that exceeds p, balancing creativity and coherence. \n",
    "\n",
    "For a factual RAG-based Q&A bot\n",
    "- you would set temperature low (0–0.2) and \n",
    "- use conservative top-p or top-k values to minimize hallucination. \n",
    "  \n",
    "For a creative brainstorming assistant\n",
    "- you would increase temperature (0.7–1.0) and \n",
    "- loosen top-p or top-k to allow more diverse and imaginative outputs.   \n",
    "\n",
    "Tuning these controls helps match the model’s behavior to user expectations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4. Prompt Engineering:** What is the difference between zero-shot and few-shot prompting? Give a concrete example of when you would choose each.\n",
    "\n",
    "- Zero-shot prompting means giving the model instructions with no examples, relying entirely on general pretrained knowledge. \n",
    "- Few-shot prompting adds representative examples, guiding the model by pattern demonstration.  \n",
    "\n",
    "Zero-shot is ideal when \n",
    "- the task is simple, well-known, or requires minimal structure\n",
    "- for example, asking for a definition or summarizing a document.   \n",
    "\n",
    "Few-shot prompting is preferred when \n",
    "- the output must follow a strict format or \n",
    "- when the task is nuanced, such as writing product descriptions in a specific brand voice.   \n",
    "  \n",
    "The choice depends on complexity, consistency needs, and how predictable the output format must be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q5. LLM Limitations:** If LLMs are so powerful, why do we still talk about hallucinations and outdated knowledge? Give a couple of concrete examples.\n",
    "\n",
    "LLMs can hallucinate because they generate text by predicting likely sequences, not by verifying facts.  \n",
    "\n",
    "They can also contain outdated knowledge if their training data doesn’t include recent information.   \n",
    "\n",
    "For example, an LLM might invent API parameters that don’t exist because they \"sound right,\" or might provide outdated information about laws or product versions.  \n",
    "\n",
    "Another limitation is that the model may confidently answer questions about nonexistent research papers or people. These issues arise from probabilistic generation and static training data, which makes external grounding essential for accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q6. RAG Definition:** What is Retrieval-Augmented Generation (RAG), and what are its main components?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines information retrieval with LLM generation to produce grounded, up-to-date, and contextually relevant responses.  \n",
    "\n",
    "Its main components include \n",
    "- a retriever, which finds relevant documents based on embeddings or keyword search, and \n",
    "- a generator, which conditions on those retrieved documents to produce an answer.  \n",
    "\n",
    "RAG reduces hallucination by anchoring the model to factual evidence. It also enables dynamic, domain-specific knowledge without the need for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q7. Naive RAG Pipeline:** Walk me through a naive RAG pipeline end-to-end, starting from raw documents and ending at the final answer.\n",
    "- A naive RAG pipeline starts by collecting raw documents such as PDFs, web pages, or text files. \n",
    "- These documents are cleaned and chunked into manageable segments to fit within embedding models’ limits. \n",
    "- Each chunk is transformed into an embedding and stored in a vector database. \n",
    "- When a user asks a question, the system embeds the query and retrieves the most semantically similar chunks. \n",
    "- The retrieved context is inserted into a prompt that instructs the LLM to answer using those sources. \n",
    "- The model then generates a final answer grounded in the retrieved information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q8. RAG vs Long Context:** If we can just use a giant LLM with a 1M token context window, why do we still need RAG?\n",
    "\n",
    "Even with large context windows (100K+ tokens):\n",
    "1. **Cost:** Processing huge contexts is expensive per query\n",
    "2. **Latency:** Longer contexts = slower response times\n",
    "3. **Relevance:** Finding needles in haystacks - RAG pre-filters relevant information\n",
    "4. **Scale:** You might have millions of documents, can't fit all in context\n",
    "5. **Precision:** RAG can search and retrieve exactly what's needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q9. RAG vs Fine-tuning:** When would you choose RAG instead of fine-tuning, and when is fine-tuning more appropriate?\n",
    "You choose RAG when   \n",
    "- the knowledge is large, frequently changing, or needs to remain external for compliance or cost reasons. \n",
    "- RAG allows updates without retraining and supports explainability by showing which documents informed the answer.   \n",
    "\n",
    "Fine-tuning is more appropriate when \n",
    "- the goal is adapting the model’s behavior, style, or reasoning patterns rather than injecting raw knowledge. \n",
    "- It also helps when the task requires domain-specific phrasing, custom tool-use patterns, or specialized reasoning techniques. \n",
    "\n",
    "In practice, many systems combine both: \n",
    "- RAG for knowledge grounding and \n",
    "- fine-tuning for stylistic or process improvements.\n",
    "\n",
    "| Use Case | Best Approach | Reason |\n",
    "|----------|---------------|---------|\n",
    "| Company documentation Q&A | RAG | Frequently updated, need citations |\n",
    "| Medical chatbot (specific terminology) | Fine-tuning | Learn specialized language patterns |\n",
    "| Legal document search | RAG | Large corpus, need exact citations |\n",
    "| Brand voice for marketing | Fine-tuning | Consistent style/tone |\n",
    "| Customer support with product manuals | RAG + Fine-tuning | RAG for facts, fine-tuning for tone |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q10. Structured Data Extraction:** How would you design a prompt to extract structured data (JSON) from unstructured text?\n",
    "- the prompt should clearly specify the **desired schema, constraints, and formatting rules**. \n",
    "- It is helpful to include **examples** of valid JSON, even for slightly different inputs, to guide model behavior. \n",
    "- The prompt should instruct the model **not to add** commentary and to return only valid, parseable JSON. \n",
    "- It should also define how to handle **missing or uncertain fields**, such as using null or empty strings.  \n",
    "\n",
    "This approach ensures predictable structure and makes downstream parsing reliable in production settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Run a Local Model with Ollama\n",
    "\n",
    "**Goal:** See how a local LLM works and connect concepts (tokens, temperature, RAG) to a real tool.\n",
    "\n",
    "*Skip this if you don't have a machine that can run Ollama.*\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Install Ollama**\n",
    "   - Visit: [https://docs.ollama.com/](https://docs.ollama.com/)\n",
    "   - Download and install for your OS\n",
    "\n",
    "2. **Run a Model**\n",
    "   ```bash\n",
    "   ollama run gemma3\n",
    "   # or\n",
    "   ollama run llama3\n",
    "   ```\n",
    "\n",
    "3. **Ask Questions**\n",
    "   - \"Explain Retrieval-Augmented Generation (RAG) in 3 bullet points.\"\n",
    "   - \"What is the exact vacation policy of ICC in 2024?\" (should show limitation)\n",
    "\n",
    "4. **Call the HTTP API**\n",
    "   ```bash\n",
    "   curl http://localhost:11434/api/generate \\\n",
    "     -d '{\n",
    "       \"model\": \"llama3\",\n",
    "       \"prompt\": \"Explain what a context window is in 3 sentences.\"\n",
    "     }'\n",
    "   ```\n",
    "\n",
    "**Deliverable:**\n",
    "- 1-2 screenshots of Ollama answers\n",
    "- 4-6 sentences comparing to ChatGPT (style, hallucinations, speed)\n",
    "- Briefly describe the API endpoint and JSON response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Call OpenAI API\n",
    "\n",
    "**Goal:** Make your first real API call to a hosted LLM and observe tokens & temperature in action.\n",
    "\n",
    "*If you don't have an API key, just read the code and explain what it does.*\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Setup**\n",
    "   ```bash\n",
    "   pip install --upgrade openai\n",
    "   ```\n",
    "\n",
    "2. **Minimal Script** (`lesson1_openai_test.py`):\n",
    "   ```python\n",
    "   from openai import OpenAI\n",
    "   \n",
    "   client = OpenAI()  # expects OPENAI_API_KEY in environment\n",
    "   \n",
    "   response = client.responses.create(\n",
    "       model=\"gpt-4.1\",\n",
    "       input=\"Explain Retrieval-Augmented Generation (RAG) in 3 bullet points.\"\n",
    "   )\n",
    "   \n",
    "   print(response.output[0].content[0].text)\n",
    "   print(\"Total tokens:\", response.usage.total_tokens)\n",
    "   ```\n",
    "\n",
    "3. **Run and Analyze**\n",
    "   - How many total tokens did this use?\n",
    "   - If price is $X per 1K tokens, roughly how much did this cost?\n",
    "\n",
    "4. **Change Sampling Parameters**\n",
    "   ```python\n",
    "   response = client.responses.create(\n",
    "       model=\"gpt-4.1\",\n",
    "       input=\"Explain Retrieval-Augmented Generation (RAG) in 3 bullet points.\",\n",
    "       temperature=0.0,  # Try 0.0 and 0.8\n",
    "       top_p=0.9,\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**Deliverable:**\n",
    "- Paste output showing token count\n",
    "- Compare outputs at different temperatures (4-6 sentences)\n",
    "- Which is more deterministic? More creative? Any hallucinations?\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
